import{j as e}from"./chunk-OIYGIGL5-BFuAKb0n.js";import{_ as h,a as l}from"./WAL_splitting-S_OUAgmz.js";let g=`



\`HRegionServer\` is the RegionServer implementation. It is responsible for serving and managing regions. In a distributed cluster, a RegionServer runs on a [DataNode](/docs/architecture/hdfs#hdfs-datanode).

## Interface

The methods exposed by \`HRegionRegionInterface\` contain both data-oriented and region-maintenance methods:

* Data (get, put, delete, next, etc.)
* Region (splitRegion, compactRegion, etc.) For example, when the \`Admin\` method \`majorCompact\` is invoked on a table, the client is actually iterating through all regions for the specified table and requesting a major compaction directly to each region.

## Processes

The RegionServer runs a variety of background threads:

### CompactSplitThread

Checks for splits and handle minor compactions.

### MajorCompactionChecker

Checks for major compactions.

### MemStoreFlusher

Periodically flushes in-memory writes in the MemStore to StoreFiles.

### LogRoller

Periodically checks the RegionServer's WAL.

## Coprocessors

Coprocessors were added in 0.92. There is a thorough [Blog Overview of CoProcessors](https://blogs.apache.org/hbase/entry/coprocessor_introduction) posted. Documentation will eventually move to this reference guide, but the blog is the most current information available at this time.

## Block Cache

HBase provides two different BlockCache implementations to cache data read from HDFS: the default on-heap \`LruBlockCache\` and the \`BucketCache\`, which is (usually) off-heap. This section discusses benefits and drawbacks of each implementation, how to choose the appropriate option, and configuration options for each.

<Callout type="info" title="Block Cache Reporting: UI">
  See the RegionServer UI for detail on caching deploy. See configurations, sizings, current usage,
  time-in-the-cache, and even detail on block counts and types.
</Callout>

### Cache Choices

\`LruBlockCache\` is the original implementation, and is entirely within the Java heap. \`BucketCache\` is optional and mainly intended for keeping block cache data off-heap, although \`BucketCache\` can also be a file-backed cache. In file-backed we can either use it in the file mode or the mmaped mode. We also have pmem mode where the bucket cache resides on the persistent memory device.

When you enable BucketCache, you are enabling a two tier caching system. We used to describe the tiers as "L1" and "L2" but have deprecated this terminology as of hbase-2.0.0. The "L1" cache referred to an instance of LruBlockCache and "L2" to an off-heap BucketCache. Instead, when BucketCache is enabled, all DATA blocks are kept in the BucketCache tier and meta blocks — INDEX and BLOOM blocks — are on-heap in the \`LruBlockCache\`. Management of these two tiers and the policy that dictates how blocks move between them is done by \`CombinedBlockCache\`.

### General Cache Configurations

Apart from the cache implementation itself, you can set some general configuration options to control how the cache performs. See [CacheConfig](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CacheConfig.html). After setting any of these options, restart or rolling restart your cluster for the configuration to take effect. Check logs for errors or unexpected behavior.

See also [Prefetch Option for Blockcache](/docs/performance#prefetch-option-for-blockcache), which discusses a new option introduced in [HBASE-9857](https://issues.apache.org/jira/browse/HBASE-9857).

### LruBlockCache Design

The LruBlockCache is an LRU cache that contains three levels of block priority to allow for scan-resistance and in-memory ColumnFamilies:

* Single access priority: The first time a block is loaded from HDFS it normally has this priority and it will be part of the first group to be considered during evictions. The advantage is that scanned blocks are more likely to get evicted than blocks that are getting more usage.
* Multi access priority: If a block in the previous priority group is accessed again, it upgrades to this priority. It is thus part of the second group considered during evictions.
* In-memory access priority: If the block's family was configured to be "in-memory", it will be part of this priority disregarding the number of times it was accessed. Catalog tables are configured like this. This group is the last one considered during evictions.\\
  To mark a column family as in-memory, call
  \`\`\`java
  HColumnDescriptor.setInMemory(true);
  \`\`\`
  if creating a table from java, or set \`IN_MEMORY ⇒ true\` when creating or altering a table in the shell: e.g.
  \`\`\`java
  hbase(main):003:0> create  't', {NAME => 'f', IN_MEMORY => 'true'}
  \`\`\`
  For more information, see the LruBlockCache source

### LruBlockCache Usage

Block caching is enabled by default for all the user tables which means that any read operation will load the LRU cache. This might be good for a large number of use cases, but further tunings are usually required in order to achieve better performance. An important concept is the [working set size](http://en.wikipedia.org/wiki/Working_set_size), or WSS, which is: "the amount of memory needed to compute the answer to a problem". For a website, this would be the data that's needed to answer the queries over a short amount of time.

The way to calculate how much memory is available in HBase for caching is:

\`\`\`java
number of region servers * heap size * hfile.block.cache.size * 0.99
\`\`\`

The default value for the block cache is 0.4 which represents 40% of the available heap. The last value (99%) is the default acceptable loading factor in the LRU cache after which eviction is started. The reason it is included in this equation is that it would be unrealistic to say that it is possible to use 100% of the available memory since this would make the process blocking from the point where it loads new blocks. Here are some examples:

* One region server with the heap size set to 1 GB and the default block cache size will have 405 MB of block cache available.
* 20 region servers with the heap size set to 8 GB and a default block cache size will have 63.3 GB of block cache.
* 100 region servers with the heap size set to 24 GB and a block cache size of 0.5 will have about 1.16 TB of block cache.

Your data is not the only resident of the block cache. Here are others that you may have to take into account:

* **Catalog Tables**\\
  The \`hbase:meta\` table is forced into the block cache and have the in-memory priority which means that they are harder to evict.

  <Callout type="info">
    The hbase:meta tables can occupy a few MBs depending on the number of regions.
  </Callout>

* **HFiles Indexes**\\
  An *HFile* is the file format that HBase uses to store data in HDFS. It contains a multi-layered index which allows HBase to seek the data without having to read the whole file. The size of those indexes is a factor of the block size (64KB by default), the size of your keys and the amount of data you are storing. For big data sets it's not unusual to see numbers around 1GB per region server, although not all of it will be in cache because the LRU will evict indexes that aren't used.

* **Keys**\\
  The values that are stored are only half the picture, since each value is stored along with its keys (row key, family qualifier, and timestamp). See [Try to minimize row and column sizes](/docs/regionserver-sizing#try-to-minimize-row-and-column-sizes).

* **Bloom Filters**\\
  Just like the HFile indexes, those data structures (when enabled) are stored in the LRU.

Currently the recommended way to measure HFile indexes and bloom filters sizes is to look at the region server web UI and checkout the relevant metrics. For keys, sampling can be done by using the HFile command line tool and look for the average key size metric. Since HBase 0.98.3, you can view details on BlockCache stats and metrics in a special Block Cache section in the UI. As of HBase 2.4.14, you can estimate HFile indexes and bloom filters vs other DATA blocks using blockCacheCount and blockCacheDataBlockCount in JMX. The formula \`(blockCacheCount - blockCacheDataBlockCount) * blockSize\` will give you an estimate which can be useful when trying to enable the BucketCache. You should make sure the post-BucketCache config gives enough memory to the on-heap LRU cache to hold at least the same number of non-DATA blocks from pre-BucketCache. Once BucketCache is enabled, the L1 metrics like l1CacheSize, l1CacheCount, and l1CacheEvictionCount can help you further tune the size.

It's generally bad to use block caching when the WSS doesn't fit in memory. This is the case when you have for example 40GB available across all your region servers' block caches but you need to process 1TB of data. One of the reasons is that the churn generated by the evictions will trigger more garbage collections unnecessarily. Here are two use cases:

* Fully random reading pattern: This is a case where you almost never access the same row twice within a short amount of time such that the chance of hitting a cached block is close to 0. Setting block caching on such a table is a waste of memory and CPU cycles, more so that it will generate more garbage to pick up by the JVM. For more information on monitoring GC, see [JVM Garbage Collection Logs](/docs/troubleshooting#jvm-garbage-collection-logs).
* Mapping a table: In a typical MapReduce job that takes a table in input, every row will be read only once so there's no need to put them into the block cache. The Scan object has the option of turning this off via the setCacheBlocks method (set it to false). You can still keep block caching turned on on this table if you need fast random read access. An example would be counting the number of rows in a table that serves live traffic, caching every block of that table would create massive churn and would surely evict data that's currently in use.

#### Caching META blocks only (DATA blocks in fscache)

An interesting setup is one where we cache META blocks only and we read DATA blocks in on each access. If the DATA blocks fit inside fscache, this alternative may make sense when access is completely random across a very large dataset. To enable this setup, alter your table and for each column family set \`BLOCKCACHE ⇒ 'false'\`. You are 'disabling' the BlockCache for this column family only. You can never disable the caching of META blocks. Since [HBASE-4683 Always cache index and bloom blocks](https://issues.apache.org/jira/browse/HBASE-4683), we will cache META blocks even if the BlockCache is disabled.

### Off-heap Block Cache

#### How to Enable BucketCache

The usual deployment of BucketCache is via a managing class that sets up two caching tiers: an on-heap cache implemented by LruBlockCache and a second cache implemented with BucketCache. The managing class is [CombinedBlockCache](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CombinedBlockCache.html) by default. The previous link describes the caching 'policy' implemented by CombinedBlockCache. In short, it works by keeping meta blocks — INDEX and BLOOM in the on-heap LruBlockCache tier — and DATA blocks are kept in the BucketCache tier.

* **Pre-hbase-2.0.0 versions**

  Fetching will always be slower when fetching from BucketCache in pre-hbase-2.0.0, as compared to the native on-heap LruBlockCache. However, latencies tend to be less erratic across time, because there is less garbage collection when you use BucketCache since it is managing BlockCache allocations, not the GC. If the BucketCache is deployed in off-heap mode, this memory is not managed by the GC at all. This is why you'd use BucketCache in pre-2.0.0, so your latencies are less erratic, to mitigate GCs and heap fragmentation, and so you can safely use more memory. See Nick Dimiduk's [BlockCache 101](http://www.n10k.com/blog/blockcache-101/) for comparisons running on-heap vs off-heap tests. Also see [Comparing BlockCache Deploys](https://web.archive.org/web/20231109025243/http://people.apache.org/~stack/bc/) which finds that if your dataset fits inside your LruBlockCache deploy, use it otherwise if you are experiencing cache churn (or you want your cache to exist beyond the vagaries of java GC), use BucketCache.

  In pre-2.0.0, one can configure the BucketCache so it receives the \`victim\` of an LruBlockCache eviction. All Data and index blocks are cached in L1 first. When eviction happens from L1, the blocks (or \`victims\`) will get moved to L2. Set \`cacheDataInL1\` via \`(HColumnDescriptor.setCacheDataInL1(true)\` or in the shell, creating or amending column families setting \`CACHE_DATA_IN_L1\` to true: e.g.

  \`\`\`java
  hbase(main):003:0> create 't', {NAME => 't', CONFIGURATION => {CACHE_DATA_IN_L1 => 'true'}}
  \`\`\`

* **hbase-2.0.0+ versions**

  HBASE-11425 changed the HBase read path so it could hold the read-data off-heap avoiding copying of cached data on to the java heap. See [Offheap read-path](/docs/offheap-read-write#offheap-read-path). In hbase-2.0.0, off-heap latencies approach those of on-heap cache latencies with the added benefit of NOT provoking GC.

  From HBase 2.0.0 onwards, the notions of L1 and L2 have been deprecated. When BucketCache is turned on, the DATA blocks will always go to BucketCache and INDEX/BLOOM blocks go to on heap LRUBlockCache. \`cacheDataInL1\` support has been removed.

#### BucketCache Deploy Modes

The BucketCache Block Cache can be deployed *offheap*, *file* or *mmaped* file mode.

You set which via the \`hbase.bucketcache.ioengine\` setting. Setting it to \`offheap\` will have BucketCache make its allocations off-heap, and an ioengine setting of \`file:PATH_TO_FILE\` will direct BucketCache to use file caching (Useful in particular if you have some fast I/O attached to the box such as SSDs). From 2.0.0, it is possible to have more than one file backing the BucketCache. This is very useful especially when the Cache size requirement is high. For multiple backing files, configure ioengine as \`files:PATH_TO_FILE1,PATH_TO_FILE2,PATH_TO_FILE3\`. BucketCache can be configured to use an mmapped file also. Configure ioengine as \`mmap:PATH_TO_FILE\` for this.

It is possible to deploy a tiered setup where we bypass the CombinedBlockCache policy and have BucketCache working as a strict L2 cache to the L1 LruBlockCache. For such a setup, set \`hbase.bucketcache.combinedcache.enabled\` to \`false\`. In this mode, on eviction from L1, blocks go to L2. When a block is cached, it is cached first in L1. When we go to look for a cached block, we look first in L1 and if none found, then search L2. Let us call this deploy format, *Raw L1+L2*. NOTE: This L1+L2 mode is removed from 2.0.0. When BucketCache is used, it will be strictly the DATA cache and the LruBlockCache will cache INDEX/META blocks.

Other BucketCache configs include: specifying a location to persist cache to across restarts, how many threads to use writing the cache, etc. See the [CacheConfig.html](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CacheConfig.html) class for configuration options and descriptions.

To check it enabled, look for the log line describing cache setup; it will detail how BucketCache has been deployed. Also see the UI. It will detail the cache tiering and their configuration.

#### BucketCache Example Configuration

This sample provides a configuration for a 4 GB off-heap BucketCache with a 1 GB on-heap cache.

Configuration is performed on the RegionServer.

Setting \`hbase.bucketcache.ioengine\` and \`hbase.bucketcache.size\` > 0 enables \`CombinedBlockCache\`. Let us presume that the RegionServer has been set to run with a 5G heap: i.e. \`HBASE_HEAPSIZE=5g\`.

1. First, edit the RegionServer's *hbase-env.sh* and set \`HBASE_OFFHEAPSIZE\` to a value greater than the off-heap size wanted, in this case, 4 GB (expressed as 4G). Let's set it to 5G. That'll be 4G for our off-heap cache and 1G for any other uses of off-heap memory (there are other users of off-heap memory other than BlockCache; e.g. DFSClient in RegionServer can make use of off-heap memory). See Direct Memory Usage In HBase below.

   \`\`\`java
   HBASE_OFFHEAPSIZE=5G
   \`\`\`

2. Next, add the following configuration to the RegionServer's *hbase-site.xml*.

   \`\`\`xml
   <property>
     <name>hbase.bucketcache.ioengine</name>
     <value>offheap</value>
   </property>
   <property>
     <name>hfile.block.cache.size</name>
     <value>0.2</value>
   </property>
   <property>
     <name>hbase.bucketcache.size</name>
     <value>4196</value>
   </property>
   \`\`\`

3. Restart or rolling restart your cluster, and check the logs for any issues.

In the above, we set the BucketCache to be 4G. We configured the on-heap LruBlockCache have 20% (0.2) of the RegionServer's heap size (0.2 \\* 5G = 1G). In other words, you configure the L1 LruBlockCache as you would normally (as if there were no L2 cache present).

[HBASE-10641](https://issues.apache.org/jira/browse/HBASE-10641) introduced the ability to configure multiple sizes for the buckets of the BucketCache, in HBase 0.98 and newer. To configurable multiple bucket sizes, configure the new property \`hbase.bucketcache.bucket.sizes\` to a comma-separated list of block sizes, ordered from smallest to largest, with no spaces. The goal is to optimize the bucket sizes based on your data access patterns. The following example configures buckets of size 4096 and 8192.

\`\`\`xml
<property>
  <name>hbase.bucketcache.bucket.sizes</name>
  <value>4096,8192</value>
</property>
\`\`\`

<Callout type="info" title="Direct Memory Usage In HBase">
  The default maximum direct memory varies by JVM. Traditionally it is 64M or some relation to allocated heap size (-Xmx) or no limit at all (JDK7 apparently). HBase servers use direct memory, in particular short-circuit reading (See [Leveraging local data](/docs/performance#leveraging-local-data)), the hosted DFSClient will allocate direct memory buffers. How much the DFSClient uses is not easy to quantify; it is the number of open HFiles \\* \`hbase.dfs.client.read.shortcircuit.buffer.size\` where \`hbase.dfs.client.read.shortcircuit.buffer.size\` is set to 128k in HBase — see *hbase-default.xml* default configurations. If you do off-heap block caching, you'll be making use of direct memory. The RPCServer uses a ByteBuffer pool. From 2.0.0, these buffers are off-heap ByteBuffers. Starting your JVM, make sure the \`-XX:MaxDirectMemorySize\` setting in *conf/hbase-env.sh* considers off-heap BlockCache (\`hbase.bucketcache.size\`), DFSClient usage, RPC side ByteBufferPool max size. This has to be bit higher than sum of off heap BlockCache size and max ByteBufferPool size. Allocating an extra of 1-2 GB for the max direct memory size has worked in tests. Direct memory, which is part of the Java process heap, is separate from the object heap allocated by -Xmx. The value allocated by \`MaxDirectMemorySize\` must not exceed physical RAM, and is likely to be less than the total available RAM due to other memory requirements and system constraints.

  You can see how much memory — on-heap and off-heap/direct — a RegionServer is configured to use and how much it is using at any one time by looking at the *Server Metrics: Memory* tab in the UI. It can also be gotten via JMX. In particular the direct memory currently used by the server can be found on the \`java.nio.type=BufferPool,name=direct\` bean. Terracotta has a [good write up](https://web.archive.org/web/20170907032911/http://terracotta.org/documentation/4.0/bigmemorygo/configuration/storage-options) on using off-heap memory in Java. It is for their product BigMemory but a lot of the issues noted apply in general to any attempt at going off-heap. Check it out.
</Callout>

<Callout type="info" title="hbase.bucketcache.percentage.in.combinedcache">
  This is a pre-HBase 1.0 configuration removed because it was confusing. It was a float that you would set to some value between 0.0 and 1.0. Its default was 0.9. If the deploy was using CombinedBlockCache, then the LruBlockCache L1 size was calculated to be \`(1 - hbase.bucketcache.percentage.in.combinedcache) * size-of-bucketcache\` and the BucketCache size was \`hbase.bucketcache.percentage.in.combinedcache * size-of-bucket-cache\`. where size-of-bucket-cache itself is EITHER the value of the configuration \`hbase.bucketcache.size\` IF it was specified as Megabytes OR \`hbase.bucketcache.size\` \\* \`-XX:MaxDirectMemorySize\` if \`hbase.bucketcache.size\` is between 0 and 1.0.

  In 1.0, it should be more straight-forward. Onheap LruBlockCache size is set as a fraction of java heap using \`hfile.block.cache.size setting\` (not the best name) and BucketCache is set as above in absolute Megabytes.
</Callout>

### Time Based Priority for BucketCache

[HBASE-28463](https://issues.apache.org/jira/browse/HBASE-28463) introduced time based priority for blocks in BucketCache. It allows for defining an age threshold at individual column families' configuration, whereby blocks older than this configured threshold would be targeted first for eviction.

Blocks from column families that don't define the age threshold wouldn't be evaluated by the time based priority, and would only be evicted following the LRU eviction logic.

This feature is mostly useful for use cases where most recent data is more frequently accessed, and therefore should get higher priority in the cache. Configuring Time Based Priority with the "age" of most accessed data would then give a finer control over blocks allocation in the BucketCache than the built-in LRU eviction logic.

Time Based Priority for BucketCache provides three different strategies for defining data age:

* Cell timestamps: Uses the timestamp portion of HBase cells for comparing the data age.
* Custom cell qualifiers: Uses a custom-defined date qualifier for comparing the data age. It uses that value to tier the entire row containing the given qualifier value. This requires that the custom qualifier be a valid Java long timestamp.
* Custom value provider: Allows for defining a pluggable implementation that contains the logic for identifying the date value to be used for comparison. This also provides additional flexibility for different use cases that might have the date stored in other formats or embedded with other data in various portions of a given row.

For use cases where priority is determined by the order of record ingestion in HBase (with the most recent being the most relevant), the built-in cell timestamp offers the most convenient and efficient method for configuring age-based priority. See [Using Cell timestamps for Time Based Priority](/docs/architecture/regionserver#using-cell-timestamps-for-time-based-priority).

Some applications may utilize a custom date column to define the priority of table records. In such instances, a custom cell qualifier-based priority is advisable. See [Using Custom Cell Qualifiers for Time Based Priority](/docs/architecture/regionserver#using-custom-cell-qualifiers-for-time-based-priority).

Finally, more intricate schemas may incorporate domain-specific logic for defining the age of each record. The custom value provider facilitates the integration of custom code to implement the appropriate parsing of the date value that should be used for the priority comparison. See [Using a Custom value provider for Time Based Priority](/docs/architecture/regionserver#using-a-custom-value-provider-for-time-based-priority).

With Time Based Priority for BucketCache, blocks age is evaluated when deciding if a block should be cached (i.e. during reads, writes, compaction and prefetch), as well as during the cache freeSpace run (mass eviction), prior to executing the LRU logic.

Because blocks don't hold any specific meta information other than type, it's necessary to group blocks of the same "age group" on separate files, using specialized compaction implementations (see more details in the configuration section below). The time range of all blocks in each file is then appended at the file meta info section, and is used for evaluating the age of blocks that should be considered in the Time Based Priority logic.

#### Configuring Time Based Priority for BucketCache

Finding the age of each block involves an extra overhead, therefore the feature is disabled by default at a global configuration level.

To enable it, the following configuration should be set on RegionServers' *hbase-site.xml*:

\`\`\`xml
<property>
  <name>hbase.regionserver.datatiering.enable</name>
  <value>true</value>
</property>
\`\`\`

Once enabled globally, it's necessary to define the desired strategy-specific settings at the individual column family level.

#### Using Cell timestamps for Time Based Priority

This strategy is the most efficient to run, as it uses the timestamp portion of each cell containing the data for comparing the age of blocks. It requires DateTieredCompaction for splitting the blocks into separate files according to blocks' ages.

The example below sets the hot age threshold to one week (in milliseconds) for the column family 'cf1' in table 'orders':

\`\`\`java
hbase(main):003:0> alter 'orders', {NAME => 'cf1',
  CONFIGURATION => {'hbase.hstore.datatiering.type' => 'TIME_RANGE',
    'hbase.hstore.datatiering.hot.age.millis' => '604800000',
    'hbase.hstore.engine.class' => 'org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine',
    'hbase.hstore.blockingStoreFiles' => '60',
    'hbase.hstore.compaction.min' => '2',
    'hbase.hstore.compaction.max' => '60'
  }
}
\`\`\`

<Callout type="info" title="Date Tiered Compaction specific tunings">
  In the example above, the properties governing the number of windows and period of each window in the date tiered compaction were not set. With the default settings, the compaction will create initially four windows of six hours, then four windows of one day each, then another four windows of four days each and so on until the minimum timestamp among the selected files is covered. This can create a large number of files, therefore, additional changes to the 'hbase.hstore.blockingStoreFiles', 'hbase.hstore.compaction.min' and 'hbase.hstore.compaction.max' are recommended.

  Alternatively, consider adjusting the initial window size to the same as the hot age threshold, and two windows only per tier:

  \`\`\`java
  hbase(main):003:0> alter 'orders', {NAME => 'cf1',
    CONFIGURATION => {'hbase.hstore.datatiering.type' => 'TIME_RANGE',
      'hbase.hstore.datatiering.hot.age.millis' => '604800000',
      'hbase.hstore.engine.class' => 'org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine',
      'hbase.hstore.compaction.date.tiered.base.window.millis' => '604800000',
      'hbase.hstore.compaction.date.tiered.windows.per.tier' => '2'
    }
  }
  \`\`\`
</Callout>

#### Using Custom Cell Qualifiers for Time Based Priority

This strategy uses a new compaction implementation designed for Time Based Priority. It extends date tiered compaction, but instead of producing multiple tiers of various time windows, it simply splits files into two groups: the "cold" group, where all blocks are older than the defined threshold age, and the "hot" group, where all blocks are newer than the threshold age.

The example below defines a cell qualifier 'event\\_date' to be used for comparing the age of blocks within the custom cell qualifier strategy:

\`\`\`java
hbase(main):003:0> alter 'orders', {NAME => 'cf1',
  CONFIGURATION => {'hbase.hstore.datatiering.type' => 'CUSTOM',
    'TIERING_CELL_QUALIFIER' => 'event_date',
    'hbase.hstore.datatiering.hot.age.millis' => '604800000',
    'hbase.hstore.engine.class' => 'org.apache.hadoop.hbase.regionserver.CustomTieredStoreEngine',
    'hbase.hstore.compaction.date.tiered.custom.age.limit.millis' => '604800000'
  }
}
\`\`\`

<Callout type="info" title="Time Based Priority x Compaction Age Threshold Configurations">
  Note that there are two different configurations for defining the hot age threshold. This is
  because the Time Based Priority enforcer operates independently of the compaction implementation.
</Callout>

#### Using a Custom value provider for Time Based Priority

It's also possible to hook in domain-specific logic for defining the data age of each row to be used for comparing blocks priorities. The Custom Time Based Priority framework defines the \`CustomTieredCompactor.TieringValueProvider\` interface, which can be implemented to provide the specific date value to be used by compaction for grouping the blocks according to the threshold age.

In the following example, the \`RowKeyPortionTieringValueProvider\` implements the \`getTieringValue\` method. This method parses the date from a segment of the row key value, specifically between positions 14 and 29, using the "yyyyMMddHHmmss" format. The parsed date is then returned as a long timestamp, which is then used by custom tiered compaction to group the blocks based on the defined hot age threshold:

\`\`\`java
public class RowKeyPortionTieringValueProvider implements CustomTieredCompactor.TieringValueProvider {
   private SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMddHHmmss");
   @Override
   public void init(Configuration configuration) throws Exception {}

   @Override
   public long getTieringValue(Cell cell) {
     byte[] rowArray = new byte[cell.getRowLength()];
     System.arraycopy(cell.getRowArray(), cell.getRowOffset(), rowArray, 0, cell.getRowLength());
     String datePortion = Bytes.toString(rowArray).substring(14, 29).trim();
     try {
       return sdf.parse(datePortion).getTime();
     } catch (ParseException e) {
       //handle error
     }
     return Long.MAX_VALUE;
   }
}
\`\`\`

The Tiering Value Provider above can then be configured for Time Based Priority as follows:

\`\`\`java
hbase(main):003:0> alter 'orders', {NAME => 'cf1',
  CONFIGURATION => {'hbase.hstore.datatiering.type' => 'CUSTOM',
    'hbase.hstore.custom-tiering-value.provider.class' =>
      'org.apache.hbase.client.example.RowKeyPortionTieringValueProvider',
    'hbase.hstore.datatiering.hot.age.millis' => '604800000',
    'hbase.hstore.engine.class' => 'org.apache.hadoop.hbase.regionserver.CustomTieredStoreEngine',
    'hbase.hstore.compaction.date.tiered.custom.age.limit.millis' => '604800000'
  }
}
\`\`\`

<Callout type="info">
  Upon enabling Custom Time Based Priority (either the custom qualifier or custom value provider) in
  the column family configuration, it is imperative that major compaction be executed twice on the
  specified tables to ensure the effective application of the newly configured priorities within the
  bucket cache.
</Callout>

<Callout type="info">
  Time Based Priority was originally implemented with the cell timestamp strategy only. The original design covering cell timestamp based strategy is available [here](https://docs.google.com/document/d/1Qd3kvZodBDxHTFCIRtoePgMbvyuUSxeydi2SEWQFQro/edit?tab=t.0#heading=h.gjdgxs).

  The second phase including the two custom strategies mentioned above is detailed in [this separate design doc](https://docs.google.com/document/d/1uBGIO9IQ-FbSrE5dnUMRtQS23NbCbAmRVDkAOADcU_E/edit?tab=t.0).
</Callout>

### Compressed BlockCache

[HBASE-11331](https://issues.apache.org/jira/browse/HBASE-11331) introduced lazy BlockCache decompression, more simply referred to as compressed BlockCache. When compressed BlockCache is enabled data and encoded data blocks are cached in the BlockCache in their on-disk format, rather than being decompressed and decrypted before caching.

For a RegionServer hosting more data than can fit into cache, enabling this feature with SNAPPY compression has been shown to result in 50% increase in throughput and 30% improvement in mean latency while, increasing garbage collection by 80% and increasing overall CPU load by 2%. See HBASE-11331 for more details about how performance was measured and achieved. For a RegionServer hosting data that can comfortably fit into cache, or if your workload is sensitive to extra CPU or garbage-collection load, you may receive less benefit.

The compressed BlockCache is disabled by default. To enable it, set \`hbase.block.data.cachecompressed\` to \`true\` in *hbase-site.xml* on all RegionServers.

### Cache Aware Load Balancer

Depending on the data size and the configured cache size, the cache warm up can take anywhere from a few minutes to a few hours. This becomes even more critical for HBase deployments over cloud storage, where compute is separated from storage. Doing this everytime the region server starts can be a very expensive process. To eliminate this, [HBASE-27313](https://issues.apache.org/jira/browse/HBASE-27313) implemented the cache persistence feature where the region servers periodically persist the blocks cached in the bucket cache. This persisted information is then used to resurrect the cache in the event of a region server restart because of normal restart or crash.

[HBASE-27999](https://issues.apache.org/jira/browse/HBASE-27999) implements the cache aware load balancer, which adds to the load balancer the ability to consider the cache allocation of each region on region servers when calculating a new assignment plan, using the region/region server cache allocation information reported by region servers to calculate the percentage of HFiles cached for each region on the hosting server. This information is then used by the balancer as a factor when deciding on an optimal, new assignment plan.

The master node captures the caching information from all the region servers and uses this information to decide on new region assignments while ensuring a minimal impact on the current cache allocation. A region is assigned to the region server where it has a better cache ratio as compared to the region server where it is currently hosted.

The CacheAwareLoadBalancer uses two cost elements for deciding the region allocation. These are described below:

1. **Cache Cost**\\
   The cache cost is calculated as the percentage of data for a region cached on the region server where it is either currently hosted or was previously hosted. A region may have multiple HFiles, each of different sizes. A HFile is considered to be fully prefetched when all the data blocks in this file are in the cache. The region server hosting this region calculates the ratio of number of HFiles fully cached in the cache to the total number of HFiles in the region. This ratio will vary from 0 (region hosted on this server, but none of its HFiles are cached into the cache) to 1 (region hosted on this server and all the HFiles for this region are cached into the cache).\\
   Every region server maintains this information for all the regions currently hosted there. In addition to that, this cache ratio is also maintained for the regions which were previously hosted on this region server giving historical information about the regions.
2. Skewness Cost

The cache aware balancer will consider cache cost with the skewness cost to decide on the region assignment plan under following conditions:

1. There is an idle server in the cluster. This can happen when an existing server is restarted or a new server is added to the cluster.
2. When the cost of maintaining the balance in the cluster is greater than the minimum threshold defined by the configuration *hbase.master.balancer.stochastic.minCostNeedBalance*.

The CacheAwareLoadBalancer can be enabled in the cluster by setting the following configuration properties in the master master configuration:

\`\`\`xml
<property>
  <name>hbase.master.loadbalancer.class</name>
  <value>org.apache.hadoop.hbase.master.balancer.CacheAwareLoadBalancer</value>
</property>
<property>
  <name>hbase.bucketcache.persistent.path</name>
  <value>/path/to/bucketcache_persistent_file</value>
</property>
\`\`\`

Within HBASE-29168, the CacheAwareLoadBalancer implements region move throttling. This mitigates the impact of "losing" cache factor when balancing mainly due to region skewness, i.e. when new region servers are added to the cluster, a large bulk of cached regions may move to the new servers at once, which can cause noticeable read performance impacts for cache sensitive use cases. The throttling sleep time is determined by the **hbase.master.balancer.move.throttlingMillis** property, and it defaults to 60000 millis. If a region planned to be moved has a cache ratio on the target server above the thershold configurable by the **hbase.master.balancer.stochastic.throttling.cacheRatio** property (80% by default), no throttling will be applied in this region move.

## RegionServer Splitting Implementation

As write requests are handled by the region server, they accumulate in an in-memory storage system called the *memstore*. Once the memstore fills, its content are written to disk as additional store files. This event is called a *memstore flush*. As store files accumulate, the RegionServer will [compact](/docs/architecture/regions#compaction) them into fewer, larger files. After each flush or compaction finishes, the amount of data stored in the region has changed. The RegionServer consults the region split policy to determine if the region has grown too large or should be split for another policy-specific reason. A region split request is enqueued if the policy recommends it.

Logically, the process of splitting a region is simple. We find a suitable point in the keyspace of the region where we should divide the region in half, then split the region's data into two new regions at that point. The details of the process however are not simple. When a split happens, the newly created *daughter regions* do not rewrite all the data into new files immediately. Instead, they create small files similar to symbolic link files, named [Reference files](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/Reference.html), which point to either the top or bottom part of the parent store file according to the split point. The reference file is used just like a regular data file, but only half of the records are considered. The region can only be split if there are no more references to the immutable data files of the parent region. Those reference files are cleaned gradually by compactions, so that the region will stop referring to its parents files, and can be split further.

Although splitting the region is a local decision made by the RegionServer, the split process itself must coordinate with many actors. The RegionServer notifies the Master before and after the split, updates the \`.META.\` table so that clients can discover the new daughter regions, and rearranges the directory structure and data files in HDFS. Splitting is a multi-task process. To enable rollback in case of an error, the RegionServer keeps an in-memory journal about the execution state. The steps taken by the RegionServer to execute the split are illustrated in the "RegionServer Split Process" schema below. Each step is labeled with its step number. Actions from RegionServers or Master are shown in red, while actions from the clients are shown in green.

<img alt="Region Split Process" src={__img0} placeholder="blur" />

1. The RegionServer decides locally to split the region, and prepares the split. **THE SPLIT TRANSACTION IS STARTED.** As a first step, the RegionServer acquires a shared read lock on the table to prevent schema modifications during the splitting process. Then it creates a znode in zookeeper under \`/hbase/region-in-transition/region-name\`, and sets the znode's state to \`SPLITTING\`.
2. The Master learns about this znode, since it has a watcher for the parent \`region-in-transition\` znode.
3. The RegionServer creates a sub-directory named \`.splits\` under the parent's \`region\` directory in HDFS.
4. The RegionServer closes the parent region and marks the region as offline in its local data structures. **THE SPLITTING REGION IS NOW OFFLINE.** At this point, client requests coming to the parent region will throw \`NotServingRegionException\`. The client will retry with some backoff. The closing region is flushed.
5. The RegionServer creates region directories under the \`.splits\` directory, for daughter regions A and B, and creates necessary data structures. Then it splits the store files, in the sense that it creates two Reference files per store file in the parent region. Those reference files will point to the parent region's files.
6. The RegionServer creates the actual region directory in HDFS, and moves the reference files for each daughter.
7. The RegionServer sends a \`Put\` request to the \`.META.\` table, to set the parent as offline in the \`.META.\` table and add information about daughter regions. At this point, there won't be individual entries in \`.META.\` for the daughters. Clients will see that the parent region is split if they scan \`.META.\`, but won't know about the daughters until they appear in \`.META.\`. Also, if this \`Put\` to \`.META\`. succeeds, the parent will be effectively split. If the RegionServer fails before this RPC succeeds, Master and the next Region Server opening the region will clean dirty state about the region split. After the \`.META.\` update, though, the region split will be rolled-forward by Master.
8. The RegionServer opens daughters A and B in parallel.
9. The RegionServer adds the daughters A and B to \`.META.\`, together with information that it hosts the regions. **THE SPLIT REGIONS (DAUGHTERS WITH REFERENCES TO PARENT) ARE NOW ONLINE.** After this point, clients can discover the new regions and issue requests to them. Clients cache the \`.META.\` entries locally, but when they make requests to the RegionServer or \`.META.\`, their caches will be invalidated, and they will learn about the new regions from \`.META.\`.
10. The RegionServer updates znode \`/hbase/region-in-transition/region-name\` in ZooKeeper to state \`SPLIT\`, so that the master can learn about it. The balancer can freely re-assign the daughter regions to other region servers if necessary. **THE SPLIT TRANSACTION IS NOW FINISHED.**
11. After the split, \`.META.\` and HDFS will still contain references to the parent region. Those references will be removed when compactions in daughter regions rewrite the data files. Garbage collection tasks in the master periodically check whether the daughter regions still refer to the parent region's files. If not, the parent region will be removed.

## Write Ahead Log (WAL)

### Purpose

The *Write Ahead Log (WAL)* records all changes to data in HBase, to file-based storage. Under normal operations, the WAL is not needed because data changes move from the MemStore to StoreFiles. However, if a RegionServer crashes or becomes unavailable before the MemStore is flushed, the WAL ensures that the changes to the data can be replayed. If writing to the WAL fails, the entire operation to modify the data fails.

HBase uses an implementation of the [WAL](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/wal/WAL.html) interface. Usually, there is only one instance of a WAL per RegionServer. An exception is the RegionServer that is carrying *hbase:meta*; the *meta* table gets its own dedicated WAL. The RegionServer records Puts and Deletes to its WAL, before recording them these Mutations [MemStore](/docs/architecture/regions#memstore) for the affected [Store](/docs/architecture/regions#store).

<Callout type="info" title="The HLog">
  Prior to 2.0, the interface for WALs in HBase was named \`HLog\`. In 0.94, HLog was the name of the
  implementation of the WAL. You will likely find references to the HLog in documentation tailored
  to these older versions.
</Callout>

The WAL resides in HDFS in the */hbase/WALs/* directory, with subdirectories per RegionServer.

For more general information about the concept of write ahead logs, see the Wikipedia [Write-Ahead Log](http://en.wikipedia.org/wiki/Write-ahead_logging) article.

### WAL Providers

In HBase, there are a number of WAL implementations (or 'Providers'). Each is known by a short name label (that unfortunately is not always descriptive). You set the provider in *hbase-site.xml* passing the WAL provider short-name as the value on the *hbase.wal.provider* property (Set the provider for *hbase:meta* using the *hbase.wal.meta\\_provider* property, otherwise it uses the same provider configured by *hbase.wal.provider*).

* *asyncfs*: The **default**. New since hbase-2.0.0 (HBASE-15536, HBASE-14790). This *AsyncFSWAL* provider, as it identifies itself in RegionServer logs, is built on a new non-blocking dfsclient implementation. It is currently resident in the hbase codebase but intent is to move it back up into HDFS itself. WALs edits are written concurrently ("fan-out") style to each of the WAL-block replicas on each DataNode rather than in a chained pipeline as the default client does. Latencies should be better. See [Apache HBase Improvements and Practices at Xiaomi](https://www.slideshare.net/HBaseCon/apache-hbase-improvements-and-practices-at-xiaomi) at slide 14 onward for more detail on implementation.
* *filesystem*: This was the default in hbase-1.x releases. It is built on the blocking *DFSClient* and writes to replicas in classic *DFSCLient* pipeline mode. In logs it identifies as *FSHLog* or *FSHLogProvider*.
* *multiwal*: This provider is made of multiple instances of *asyncfs* or *filesystem*. See the next section for more on *multiwal*.

Look for the lines like the below in the RegionServer log to see which provider is in place (The below shows the default AsyncFSWALProvider):

\`\`\`java
2018-04-02 13:22:37,983 INFO  [regionserver/ve0528:16020] wal.WALFactory: Instantiating WALProvider of type class org.apache.hadoop.hbase.wal.AsyncFSWALProvider
\`\`\`

<Callout type="info">
  As the *AsyncFSWAL* hacks into the internal of DFSClient implementation, it will be easily broken
  by upgrading the hadoop dependencies, even for a simple patch release. So if you do not specify
  the wal provider explicitly, we will first try to use the *asyncfs*, if failed, we will fall back
  to use *filesystem*. And notice that this may not always work, so if you still have problem
  starting HBase due to the problem of starting *AsyncFSWAL*, please specify *filesystem* explicitly
  in the config file.
</Callout>

<Callout type="info">
  EC support has been added to hadoop-3.x, and it is incompatible with WAL as the EC output stream
  does not support hflush/hsync. In order to create a non-EC file in an EC directory, we need to use
  the new builder-based create API for *FileSystem*, but it is only introduced in hadoop-2.9+ and
  for HBase we still need to support hadoop-2.7.x. So please do not enable EC for the WAL directory
  until we find a way to deal with it.
</Callout>

### MultiWAL

With a single WAL per RegionServer, the RegionServer must write to the WAL serially, because HDFS files must be sequential. This causes the WAL to be a performance bottleneck.

HBase 1.0 introduces support MultiWal in [HBASE-5699](https://issues.apache.org/jira/browse/HBASE-5699). MultiWAL allows a RegionServer to write multiple WAL streams in parallel, by using multiple pipelines in the underlying HDFS instance, which increases total throughput during writes. This parallelization is done by partitioning incoming edits by their Region. Thus, the current implementation will not help with increasing the throughput to a single Region.

RegionServers using the original WAL implementation and those using the MultiWAL implementation can each handle recovery of either set of WALs, so a zero-downtime configuration update is possible through a rolling restart.

#### Configure MultiWAL

To configure MultiWAL for a RegionServer, set the value of the property \`hbase.wal.provider\` to \`multiwal\` by pasting in the following XML:

\`\`\`xml
<property>
  <name>hbase.wal.provider</name>
  <value>multiwal</value>
</property>
\`\`\`

Restart the RegionServer for the changes to take effect.

To disable MultiWAL for a RegionServer, unset the property and restart the RegionServer.

### WAL Flushing

TODO (describe).

### WAL Splitting

A RegionServer serves many regions. All of the regions in a region server share the same active WAL file. Each edit in the WAL file includes information about which region it belongs to. When a region is opened, the edits in the WAL file which belong to that region need to be replayed. Therefore, edits in the WAL file must be grouped by region so that particular sets can be replayed to regenerate the data in a particular region. The process of grouping the WAL edits by region is called *log splitting*. It is a critical process for recovering data if a region server fails.

Log splitting is done by the HMaster during cluster start-up or by the ServerShutdownHandler as a region server shuts down. So that consistency is guaranteed, affected regions are unavailable until data is restored. All WAL edits need to be recovered and replayed before a given region can become available again. As a result, regions affected by log splitting are unavailable until the process completes.

#### Procedure: Log Splitting, Step by Step

<Steps>
  <Step>
    ##### The \`/hbase/WALs/HOST,PORT,STARTCODE\` directory is renamed

    Renaming the directory is important because a RegionServer may still be up and accepting requests even if the HMaster thinks it is down. If the RegionServer does not respond immediately and does not heartbeat its ZooKeeper session, the HMaster may interpret this as a RegionServer failure. Renaming the logs directory ensures that existing, valid WAL files which are still in use by an active but busy RegionServer are not written to by accident.

    The new directory is named according to the following pattern:

    \`\`\`text
    /hbase/WALs/HOST,PORT,STARTCODE-splitting
    \`\`\`

    An example of such a renamed directory might look like the following:

    \`\`\`text
    /hbase/WALs/srv.example.com,60020,1254173957298-splitting
    \`\`\`
  </Step>

  <Step>
    ##### Each log file is split, one at a time

    The log splitter reads the log file one edit entry at a time and puts each edit entry into the buffer corresponding to the edit's region. At the same time, the splitter starts several writer threads. Writer threads pick up a corresponding buffer and write the edit entries in the buffer to a temporary recovered edit file. The temporary edit file is stored to disk with the following naming pattern:

    \`\`\`text
    /hbase/TABLE_NAME/REGION_ID/recovered.edits/.temp
    \`\`\`

    This file is used to store all the edits in the WAL log for this region. After log splitting completes, the *.temp* file is renamed to the sequence ID of the first log written to the file.

    To determine whether all edits have been written, the sequence ID is compared to the sequence of the last edit that was written to the HFile. If the sequence of the last edit is greater than or equal to the sequence ID included in the file name, it is clear that all writes from the edit file have been completed.
  </Step>

  <Step>
    ##### After log splitting is complete, each affected region is assigned to a RegionServer

    When the region is opened, the *recovered.edits* folder is checked for recovered edits files. If any such files are present, they are replayed by reading the edits and saving them to the MemStore. After all edit files are replayed, the contents of the MemStore are written to disk (HFile) and the edit files are deleted.
  </Step>
</Steps>

#### Handling of Errors During Log Splitting

If you set the \`hbase.hlog.split.skip.errors\` option to \`true\`, errors are treated as follows:

* Any error encountered during splitting will be logged.
* The problematic WAL log will be moved into the *.corrupt* directory under the hbase \`rootdir\`,
* Processing of the WAL will continue

If the \`hbase.hlog.split.skip.errors\` option is set to \`false\`, the default, the exception will be propagated and the split will be logged as failed. See [HBASE-2958 When hbase.hlog.split.skip.errors is set to false, we fail the split but that's it](https://issues.apache.org/jira/browse/HBASE-2958). We need to do more than just fail split if this flag is set.

#### How EOFExceptions are treated when splitting a crashed RegionServer's WALs

If an EOFException occurs while splitting logs, the split proceeds even when \`hbase.hlog.split.skip.errors\` is set to \`false\`. An EOFException while reading the last log in the set of files to split is likely, because the RegionServer was likely in the process of writing a record at the time of a crash. For background, see [HBASE-2643 Figure how to deal with eof splitting logs](https://issues.apache.org/jira/browse/HBASE-2643)

#### Performance Improvements during Log Splitting

WAL log splitting and recovery can be resource intensive and take a long time, depending on the number of RegionServers involved in the crash and the size of the regions. [Distributed log splitting](/docs/architecture/regionserver#enabling-or-disabling-distributed-log-splitting) was developed to improve performance during log splitting.

#### Enabling or Disabling Distributed Log Splitting

Distributed log processing is enabled by default since HBase 0.92. The setting is controlled by the \`hbase.master.distributed.log.splitting\` property, which can be set to \`true\` or \`false\`, but defaults to \`true\`.

### WAL splitting based on procedureV2

After HBASE-20610, we introduce a new way to do WAL splitting coordination by procedureV2 framework. This can simplify the process of WAL splitting and no need to connect zookeeper any more.

#### Background \\[!toc]

Currently, splitting WAL processes are coordinated by zookeeper. Each region server are trying to grab tasks from zookeeper. And the burden becomes heavier when the number of region server increase.

#### Implementation on Master side \\[!toc]

During ServerCrashProcedure, SplitWALManager will create one SplitWALProcedure for each WAL file which should be split. Then each SplitWALProcedure will spawn a SplitWalRemoteProcedure to send the request to region server. SplitWALProcedure is a StateMachineProcedure and here is the state transfer diagram.

<img alt="WAL splitting" src={__img1} placeholder="blur" />

#### Implementation on Region Server side \\[!toc]

Region Server will receive a SplitWALCallable and execute it, which is much more straightforward than before. It will return null if success and return exception if there is any error.

#### Performance \\[!toc]

According to tests on a cluster which has 5 regionserver and 1 master. procedureV2 coordinated WAL splitting has a better performance than ZK coordinated WAL splitting no master when restarting the whole cluster or one region server crashing.

#### Enable this feature \\[!toc]

To enable this feature, first we should ensure our package of HBase already contains these code. If not, please upgrade the package of HBase cluster without any configuration change first. Then change configuration 'hbase.split.wal.zk.coordinated' to false. Rolling upgrade the master with new configuration. Now WAL splitting are handled by our new implementation. But region server are still trying to grab tasks from zookeeper, we can rolling upgrade the region servers with the new configuration to stop that.

* Steps as follows:
  * Upgrade whole cluster to get the new Implementation.
  * Upgrade Master with new configuration 'hbase.split.wal.zk.coordinated'=false.
  * Upgrade region server to stop grab tasks from zookeeper.

### WAL Compression

The content of the WAL can be compressed using LRU Dictionary compression. This can be used to speed up WAL replication to different datanodes. The dictionary can store up to 2<sup>15</sup> elements; eviction starts after this number is exceeded.

To enable WAL compression, set the \`hbase.regionserver.wal.enablecompression\` property to \`true\`. The default value for this property is \`false\`. By default, WAL tag compression is turned on when WAL compression is enabled. You can turn off WAL tag compression by setting the \`hbase.regionserver.wal.tags.enablecompression\` property to 'false'.

A possible downside to WAL compression is that we lose more data from the last block in the WAL if it is ill-terminated mid-write. If entries in this last block were added with new dictionary entries but we failed persist the amended dictionary because of an abrupt termination, a read of this last block may not be able to resolve last-written entries.

### Durability

It is possible to set *durability* on each Mutation or on a Table basis. Options include:

* *SKIP\\_WAL*: Do not write Mutations to the WAL (See the next section, [Disabling the WAL](/docs/architecture/regionserver#disabling-the-wal)).
* *ASYNC\\_WAL*: Write the WAL asynchronously; do not hold-up clients waiting on the sync of their write to the filesystem but return immediately. The edit becomes visible. Meanwhile, in the background, the Mutation will be flushed to the WAL at some time later. This option currently may lose data. See HBASE-16689.
* *SYNC\\_WAL*: The **default**. Each edit is sync'd to HDFS before we return success to the client.
* *FSYNC\\_WAL*: Each edit is fsync'd to HDFS and the filesystem before we return success to the client.

Do not confuse the *ASYNC\\_WAL* option on a Mutation or Table with the *AsyncFSWAL* writer; they are distinct options unfortunately closely named

### Custom WAL Directory

HBASE-17437 added support for specifying a WAL directory outside the HBase root directory or even in a different FileSystem since 1.3.3/2.0+. Some FileSystems (such as Amazon S3) don't support append or consistent writes, in such scenario WAL directory needs to be configured in a different FileSystem to avoid loss of writes.

Following configurations are added to accomplish this:

1. \`hbase.wal.dir\`\\
   This defines where the root WAL directory is located, could be on a different FileSystem than the root directory. WAL directory can not be set to a subdirectory of the root directory. The default value of this is the root directory if unset.
2. \`hbase.rootdir.perms\`\\
   Configures FileSystem permissions to set on the root directory. This is '700' by default.
3. \`hbase.wal.dir.perms\`\\
   Configures FileSystem permissions to set on the WAL directory FileSystem. This is '700' by default.

<Callout type="info">
  While migrating to custom WAL dir (outside the HBase root directory or a different FileSystem)
  existing WAL files must be copied manually to new WAL dir, otherwise it may lead to data
  loss/inconsistency as HMaster has no information about previous WAL directory.
</Callout>

### Disabling the WAL

It is possible to disable the WAL, to improve performance in certain specific situations. However, disabling the WAL puts your data at risk. The only situation where this is recommended is during a bulk load. This is because, in the event of a problem, the bulk load can be re-run with no risk of data loss.

The WAL is disabled by calling the HBase client field \`Mutation.writeToWAL(false)\`. Use the \`Mutation.setDurability(Durability.SKIP_WAL)\` and Mutation.getDurability() methods to set and get the field's value. There is no way to disable the WAL for only a specific table.

<Callout type="warn">
  If you disable the WAL for anything other than bulk loads, your data is at risk.
</Callout>
`,p={title:"RegionServer",description:"HBase RegionServer implementation, interfaces, read/write paths, block cache, memstore management, and performance tuning."},u=[{href:"/docs/architecture/hdfs#hdfs-datanode"},{href:"https://blogs.apache.org/hbase/entry/coprocessor_introduction"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CacheConfig.html"},{href:"/docs/performance#prefetch-option-for-blockcache"},{href:"https://issues.apache.org/jira/browse/HBASE-9857"},{href:"http://en.wikipedia.org/wiki/Working_set_size"},{href:"/docs/regionserver-sizing#try-to-minimize-row-and-column-sizes"},{href:"/docs/troubleshooting#jvm-garbage-collection-logs"},{href:"https://issues.apache.org/jira/browse/HBASE-4683"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CombinedBlockCache.html"},{href:"http://www.n10k.com/blog/blockcache-101/"},{href:"https://web.archive.org/web/20231109025243/http://people.apache.org/~stack/bc/"},{href:"/docs/offheap-read-write#offheap-read-path"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CacheConfig.html"},{href:"https://issues.apache.org/jira/browse/HBASE-10641"},{href:"/docs/performance#leveraging-local-data"},{href:"https://web.archive.org/web/20170907032911/http://terracotta.org/documentation/4.0/bigmemorygo/configuration/storage-options"},{href:"https://issues.apache.org/jira/browse/HBASE-28463"},{href:"/docs/architecture/regionserver#using-cell-timestamps-for-time-based-priority"},{href:"/docs/architecture/regionserver#using-custom-cell-qualifiers-for-time-based-priority"},{href:"/docs/architecture/regionserver#using-a-custom-value-provider-for-time-based-priority"},{href:"https://docs.google.com/document/d/1Qd3kvZodBDxHTFCIRtoePgMbvyuUSxeydi2SEWQFQro/edit?tab=t.0#heading=h.gjdgxs"},{href:"https://docs.google.com/document/d/1uBGIO9IQ-FbSrE5dnUMRtQS23NbCbAmRVDkAOADcU_E/edit?tab=t.0"},{href:"https://issues.apache.org/jira/browse/HBASE-11331"},{href:"https://issues.apache.org/jira/browse/HBASE-27313"},{href:"https://issues.apache.org/jira/browse/HBASE-27999"},{href:"/docs/architecture/regions#compaction"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/Reference.html"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/wal/WAL.html"},{href:"/docs/architecture/regions#memstore"},{href:"/docs/architecture/regions#store"},{href:"http://en.wikipedia.org/wiki/Write-ahead_logging"},{href:"https://www.slideshare.net/HBaseCon/apache-hbase-improvements-and-practices-at-xiaomi"},{href:"https://issues.apache.org/jira/browse/HBASE-5699"},{href:"https://issues.apache.org/jira/browse/HBASE-2958"},{href:"https://issues.apache.org/jira/browse/HBASE-2643"},{href:"/docs/architecture/regionserver#enabling-or-disabling-distributed-log-splitting"},{href:"/docs/architecture/regionserver#disabling-the-wal"}],m={contents:[{heading:void 0,content:"HRegionServer is the RegionServer implementation. It is responsible for serving and managing regions. In a distributed cluster, a RegionServer runs on a DataNode."},{heading:"architecture-regionserver-interface",content:"The methods exposed by HRegionRegionInterface contain both data-oriented and region-maintenance methods:"},{heading:"architecture-regionserver-interface",content:"Data (get, put, delete, next, etc.)"},{heading:"architecture-regionserver-interface",content:"Region (splitRegion, compactRegion, etc.) For example, when the Admin method majorCompact is invoked on a table, the client is actually iterating through all regions for the specified table and requesting a major compaction directly to each region."},{heading:"architecture-regionserver-processes",content:"The RegionServer runs a variety of background threads:"},{heading:"compactsplitthread",content:"Checks for splits and handle minor compactions."},{heading:"majorcompactionchecker",content:"Checks for major compactions."},{heading:"memstoreflusher",content:"Periodically flushes in-memory writes in the MemStore to StoreFiles."},{heading:"logroller",content:"Periodically checks the RegionServer's WAL."},{heading:"coprocessors",content:"Coprocessors were added in 0.92. There is a thorough Blog Overview of CoProcessors posted. Documentation will eventually move to this reference guide, but the blog is the most current information available at this time."},{heading:"architecture-regionserver-block-cache",content:"HBase provides two different BlockCache implementations to cache data read from HDFS: the default on-heap LruBlockCache and the BucketCache, which is (usually) off-heap. This section discusses benefits and drawbacks of each implementation, how to choose the appropriate option, and configuration options for each."},{heading:"architecture-regionserver-block-cache",content:"type: info"},{heading:"architecture-regionserver-block-cache",content:"title: Block Cache Reporting: UI"},{heading:"architecture-regionserver-block-cache",content:`See the RegionServer UI for detail on caching deploy. See configurations, sizings, current usage,
time-in-the-cache, and even detail on block counts and types.`},{heading:"cache-choices",content:"LruBlockCache is the original implementation, and is entirely within the Java heap. BucketCache is optional and mainly intended for keeping block cache data off-heap, although BucketCache can also be a file-backed cache. In file-backed we can either use it in the file mode or the mmaped mode. We also have pmem mode where the bucket cache resides on the persistent memory device."},{heading:"cache-choices",content:'When you enable BucketCache, you are enabling a two tier caching system. We used to describe the tiers as "L1" and "L2" but have deprecated this terminology as of hbase-2.0.0. The "L1" cache referred to an instance of LruBlockCache and "L2" to an off-heap BucketCache. Instead, when BucketCache is enabled, all DATA blocks are kept in the BucketCache tier and meta blocks — INDEX and BLOOM blocks — are on-heap in the LruBlockCache. Management of these two tiers and the policy that dictates how blocks move between them is done by CombinedBlockCache.'},{heading:"general-cache-configurations",content:"Apart from the cache implementation itself, you can set some general configuration options to control how the cache performs. See CacheConfig. After setting any of these options, restart or rolling restart your cluster for the configuration to take effect. Check logs for errors or unexpected behavior."},{heading:"general-cache-configurations",content:"See also Prefetch Option for Blockcache, which discusses a new option introduced in HBASE-9857."},{heading:"lrublockcache-design",content:"The LruBlockCache is an LRU cache that contains three levels of block priority to allow for scan-resistance and in-memory ColumnFamilies:"},{heading:"lrublockcache-design",content:"Single access priority: The first time a block is loaded from HDFS it normally has this priority and it will be part of the first group to be considered during evictions. The advantage is that scanned blocks are more likely to get evicted than blocks that are getting more usage."},{heading:"lrublockcache-design",content:"Multi access priority: If a block in the previous priority group is accessed again, it upgrades to this priority. It is thus part of the second group considered during evictions."},{heading:"lrublockcache-design",content:`In-memory access priority: If the block's family was configured to be "in-memory", it will be part of this priority disregarding the number of times it was accessed. Catalog tables are configured like this. This group is the last one considered during evictions.To mark a column family as in-memory, call`},{heading:"lrublockcache-design",content:"if creating a table from java, or set IN_MEMORY ⇒ true when creating or altering a table in the shell: e.g."},{heading:"lrublockcache-design",content:"For more information, see the LruBlockCache source"},{heading:"lrublockcache-usage",content:`Block caching is enabled by default for all the user tables which means that any read operation will load the LRU cache. This might be good for a large number of use cases, but further tunings are usually required in order to achieve better performance. An important concept is the working set size, or WSS, which is: "the amount of memory needed to compute the answer to a problem". For a website, this would be the data that's needed to answer the queries over a short amount of time.`},{heading:"lrublockcache-usage",content:"The way to calculate how much memory is available in HBase for caching is:"},{heading:"lrublockcache-usage",content:"The default value for the block cache is 0.4 which represents 40% of the available heap. The last value (99%) is the default acceptable loading factor in the LRU cache after which eviction is started. The reason it is included in this equation is that it would be unrealistic to say that it is possible to use 100% of the available memory since this would make the process blocking from the point where it loads new blocks. Here are some examples:"},{heading:"lrublockcache-usage",content:"One region server with the heap size set to 1 GB and the default block cache size will have 405 MB of block cache available."},{heading:"lrublockcache-usage",content:"20 region servers with the heap size set to 8 GB and a default block cache size will have 63.3 GB of block cache."},{heading:"lrublockcache-usage",content:"100 region servers with the heap size set to 24 GB and a block cache size of 0.5 will have about 1.16 TB of block cache."},{heading:"lrublockcache-usage",content:"Your data is not the only resident of the block cache. Here are others that you may have to take into account:"},{heading:"lrublockcache-usage",content:"Catalog TablesThe hbase:meta table is forced into the block cache and have the in-memory priority which means that they are harder to evict."},{heading:"lrublockcache-usage",content:"type: info"},{heading:"lrublockcache-usage",content:"The hbase:meta tables can occupy a few MBs depending on the number of regions."},{heading:"lrublockcache-usage",content:"HFiles IndexesAn HFile is the file format that HBase uses to store data in HDFS. It contains a multi-layered index which allows HBase to seek the data without having to read the whole file. The size of those indexes is a factor of the block size (64KB by default), the size of your keys and the amount of data you are storing. For big data sets it's not unusual to see numbers around 1GB per region server, although not all of it will be in cache because the LRU will evict indexes that aren't used."},{heading:"lrublockcache-usage",content:"KeysThe values that are stored are only half the picture, since each value is stored along with its keys (row key, family qualifier, and timestamp). See Try to minimize row and column sizes."},{heading:"lrublockcache-usage",content:"Bloom FiltersJust like the HFile indexes, those data structures (when enabled) are stored in the LRU."},{heading:"lrublockcache-usage",content:"Currently the recommended way to measure HFile indexes and bloom filters sizes is to look at the region server web UI and checkout the relevant metrics. For keys, sampling can be done by using the HFile command line tool and look for the average key size metric. Since HBase 0.98.3, you can view details on BlockCache stats and metrics in a special Block Cache section in the UI. As of HBase 2.4.14, you can estimate HFile indexes and bloom filters vs other DATA blocks using blockCacheCount and blockCacheDataBlockCount in JMX. The formula (blockCacheCount - blockCacheDataBlockCount) * blockSize will give you an estimate which can be useful when trying to enable the BucketCache. You should make sure the post-BucketCache config gives enough memory to the on-heap LRU cache to hold at least the same number of non-DATA blocks from pre-BucketCache. Once BucketCache is enabled, the L1 metrics like l1CacheSize, l1CacheCount, and l1CacheEvictionCount can help you further tune the size."},{heading:"lrublockcache-usage",content:"It's generally bad to use block caching when the WSS doesn't fit in memory. This is the case when you have for example 40GB available across all your region servers' block caches but you need to process 1TB of data. One of the reasons is that the churn generated by the evictions will trigger more garbage collections unnecessarily. Here are two use cases:"},{heading:"lrublockcache-usage",content:"Fully random reading pattern: This is a case where you almost never access the same row twice within a short amount of time such that the chance of hitting a cached block is close to 0. Setting block caching on such a table is a waste of memory and CPU cycles, more so that it will generate more garbage to pick up by the JVM. For more information on monitoring GC, see JVM Garbage Collection Logs."},{heading:"lrublockcache-usage",content:"Mapping a table: In a typical MapReduce job that takes a table in input, every row will be read only once so there's no need to put them into the block cache. The Scan object has the option of turning this off via the setCacheBlocks method (set it to false). You can still keep block caching turned on on this table if you need fast random read access. An example would be counting the number of rows in a table that serves live traffic, caching every block of that table would create massive churn and would surely evict data that's currently in use."},{heading:"caching-meta-blocks-only-data-blocks-in-fscache",content:"An interesting setup is one where we cache META blocks only and we read DATA blocks in on each access. If the DATA blocks fit inside fscache, this alternative may make sense when access is completely random across a very large dataset. To enable this setup, alter your table and for each column family set BLOCKCACHE ⇒ 'false'. You are 'disabling' the BlockCache for this column family only. You can never disable the caching of META blocks. Since HBASE-4683 Always cache index and bloom blocks, we will cache META blocks even if the BlockCache is disabled."},{heading:"how-to-enable-bucketcache",content:"The usual deployment of BucketCache is via a managing class that sets up two caching tiers: an on-heap cache implemented by LruBlockCache and a second cache implemented with BucketCache. The managing class is CombinedBlockCache by default. The previous link describes the caching 'policy' implemented by CombinedBlockCache. In short, it works by keeping meta blocks — INDEX and BLOOM in the on-heap LruBlockCache tier — and DATA blocks are kept in the BucketCache tier."},{heading:"how-to-enable-bucketcache",content:"Pre-hbase-2.0.0 versions"},{heading:"how-to-enable-bucketcache",content:"Fetching will always be slower when fetching from BucketCache in pre-hbase-2.0.0, as compared to the native on-heap LruBlockCache. However, latencies tend to be less erratic across time, because there is less garbage collection when you use BucketCache since it is managing BlockCache allocations, not the GC. If the BucketCache is deployed in off-heap mode, this memory is not managed by the GC at all. This is why you'd use BucketCache in pre-2.0.0, so your latencies are less erratic, to mitigate GCs and heap fragmentation, and so you can safely use more memory. See Nick Dimiduk's BlockCache 101 for comparisons running on-heap vs off-heap tests. Also see Comparing BlockCache Deploys which finds that if your dataset fits inside your LruBlockCache deploy, use it otherwise if you are experiencing cache churn (or you want your cache to exist beyond the vagaries of java GC), use BucketCache."},{heading:"how-to-enable-bucketcache",content:"In pre-2.0.0, one can configure the BucketCache so it receives the victim of an LruBlockCache eviction. All Data and index blocks are cached in L1 first. When eviction happens from L1, the blocks (or victims) will get moved to L2. Set cacheDataInL1 via (HColumnDescriptor.setCacheDataInL1(true) or in the shell, creating or amending column families setting CACHE_DATA_IN_L1 to true: e.g."},{heading:"how-to-enable-bucketcache",content:"hbase-2.0.0+ versions"},{heading:"how-to-enable-bucketcache",content:"HBASE-11425 changed the HBase read path so it could hold the read-data off-heap avoiding copying of cached data on to the java heap. See Offheap read-path. In hbase-2.0.0, off-heap latencies approach those of on-heap cache latencies with the added benefit of NOT provoking GC."},{heading:"how-to-enable-bucketcache",content:"From HBase 2.0.0 onwards, the notions of L1 and L2 have been deprecated. When BucketCache is turned on, the DATA blocks will always go to BucketCache and INDEX/BLOOM blocks go to on heap LRUBlockCache. cacheDataInL1 support has been removed."},{heading:"bucketcache-deploy-modes",content:"The BucketCache Block Cache can be deployed offheap, file or mmaped file mode."},{heading:"bucketcache-deploy-modes",content:"You set which via the hbase.bucketcache.ioengine setting. Setting it to offheap will have BucketCache make its allocations off-heap, and an ioengine setting of file:PATH_TO_FILE will direct BucketCache to use file caching (Useful in particular if you have some fast I/O attached to the box such as SSDs). From 2.0.0, it is possible to have more than one file backing the BucketCache. This is very useful especially when the Cache size requirement is high. For multiple backing files, configure ioengine as files:PATH_TO_FILE1,PATH_TO_FILE2,PATH_TO_FILE3. BucketCache can be configured to use an mmapped file also. Configure ioengine as mmap:PATH_TO_FILE for this."},{heading:"bucketcache-deploy-modes",content:"It is possible to deploy a tiered setup where we bypass the CombinedBlockCache policy and have BucketCache working as a strict L2 cache to the L1 LruBlockCache. For such a setup, set hbase.bucketcache.combinedcache.enabled to false. In this mode, on eviction from L1, blocks go to L2. When a block is cached, it is cached first in L1. When we go to look for a cached block, we look first in L1 and if none found, then search L2. Let us call this deploy format, Raw L1+L2. NOTE: This L1+L2 mode is removed from 2.0.0. When BucketCache is used, it will be strictly the DATA cache and the LruBlockCache will cache INDEX/META blocks."},{heading:"bucketcache-deploy-modes",content:"Other BucketCache configs include: specifying a location to persist cache to across restarts, how many threads to use writing the cache, etc. See the CacheConfig.html class for configuration options and descriptions."},{heading:"bucketcache-deploy-modes",content:"To check it enabled, look for the log line describing cache setup; it will detail how BucketCache has been deployed. Also see the UI. It will detail the cache tiering and their configuration."},{heading:"bucketcache-example-configuration",content:"This sample provides a configuration for a 4 GB off-heap BucketCache with a 1 GB on-heap cache."},{heading:"bucketcache-example-configuration",content:"Configuration is performed on the RegionServer."},{heading:"bucketcache-example-configuration",content:"Setting hbase.bucketcache.ioengine and hbase.bucketcache.size > 0 enables CombinedBlockCache. Let us presume that the RegionServer has been set to run with a 5G heap: i.e. HBASE_HEAPSIZE=5g."},{heading:"bucketcache-example-configuration",content:"First, edit the RegionServer's hbase-env.sh and set HBASE_OFFHEAPSIZE to a value greater than the off-heap size wanted, in this case, 4 GB (expressed as 4G). Let's set it to 5G. That'll be 4G for our off-heap cache and 1G for any other uses of off-heap memory (there are other users of off-heap memory other than BlockCache; e.g. DFSClient in RegionServer can make use of off-heap memory). See Direct Memory Usage In HBase below."},{heading:"bucketcache-example-configuration",content:"Next, add the following configuration to the RegionServer's hbase-site.xml."},{heading:"bucketcache-example-configuration",content:"Restart or rolling restart your cluster, and check the logs for any issues."},{heading:"bucketcache-example-configuration",content:"In the above, we set the BucketCache to be 4G. We configured the on-heap LruBlockCache have 20% (0.2) of the RegionServer's heap size (0.2 * 5G = 1G). In other words, you configure the L1 LruBlockCache as you would normally (as if there were no L2 cache present)."},{heading:"bucketcache-example-configuration",content:"HBASE-10641 introduced the ability to configure multiple sizes for the buckets of the BucketCache, in HBase 0.98 and newer. To configurable multiple bucket sizes, configure the new property hbase.bucketcache.bucket.sizes to a comma-separated list of block sizes, ordered from smallest to largest, with no spaces. The goal is to optimize the bucket sizes based on your data access patterns. The following example configures buckets of size 4096 and 8192."},{heading:"bucketcache-example-configuration",content:"type: info"},{heading:"bucketcache-example-configuration",content:"title: Direct Memory Usage In HBase"},{heading:"bucketcache-example-configuration",content:"The default maximum direct memory varies by JVM. Traditionally it is 64M or some relation to allocated heap size (-Xmx) or no limit at all (JDK7 apparently). HBase servers use direct memory, in particular short-circuit reading (See Leveraging local data), the hosted DFSClient will allocate direct memory buffers. How much the DFSClient uses is not easy to quantify; it is the number of open HFiles * hbase.dfs.client.read.shortcircuit.buffer.size where hbase.dfs.client.read.shortcircuit.buffer.size is set to 128k in HBase — see hbase-default.xml default configurations. If you do off-heap block caching, you'll be making use of direct memory. The RPCServer uses a ByteBuffer pool. From 2.0.0, these buffers are off-heap ByteBuffers. Starting your JVM, make sure the -XX:MaxDirectMemorySize setting in conf/hbase-env.sh considers off-heap BlockCache (hbase.bucketcache.size), DFSClient usage, RPC side ByteBufferPool max size. This has to be bit higher than sum of off heap BlockCache size and max ByteBufferPool size. Allocating an extra of 1-2 GB for the max direct memory size has worked in tests. Direct memory, which is part of the Java process heap, is separate from the object heap allocated by -Xmx. The value allocated by MaxDirectMemorySize must not exceed physical RAM, and is likely to be less than the total available RAM due to other memory requirements and system constraints."},{heading:"bucketcache-example-configuration",content:"You can see how much memory — on-heap and off-heap/direct — a RegionServer is configured to use and how much it is using at any one time by looking at the Server Metrics: Memory tab in the UI. It can also be gotten via JMX. In particular the direct memory currently used by the server can be found on the java.nio.type=BufferPool,name=direct bean. Terracotta has a good write up on using off-heap memory in Java. It is for their product BigMemory but a lot of the issues noted apply in general to any attempt at going off-heap. Check it out."},{heading:"bucketcache-example-configuration",content:"type: info"},{heading:"bucketcache-example-configuration",content:"title: hbase.bucketcache.percentage.in.combinedcache"},{heading:"bucketcache-example-configuration",content:"This is a pre-HBase 1.0 configuration removed because it was confusing. It was a float that you would set to some value between 0.0 and 1.0. Its default was 0.9. If the deploy was using CombinedBlockCache, then the LruBlockCache L1 size was calculated to be (1 - hbase.bucketcache.percentage.in.combinedcache) * size-of-bucketcache and the BucketCache size was hbase.bucketcache.percentage.in.combinedcache * size-of-bucket-cache. where size-of-bucket-cache itself is EITHER the value of the configuration hbase.bucketcache.size IF it was specified as Megabytes OR hbase.bucketcache.size * -XX:MaxDirectMemorySize if hbase.bucketcache.size is between 0 and 1.0."},{heading:"bucketcache-example-configuration",content:"In 1.0, it should be more straight-forward. Onheap LruBlockCache size is set as a fraction of java heap using hfile.block.cache.size setting (not the best name) and BucketCache is set as above in absolute Megabytes."},{heading:"time-based-priority-for-bucketcache",content:"HBASE-28463 introduced time based priority for blocks in BucketCache. It allows for defining an age threshold at individual column families' configuration, whereby blocks older than this configured threshold would be targeted first for eviction."},{heading:"time-based-priority-for-bucketcache",content:"Blocks from column families that don't define the age threshold wouldn't be evaluated by the time based priority, and would only be evicted following the LRU eviction logic."},{heading:"time-based-priority-for-bucketcache",content:'This feature is mostly useful for use cases where most recent data is more frequently accessed, and therefore should get higher priority in the cache. Configuring Time Based Priority with the "age" of most accessed data would then give a finer control over blocks allocation in the BucketCache than the built-in LRU eviction logic.'},{heading:"time-based-priority-for-bucketcache",content:"Time Based Priority for BucketCache provides three different strategies for defining data age:"},{heading:"time-based-priority-for-bucketcache",content:"Cell timestamps: Uses the timestamp portion of HBase cells for comparing the data age."},{heading:"time-based-priority-for-bucketcache",content:"Custom cell qualifiers: Uses a custom-defined date qualifier for comparing the data age. It uses that value to tier the entire row containing the given qualifier value. This requires that the custom qualifier be a valid Java long timestamp."},{heading:"time-based-priority-for-bucketcache",content:"Custom value provider: Allows for defining a pluggable implementation that contains the logic for identifying the date value to be used for comparison. This also provides additional flexibility for different use cases that might have the date stored in other formats or embedded with other data in various portions of a given row."},{heading:"time-based-priority-for-bucketcache",content:"For use cases where priority is determined by the order of record ingestion in HBase (with the most recent being the most relevant), the built-in cell timestamp offers the most convenient and efficient method for configuring age-based priority. See Using Cell timestamps for Time Based Priority."},{heading:"time-based-priority-for-bucketcache",content:"Some applications may utilize a custom date column to define the priority of table records. In such instances, a custom cell qualifier-based priority is advisable. See Using Custom Cell Qualifiers for Time Based Priority."},{heading:"time-based-priority-for-bucketcache",content:"Finally, more intricate schemas may incorporate domain-specific logic for defining the age of each record. The custom value provider facilitates the integration of custom code to implement the appropriate parsing of the date value that should be used for the priority comparison. See Using a Custom value provider for Time Based Priority."},{heading:"time-based-priority-for-bucketcache",content:"With Time Based Priority for BucketCache, blocks age is evaluated when deciding if a block should be cached (i.e. during reads, writes, compaction and prefetch), as well as during the cache freeSpace run (mass eviction), prior to executing the LRU logic."},{heading:"time-based-priority-for-bucketcache",content:`Because blocks don't hold any specific meta information other than type, it's necessary to group blocks of the same "age group" on separate files, using specialized compaction implementations (see more details in the configuration section below). The time range of all blocks in each file is then appended at the file meta info section, and is used for evaluating the age of blocks that should be considered in the Time Based Priority logic.`},{heading:"configuring-time-based-priority-for-bucketcache",content:"Finding the age of each block involves an extra overhead, therefore the feature is disabled by default at a global configuration level."},{heading:"configuring-time-based-priority-for-bucketcache",content:"To enable it, the following configuration should be set on RegionServers' hbase-site.xml:"},{heading:"configuring-time-based-priority-for-bucketcache",content:"Once enabled globally, it's necessary to define the desired strategy-specific settings at the individual column family level."},{heading:"using-cell-timestamps-for-time-based-priority",content:"This strategy is the most efficient to run, as it uses the timestamp portion of each cell containing the data for comparing the age of blocks. It requires DateTieredCompaction for splitting the blocks into separate files according to blocks' ages."},{heading:"using-cell-timestamps-for-time-based-priority",content:"The example below sets the hot age threshold to one week (in milliseconds) for the column family 'cf1' in table 'orders':"},{heading:"using-cell-timestamps-for-time-based-priority",content:"type: info"},{heading:"using-cell-timestamps-for-time-based-priority",content:"title: Date Tiered Compaction specific tunings"},{heading:"using-cell-timestamps-for-time-based-priority",content:"In the example above, the properties governing the number of windows and period of each window in the date tiered compaction were not set. With the default settings, the compaction will create initially four windows of six hours, then four windows of one day each, then another four windows of four days each and so on until the minimum timestamp among the selected files is covered. This can create a large number of files, therefore, additional changes to the 'hbase.hstore.blockingStoreFiles', 'hbase.hstore.compaction.min' and 'hbase.hstore.compaction.max' are recommended."},{heading:"using-cell-timestamps-for-time-based-priority",content:"Alternatively, consider adjusting the initial window size to the same as the hot age threshold, and two windows only per tier:"},{heading:"using-custom-cell-qualifiers-for-time-based-priority",content:'This strategy uses a new compaction implementation designed for Time Based Priority. It extends date tiered compaction, but instead of producing multiple tiers of various time windows, it simply splits files into two groups: the "cold" group, where all blocks are older than the defined threshold age, and the "hot" group, where all blocks are newer than the threshold age.'},{heading:"using-custom-cell-qualifiers-for-time-based-priority",content:"The example below defines a cell qualifier 'event_date' to be used for comparing the age of blocks within the custom cell qualifier strategy:"},{heading:"using-custom-cell-qualifiers-for-time-based-priority",content:"type: info"},{heading:"using-custom-cell-qualifiers-for-time-based-priority",content:"title: Time Based Priority x Compaction Age Threshold Configurations"},{heading:"using-custom-cell-qualifiers-for-time-based-priority",content:`Note that there are two different configurations for defining the hot age threshold. This is
because the Time Based Priority enforcer operates independently of the compaction implementation.`},{heading:"using-a-custom-value-provider-for-time-based-priority",content:"It's also possible to hook in domain-specific logic for defining the data age of each row to be used for comparing blocks priorities. The Custom Time Based Priority framework defines the CustomTieredCompactor.TieringValueProvider interface, which can be implemented to provide the specific date value to be used by compaction for grouping the blocks according to the threshold age."},{heading:"using-a-custom-value-provider-for-time-based-priority",content:'In the following example, the RowKeyPortionTieringValueProvider implements the getTieringValue method. This method parses the date from a segment of the row key value, specifically between positions 14 and 29, using the "yyyyMMddHHmmss" format. The parsed date is then returned as a long timestamp, which is then used by custom tiered compaction to group the blocks based on the defined hot age threshold:'},{heading:"using-a-custom-value-provider-for-time-based-priority",content:"The Tiering Value Provider above can then be configured for Time Based Priority as follows:"},{heading:"using-a-custom-value-provider-for-time-based-priority",content:"type: info"},{heading:"using-a-custom-value-provider-for-time-based-priority",content:`Upon enabling Custom Time Based Priority (either the custom qualifier or custom value provider) in
the column family configuration, it is imperative that major compaction be executed twice on the
specified tables to ensure the effective application of the newly configured priorities within the
bucket cache.`},{heading:"using-a-custom-value-provider-for-time-based-priority",content:"type: info"},{heading:"using-a-custom-value-provider-for-time-based-priority",content:"Time Based Priority was originally implemented with the cell timestamp strategy only. The original design covering cell timestamp based strategy is available here."},{heading:"using-a-custom-value-provider-for-time-based-priority",content:"The second phase including the two custom strategies mentioned above is detailed in this separate design doc."},{heading:"compressed-blockcache",content:"HBASE-11331 introduced lazy BlockCache decompression, more simply referred to as compressed BlockCache. When compressed BlockCache is enabled data and encoded data blocks are cached in the BlockCache in their on-disk format, rather than being decompressed and decrypted before caching."},{heading:"compressed-blockcache",content:"For a RegionServer hosting more data than can fit into cache, enabling this feature with SNAPPY compression has been shown to result in 50% increase in throughput and 30% improvement in mean latency while, increasing garbage collection by 80% and increasing overall CPU load by 2%. See HBASE-11331 for more details about how performance was measured and achieved. For a RegionServer hosting data that can comfortably fit into cache, or if your workload is sensitive to extra CPU or garbage-collection load, you may receive less benefit."},{heading:"compressed-blockcache",content:"The compressed BlockCache is disabled by default. To enable it, set hbase.block.data.cachecompressed to true in hbase-site.xml on all RegionServers."},{heading:"cache-aware-load-balancer",content:"Depending on the data size and the configured cache size, the cache warm up can take anywhere from a few minutes to a few hours. This becomes even more critical for HBase deployments over cloud storage, where compute is separated from storage. Doing this everytime the region server starts can be a very expensive process. To eliminate this, HBASE-27313 implemented the cache persistence feature where the region servers periodically persist the blocks cached in the bucket cache. This persisted information is then used to resurrect the cache in the event of a region server restart because of normal restart or crash."},{heading:"cache-aware-load-balancer",content:"HBASE-27999 implements the cache aware load balancer, which adds to the load balancer the ability to consider the cache allocation of each region on region servers when calculating a new assignment plan, using the region/region server cache allocation information reported by region servers to calculate the percentage of HFiles cached for each region on the hosting server. This information is then used by the balancer as a factor when deciding on an optimal, new assignment plan."},{heading:"cache-aware-load-balancer",content:"The master node captures the caching information from all the region servers and uses this information to decide on new region assignments while ensuring a minimal impact on the current cache allocation. A region is assigned to the region server where it has a better cache ratio as compared to the region server where it is currently hosted."},{heading:"cache-aware-load-balancer",content:"The CacheAwareLoadBalancer uses two cost elements for deciding the region allocation. These are described below:"},{heading:"cache-aware-load-balancer",content:"Cache CostThe cache cost is calculated as the percentage of data for a region cached on the region server where it is either currently hosted or was previously hosted. A region may have multiple HFiles, each of different sizes. A HFile is considered to be fully prefetched when all the data blocks in this file are in the cache. The region server hosting this region calculates the ratio of number of HFiles fully cached in the cache to the total number of HFiles in the region. This ratio will vary from 0 (region hosted on this server, but none of its HFiles are cached into the cache) to 1 (region hosted on this server and all the HFiles for this region are cached into the cache).Every region server maintains this information for all the regions currently hosted there. In addition to that, this cache ratio is also maintained for the regions which were previously hosted on this region server giving historical information about the regions."},{heading:"cache-aware-load-balancer",content:"Skewness Cost"},{heading:"cache-aware-load-balancer",content:"The cache aware balancer will consider cache cost with the skewness cost to decide on the region assignment plan under following conditions:"},{heading:"cache-aware-load-balancer",content:"There is an idle server in the cluster. This can happen when an existing server is restarted or a new server is added to the cluster."},{heading:"cache-aware-load-balancer",content:"When the cost of maintaining the balance in the cluster is greater than the minimum threshold defined by the configuration hbase.master.balancer.stochastic.minCostNeedBalance."},{heading:"cache-aware-load-balancer",content:"The CacheAwareLoadBalancer can be enabled in the cluster by setting the following configuration properties in the master master configuration:"},{heading:"cache-aware-load-balancer",content:'Within HBASE-29168, the CacheAwareLoadBalancer implements region move throttling. This mitigates the impact of "losing" cache factor when balancing mainly due to region skewness, i.e. when new region servers are added to the cluster, a large bulk of cached regions may move to the new servers at once, which can cause noticeable read performance impacts for cache sensitive use cases. The throttling sleep time is determined by the hbase.master.balancer.move.throttlingMillis property, and it defaults to 60000 millis. If a region planned to be moved has a cache ratio on the target server above the thershold configurable by the hbase.master.balancer.stochastic.throttling.cacheRatio property (80% by default), no throttling will be applied in this region move.'},{heading:"regionserver-splitting-implementation",content:"As write requests are handled by the region server, they accumulate in an in-memory storage system called the memstore. Once the memstore fills, its content are written to disk as additional store files. This event is called a memstore flush. As store files accumulate, the RegionServer will compact them into fewer, larger files. After each flush or compaction finishes, the amount of data stored in the region has changed. The RegionServer consults the region split policy to determine if the region has grown too large or should be split for another policy-specific reason. A region split request is enqueued if the policy recommends it."},{heading:"regionserver-splitting-implementation",content:"Logically, the process of splitting a region is simple. We find a suitable point in the keyspace of the region where we should divide the region in half, then split the region's data into two new regions at that point. The details of the process however are not simple. When a split happens, the newly created daughter regions do not rewrite all the data into new files immediately. Instead, they create small files similar to symbolic link files, named Reference files, which point to either the top or bottom part of the parent store file according to the split point. The reference file is used just like a regular data file, but only half of the records are considered. The region can only be split if there are no more references to the immutable data files of the parent region. Those reference files are cleaned gradually by compactions, so that the region will stop referring to its parents files, and can be split further."},{heading:"regionserver-splitting-implementation",content:'Although splitting the region is a local decision made by the RegionServer, the split process itself must coordinate with many actors. The RegionServer notifies the Master before and after the split, updates the .META. table so that clients can discover the new daughter regions, and rearranges the directory structure and data files in HDFS. Splitting is a multi-task process. To enable rollback in case of an error, the RegionServer keeps an in-memory journal about the execution state. The steps taken by the RegionServer to execute the split are illustrated in the "RegionServer Split Process" schema below. Each step is labeled with its step number. Actions from RegionServers or Master are shown in red, while actions from the clients are shown in green.'},{heading:"regionserver-splitting-implementation",content:"The RegionServer decides locally to split the region, and prepares the split. THE SPLIT TRANSACTION IS STARTED. As a first step, the RegionServer acquires a shared read lock on the table to prevent schema modifications during the splitting process. Then it creates a znode in zookeeper under /hbase/region-in-transition/region-name, and sets the znode's state to SPLITTING."},{heading:"regionserver-splitting-implementation",content:"The Master learns about this znode, since it has a watcher for the parent region-in-transition znode."},{heading:"regionserver-splitting-implementation",content:"The RegionServer creates a sub-directory named .splits under the parent's region directory in HDFS."},{heading:"regionserver-splitting-implementation",content:"The RegionServer closes the parent region and marks the region as offline in its local data structures. THE SPLITTING REGION IS NOW OFFLINE. At this point, client requests coming to the parent region will throw NotServingRegionException. The client will retry with some backoff. The closing region is flushed."},{heading:"regionserver-splitting-implementation",content:"The RegionServer creates region directories under the .splits directory, for daughter regions A and B, and creates necessary data structures. Then it splits the store files, in the sense that it creates two Reference files per store file in the parent region. Those reference files will point to the parent region's files."},{heading:"regionserver-splitting-implementation",content:"The RegionServer creates the actual region directory in HDFS, and moves the reference files for each daughter."},{heading:"regionserver-splitting-implementation",content:"The RegionServer sends a Put request to the .META. table, to set the parent as offline in the .META. table and add information about daughter regions. At this point, there won't be individual entries in .META. for the daughters. Clients will see that the parent region is split if they scan .META., but won't know about the daughters until they appear in .META.. Also, if this Put to .META. succeeds, the parent will be effectively split. If the RegionServer fails before this RPC succeeds, Master and the next Region Server opening the region will clean dirty state about the region split. After the .META. update, though, the region split will be rolled-forward by Master."},{heading:"regionserver-splitting-implementation",content:"The RegionServer opens daughters A and B in parallel."},{heading:"regionserver-splitting-implementation",content:"The RegionServer adds the daughters A and B to .META., together with information that it hosts the regions. THE SPLIT REGIONS (DAUGHTERS WITH REFERENCES TO PARENT) ARE NOW ONLINE. After this point, clients can discover the new regions and issue requests to them. Clients cache the .META. entries locally, but when they make requests to the RegionServer or .META., their caches will be invalidated, and they will learn about the new regions from .META.."},{heading:"regionserver-splitting-implementation",content:"The RegionServer updates znode /hbase/region-in-transition/region-name in ZooKeeper to state SPLIT, so that the master can learn about it. The balancer can freely re-assign the daughter regions to other region servers if necessary. THE SPLIT TRANSACTION IS NOW FINISHED."},{heading:"regionserver-splitting-implementation",content:"After the split, .META. and HDFS will still contain references to the parent region. Those references will be removed when compactions in daughter regions rewrite the data files. Garbage collection tasks in the master periodically check whether the daughter regions still refer to the parent region's files. If not, the parent region will be removed."},{heading:"purpose",content:"The Write Ahead Log (WAL) records all changes to data in HBase, to file-based storage. Under normal operations, the WAL is not needed because data changes move from the MemStore to StoreFiles. However, if a RegionServer crashes or becomes unavailable before the MemStore is flushed, the WAL ensures that the changes to the data can be replayed. If writing to the WAL fails, the entire operation to modify the data fails."},{heading:"purpose",content:"HBase uses an implementation of the WAL interface. Usually, there is only one instance of a WAL per RegionServer. An exception is the RegionServer that is carrying hbase:meta; the meta table gets its own dedicated WAL. The RegionServer records Puts and Deletes to its WAL, before recording them these Mutations MemStore for the affected Store."},{heading:"purpose",content:"type: info"},{heading:"purpose",content:"title: The HLog"},{heading:"purpose",content:`Prior to 2.0, the interface for WALs in HBase was named HLog. In 0.94, HLog was the name of the
implementation of the WAL. You will likely find references to the HLog in documentation tailored
to these older versions.`},{heading:"purpose",content:"The WAL resides in HDFS in the /hbase/WALs/ directory, with subdirectories per RegionServer."},{heading:"purpose",content:"For more general information about the concept of write ahead logs, see the Wikipedia Write-Ahead Log article."},{heading:"wal-providers",content:"In HBase, there are a number of WAL implementations (or 'Providers'). Each is known by a short name label (that unfortunately is not always descriptive). You set the provider in hbase-site.xml passing the WAL provider short-name as the value on the hbase.wal.provider property (Set the provider for hbase:meta using the hbase.wal.meta_provider property, otherwise it uses the same provider configured by hbase.wal.provider)."},{heading:"wal-providers",content:'asyncfs: The default. New since hbase-2.0.0 (HBASE-15536, HBASE-14790). This AsyncFSWAL provider, as it identifies itself in RegionServer logs, is built on a new non-blocking dfsclient implementation. It is currently resident in the hbase codebase but intent is to move it back up into HDFS itself. WALs edits are written concurrently ("fan-out") style to each of the WAL-block replicas on each DataNode rather than in a chained pipeline as the default client does. Latencies should be better. See Apache HBase Improvements and Practices at Xiaomi at slide 14 onward for more detail on implementation.'},{heading:"wal-providers",content:"filesystem: This was the default in hbase-1.x releases. It is built on the blocking DFSClient and writes to replicas in classic DFSCLient pipeline mode. In logs it identifies as FSHLog or FSHLogProvider."},{heading:"wal-providers",content:"multiwal: This provider is made of multiple instances of asyncfs or filesystem. See the next section for more on multiwal."},{heading:"wal-providers",content:"Look for the lines like the below in the RegionServer log to see which provider is in place (The below shows the default AsyncFSWALProvider):"},{heading:"wal-providers",content:"type: info"},{heading:"wal-providers",content:`As the AsyncFSWAL hacks into the internal of DFSClient implementation, it will be easily broken
by upgrading the hadoop dependencies, even for a simple patch release. So if you do not specify
the wal provider explicitly, we will first try to use the asyncfs, if failed, we will fall back
to use filesystem. And notice that this may not always work, so if you still have problem
starting HBase due to the problem of starting AsyncFSWAL, please specify filesystem explicitly
in the config file.`},{heading:"wal-providers",content:"type: info"},{heading:"wal-providers",content:`EC support has been added to hadoop-3.x, and it is incompatible with WAL as the EC output stream
does not support hflush/hsync. In order to create a non-EC file in an EC directory, we need to use
the new builder-based create API for FileSystem, but it is only introduced in hadoop-2.9+ and
for HBase we still need to support hadoop-2.7.x. So please do not enable EC for the WAL directory
until we find a way to deal with it.`},{heading:"multiwal",content:"With a single WAL per RegionServer, the RegionServer must write to the WAL serially, because HDFS files must be sequential. This causes the WAL to be a performance bottleneck."},{heading:"multiwal",content:"HBase 1.0 introduces support MultiWal in HBASE-5699. MultiWAL allows a RegionServer to write multiple WAL streams in parallel, by using multiple pipelines in the underlying HDFS instance, which increases total throughput during writes. This parallelization is done by partitioning incoming edits by their Region. Thus, the current implementation will not help with increasing the throughput to a single Region."},{heading:"multiwal",content:"RegionServers using the original WAL implementation and those using the MultiWAL implementation can each handle recovery of either set of WALs, so a zero-downtime configuration update is possible through a rolling restart."},{heading:"configure-multiwal",content:"To configure MultiWAL for a RegionServer, set the value of the property hbase.wal.provider to multiwal by pasting in the following XML:"},{heading:"configure-multiwal",content:"Restart the RegionServer for the changes to take effect."},{heading:"configure-multiwal",content:"To disable MultiWAL for a RegionServer, unset the property and restart the RegionServer."},{heading:"wal-flushing",content:"TODO (describe)."},{heading:"wal-splitting",content:"A RegionServer serves many regions. All of the regions in a region server share the same active WAL file. Each edit in the WAL file includes information about which region it belongs to. When a region is opened, the edits in the WAL file which belong to that region need to be replayed. Therefore, edits in the WAL file must be grouped by region so that particular sets can be replayed to regenerate the data in a particular region. The process of grouping the WAL edits by region is called log splitting. It is a critical process for recovering data if a region server fails."},{heading:"wal-splitting",content:"Log splitting is done by the HMaster during cluster start-up or by the ServerShutdownHandler as a region server shuts down. So that consistency is guaranteed, affected regions are unavailable until data is restored. All WAL edits need to be recovered and replayed before a given region can become available again. As a result, regions affected by log splitting are unavailable until the process completes."},{heading:"the-hbasewalshostportstartcode-directory-is-renamed",content:"Renaming the directory is important because a RegionServer may still be up and accepting requests even if the HMaster thinks it is down. If the RegionServer does not respond immediately and does not heartbeat its ZooKeeper session, the HMaster may interpret this as a RegionServer failure. Renaming the logs directory ensures that existing, valid WAL files which are still in use by an active but busy RegionServer are not written to by accident."},{heading:"the-hbasewalshostportstartcode-directory-is-renamed",content:"The new directory is named according to the following pattern:"},{heading:"the-hbasewalshostportstartcode-directory-is-renamed",content:"An example of such a renamed directory might look like the following:"},{heading:"each-log-file-is-split-one-at-a-time",content:"The log splitter reads the log file one edit entry at a time and puts each edit entry into the buffer corresponding to the edit's region. At the same time, the splitter starts several writer threads. Writer threads pick up a corresponding buffer and write the edit entries in the buffer to a temporary recovered edit file. The temporary edit file is stored to disk with the following naming pattern:"},{heading:"each-log-file-is-split-one-at-a-time",content:"This file is used to store all the edits in the WAL log for this region. After log splitting completes, the .temp file is renamed to the sequence ID of the first log written to the file."},{heading:"each-log-file-is-split-one-at-a-time",content:"To determine whether all edits have been written, the sequence ID is compared to the sequence of the last edit that was written to the HFile. If the sequence of the last edit is greater than or equal to the sequence ID included in the file name, it is clear that all writes from the edit file have been completed."},{heading:"after-log-splitting-is-complete-each-affected-region-is-assigned-to-a-regionserver",content:"When the region is opened, the recovered.edits folder is checked for recovered edits files. If any such files are present, they are replayed by reading the edits and saving them to the MemStore. After all edit files are replayed, the contents of the MemStore are written to disk (HFile) and the edit files are deleted."},{heading:"handling-of-errors-during-log-splitting",content:"If you set the hbase.hlog.split.skip.errors option to true, errors are treated as follows:"},{heading:"handling-of-errors-during-log-splitting",content:"Any error encountered during splitting will be logged."},{heading:"handling-of-errors-during-log-splitting",content:"The problematic WAL log will be moved into the .corrupt directory under the hbase rootdir,"},{heading:"handling-of-errors-during-log-splitting",content:"Processing of the WAL will continue"},{heading:"handling-of-errors-during-log-splitting",content:"If the hbase.hlog.split.skip.errors option is set to false, the default, the exception will be propagated and the split will be logged as failed. See HBASE-2958 When hbase.hlog.split.skip.errors is set to false, we fail the split but that's it. We need to do more than just fail split if this flag is set."},{heading:"how-eofexceptions-are-treated-when-splitting-a-crashed-regionservers-wals",content:"If an EOFException occurs while splitting logs, the split proceeds even when hbase.hlog.split.skip.errors is set to false. An EOFException while reading the last log in the set of files to split is likely, because the RegionServer was likely in the process of writing a record at the time of a crash. For background, see HBASE-2643 Figure how to deal with eof splitting logs"},{heading:"performance-improvements-during-log-splitting",content:"WAL log splitting and recovery can be resource intensive and take a long time, depending on the number of RegionServers involved in the crash and the size of the regions. Distributed log splitting was developed to improve performance during log splitting."},{heading:"enabling-or-disabling-distributed-log-splitting",content:"Distributed log processing is enabled by default since HBase 0.92. The setting is controlled by the hbase.master.distributed.log.splitting property, which can be set to true or false, but defaults to true."},{heading:"wal-splitting-based-on-procedurev2",content:"After HBASE-20610, we introduce a new way to do WAL splitting coordination by procedureV2 framework. This can simplify the process of WAL splitting and no need to connect zookeeper any more."},{heading:"background-toc",content:"Currently, splitting WAL processes are coordinated by zookeeper. Each region server are trying to grab tasks from zookeeper. And the burden becomes heavier when the number of region server increase."},{heading:"implementation-on-master-side-toc",content:"During ServerCrashProcedure, SplitWALManager will create one SplitWALProcedure for each WAL file which should be split. Then each SplitWALProcedure will spawn a SplitWalRemoteProcedure to send the request to region server. SplitWALProcedure is a StateMachineProcedure and here is the state transfer diagram."},{heading:"implementation-on-region-server-side-toc",content:"Region Server will receive a SplitWALCallable and execute it, which is much more straightforward than before. It will return null if success and return exception if there is any error."},{heading:"architecture-regionserver-wal-splitting-based-on-procedurev2-performance",content:"According to tests on a cluster which has 5 regionserver and 1 master. procedureV2 coordinated WAL splitting has a better performance than ZK coordinated WAL splitting no master when restarting the whole cluster or one region server crashing."},{heading:"enable-this-feature-toc",content:"To enable this feature, first we should ensure our package of HBase already contains these code. If not, please upgrade the package of HBase cluster without any configuration change first. Then change configuration 'hbase.split.wal.zk.coordinated' to false. Rolling upgrade the master with new configuration. Now WAL splitting are handled by our new implementation. But region server are still trying to grab tasks from zookeeper, we can rolling upgrade the region servers with the new configuration to stop that."},{heading:"enable-this-feature-toc",content:"Steps as follows:"},{heading:"enable-this-feature-toc",content:"Upgrade whole cluster to get the new Implementation."},{heading:"enable-this-feature-toc",content:"Upgrade Master with new configuration 'hbase.split.wal.zk.coordinated'=false."},{heading:"enable-this-feature-toc",content:"Upgrade region server to stop grab tasks from zookeeper."},{heading:"wal-compression",content:"The content of the WAL can be compressed using LRU Dictionary compression. This can be used to speed up WAL replication to different datanodes. The dictionary can store up to 215 elements; eviction starts after this number is exceeded."},{heading:"wal-compression",content:"To enable WAL compression, set the hbase.regionserver.wal.enablecompression property to true. The default value for this property is false. By default, WAL tag compression is turned on when WAL compression is enabled. You can turn off WAL tag compression by setting the hbase.regionserver.wal.tags.enablecompression property to 'false'."},{heading:"wal-compression",content:"A possible downside to WAL compression is that we lose more data from the last block in the WAL if it is ill-terminated mid-write. If entries in this last block were added with new dictionary entries but we failed persist the amended dictionary because of an abrupt termination, a read of this last block may not be able to resolve last-written entries."},{heading:"durability",content:"It is possible to set durability on each Mutation or on a Table basis. Options include:"},{heading:"durability",content:"SKIP_WAL: Do not write Mutations to the WAL (See the next section, Disabling the WAL)."},{heading:"durability",content:"ASYNC_WAL: Write the WAL asynchronously; do not hold-up clients waiting on the sync of their write to the filesystem but return immediately. The edit becomes visible. Meanwhile, in the background, the Mutation will be flushed to the WAL at some time later. This option currently may lose data. See HBASE-16689."},{heading:"durability",content:"SYNC_WAL: The default. Each edit is sync'd to HDFS before we return success to the client."},{heading:"durability",content:"FSYNC_WAL: Each edit is fsync'd to HDFS and the filesystem before we return success to the client."},{heading:"durability",content:"Do not confuse the ASYNC_WAL option on a Mutation or Table with the AsyncFSWAL writer; they are distinct options unfortunately closely named"},{heading:"custom-wal-directory",content:"HBASE-17437 added support for specifying a WAL directory outside the HBase root directory or even in a different FileSystem since 1.3.3/2.0+. Some FileSystems (such as Amazon S3) don't support append or consistent writes, in such scenario WAL directory needs to be configured in a different FileSystem to avoid loss of writes."},{heading:"custom-wal-directory",content:"Following configurations are added to accomplish this:"},{heading:"custom-wal-directory",content:"hbase.wal.dirThis defines where the root WAL directory is located, could be on a different FileSystem than the root directory. WAL directory can not be set to a subdirectory of the root directory. The default value of this is the root directory if unset."},{heading:"custom-wal-directory",content:"hbase.rootdir.permsConfigures FileSystem permissions to set on the root directory. This is '700' by default."},{heading:"custom-wal-directory",content:"hbase.wal.dir.permsConfigures FileSystem permissions to set on the WAL directory FileSystem. This is '700' by default."},{heading:"custom-wal-directory",content:"type: info"},{heading:"custom-wal-directory",content:`While migrating to custom WAL dir (outside the HBase root directory or a different FileSystem)
existing WAL files must be copied manually to new WAL dir, otherwise it may lead to data
loss/inconsistency as HMaster has no information about previous WAL directory.`},{heading:"disabling-the-wal",content:"It is possible to disable the WAL, to improve performance in certain specific situations. However, disabling the WAL puts your data at risk. The only situation where this is recommended is during a bulk load. This is because, in the event of a problem, the bulk load can be re-run with no risk of data loss."},{heading:"disabling-the-wal",content:"The WAL is disabled by calling the HBase client field Mutation.writeToWAL(false). Use the Mutation.setDurability(Durability.SKIP_WAL) and Mutation.getDurability() methods to set and get the field's value. There is no way to disable the WAL for only a specific table."},{heading:"disabling-the-wal",content:"type: warn"},{heading:"disabling-the-wal",content:"If you disable the WAL for anything other than bulk loads, your data is at risk."}],headings:[{id:"architecture-regionserver-interface",content:"Interface"},{id:"architecture-regionserver-processes",content:"Processes"},{id:"compactsplitthread",content:"CompactSplitThread"},{id:"majorcompactionchecker",content:"MajorCompactionChecker"},{id:"memstoreflusher",content:"MemStoreFlusher"},{id:"logroller",content:"LogRoller"},{id:"coprocessors",content:"Coprocessors"},{id:"architecture-regionserver-block-cache",content:"Block Cache"},{id:"cache-choices",content:"Cache Choices"},{id:"general-cache-configurations",content:"General Cache Configurations"},{id:"lrublockcache-design",content:"LruBlockCache Design"},{id:"lrublockcache-usage",content:"LruBlockCache Usage"},{id:"caching-meta-blocks-only-data-blocks-in-fscache",content:"Caching META blocks only (DATA blocks in fscache)"},{id:"off-heap-block-cache",content:"Off-heap Block Cache"},{id:"how-to-enable-bucketcache",content:"How to Enable BucketCache"},{id:"bucketcache-deploy-modes",content:"BucketCache Deploy Modes"},{id:"bucketcache-example-configuration",content:"BucketCache Example Configuration"},{id:"time-based-priority-for-bucketcache",content:"Time Based Priority for BucketCache"},{id:"configuring-time-based-priority-for-bucketcache",content:"Configuring Time Based Priority for BucketCache"},{id:"using-cell-timestamps-for-time-based-priority",content:"Using Cell timestamps for Time Based Priority"},{id:"using-custom-cell-qualifiers-for-time-based-priority",content:"Using Custom Cell Qualifiers for Time Based Priority"},{id:"using-a-custom-value-provider-for-time-based-priority",content:"Using a Custom value provider for Time Based Priority"},{id:"compressed-blockcache",content:"Compressed BlockCache"},{id:"cache-aware-load-balancer",content:"Cache Aware Load Balancer"},{id:"regionserver-splitting-implementation",content:"RegionServer Splitting Implementation"},{id:"write-ahead-log-wal",content:"Write Ahead Log (WAL)"},{id:"purpose",content:"Purpose"},{id:"wal-providers",content:"WAL Providers"},{id:"multiwal",content:"MultiWAL"},{id:"configure-multiwal",content:"Configure MultiWAL"},{id:"wal-flushing",content:"WAL Flushing"},{id:"wal-splitting",content:"WAL Splitting"},{id:"procedure-log-splitting-step-by-step",content:"Procedure: Log Splitting, Step by Step"},{id:"the-hbasewalshostportstartcode-directory-is-renamed",content:"The /hbase/WALs/HOST,PORT,STARTCODE directory is renamed"},{id:"each-log-file-is-split-one-at-a-time",content:"Each log file is split, one at a time"},{id:"after-log-splitting-is-complete-each-affected-region-is-assigned-to-a-regionserver",content:"After log splitting is complete, each affected region is assigned to a RegionServer"},{id:"handling-of-errors-during-log-splitting",content:"Handling of Errors During Log Splitting"},{id:"how-eofexceptions-are-treated-when-splitting-a-crashed-regionservers-wals",content:"How EOFExceptions are treated when splitting a crashed RegionServer's WALs"},{id:"performance-improvements-during-log-splitting",content:"Performance Improvements during Log Splitting"},{id:"enabling-or-disabling-distributed-log-splitting",content:"Enabling or Disabling Distributed Log Splitting"},{id:"wal-splitting-based-on-procedurev2",content:"WAL splitting based on procedureV2"},{id:"background-toc",content:"Background [!toc]"},{id:"implementation-on-master-side-toc",content:"Implementation on Master side [!toc]"},{id:"implementation-on-region-server-side-toc",content:"Implementation on Region Server side [!toc]"},{id:"architecture-regionserver-wal-splitting-based-on-procedurev2-performance",content:"Performance [!toc]"},{id:"enable-this-feature-toc",content:"Enable this feature [!toc]"},{id:"wal-compression",content:"WAL Compression"},{id:"durability",content:"Durability"},{id:"custom-wal-directory",content:"Custom WAL Directory"},{id:"disabling-the-wal",content:"Disabling the WAL"}]};const f=[{depth:2,url:"#architecture-regionserver-interface",title:e.jsx(e.Fragment,{children:"Interface"})},{depth:2,url:"#architecture-regionserver-processes",title:e.jsx(e.Fragment,{children:"Processes"})},{depth:3,url:"#compactsplitthread",title:e.jsx(e.Fragment,{children:"CompactSplitThread"})},{depth:3,url:"#majorcompactionchecker",title:e.jsx(e.Fragment,{children:"MajorCompactionChecker"})},{depth:3,url:"#memstoreflusher",title:e.jsx(e.Fragment,{children:"MemStoreFlusher"})},{depth:3,url:"#logroller",title:e.jsx(e.Fragment,{children:"LogRoller"})},{depth:2,url:"#coprocessors",title:e.jsx(e.Fragment,{children:"Coprocessors"})},{depth:2,url:"#architecture-regionserver-block-cache",title:e.jsx(e.Fragment,{children:"Block Cache"})},{depth:3,url:"#cache-choices",title:e.jsx(e.Fragment,{children:"Cache Choices"})},{depth:3,url:"#general-cache-configurations",title:e.jsx(e.Fragment,{children:"General Cache Configurations"})},{depth:3,url:"#lrublockcache-design",title:e.jsx(e.Fragment,{children:"LruBlockCache Design"})},{depth:3,url:"#lrublockcache-usage",title:e.jsx(e.Fragment,{children:"LruBlockCache Usage"})},{depth:4,url:"#caching-meta-blocks-only-data-blocks-in-fscache",title:e.jsx(e.Fragment,{children:"Caching META blocks only (DATA blocks in fscache)"})},{depth:3,url:"#off-heap-block-cache",title:e.jsx(e.Fragment,{children:"Off-heap Block Cache"})},{depth:4,url:"#how-to-enable-bucketcache",title:e.jsx(e.Fragment,{children:"How to Enable BucketCache"})},{depth:4,url:"#bucketcache-deploy-modes",title:e.jsx(e.Fragment,{children:"BucketCache Deploy Modes"})},{depth:4,url:"#bucketcache-example-configuration",title:e.jsx(e.Fragment,{children:"BucketCache Example Configuration"})},{depth:3,url:"#time-based-priority-for-bucketcache",title:e.jsx(e.Fragment,{children:"Time Based Priority for BucketCache"})},{depth:4,url:"#configuring-time-based-priority-for-bucketcache",title:e.jsx(e.Fragment,{children:"Configuring Time Based Priority for BucketCache"})},{depth:4,url:"#using-cell-timestamps-for-time-based-priority",title:e.jsx(e.Fragment,{children:"Using Cell timestamps for Time Based Priority"})},{depth:4,url:"#using-custom-cell-qualifiers-for-time-based-priority",title:e.jsx(e.Fragment,{children:"Using Custom Cell Qualifiers for Time Based Priority"})},{depth:4,url:"#using-a-custom-value-provider-for-time-based-priority",title:e.jsx(e.Fragment,{children:"Using a Custom value provider for Time Based Priority"})},{depth:3,url:"#compressed-blockcache",title:e.jsx(e.Fragment,{children:"Compressed BlockCache"})},{depth:3,url:"#cache-aware-load-balancer",title:e.jsx(e.Fragment,{children:"Cache Aware Load Balancer"})},{depth:2,url:"#regionserver-splitting-implementation",title:e.jsx(e.Fragment,{children:"RegionServer Splitting Implementation"})},{depth:2,url:"#write-ahead-log-wal",title:e.jsx(e.Fragment,{children:"Write Ahead Log (WAL)"})},{depth:3,url:"#purpose",title:e.jsx(e.Fragment,{children:"Purpose"})},{depth:3,url:"#wal-providers",title:e.jsx(e.Fragment,{children:"WAL Providers"})},{depth:3,url:"#multiwal",title:e.jsx(e.Fragment,{children:"MultiWAL"})},{depth:4,url:"#configure-multiwal",title:e.jsx(e.Fragment,{children:"Configure MultiWAL"})},{depth:3,url:"#wal-flushing",title:e.jsx(e.Fragment,{children:"WAL Flushing"})},{depth:3,url:"#wal-splitting",title:e.jsx(e.Fragment,{children:"WAL Splitting"})},{depth:4,url:"#procedure-log-splitting-step-by-step",title:e.jsx(e.Fragment,{children:"Procedure: Log Splitting, Step by Step"})},{depth:5,url:"#the-hbasewalshostportstartcode-directory-is-renamed",title:e.jsxs(e.Fragment,{children:["The ",e.jsx("code",{children:"/hbase/WALs/HOST,PORT,STARTCODE"})," directory is renamed"]})},{depth:5,url:"#each-log-file-is-split-one-at-a-time",title:e.jsx(e.Fragment,{children:"Each log file is split, one at a time"})},{depth:5,url:"#after-log-splitting-is-complete-each-affected-region-is-assigned-to-a-regionserver",title:e.jsx(e.Fragment,{children:"After log splitting is complete, each affected region is assigned to a RegionServer"})},{depth:4,url:"#handling-of-errors-during-log-splitting",title:e.jsx(e.Fragment,{children:"Handling of Errors During Log Splitting"})},{depth:4,url:"#how-eofexceptions-are-treated-when-splitting-a-crashed-regionservers-wals",title:e.jsx(e.Fragment,{children:"How EOFExceptions are treated when splitting a crashed RegionServer's WALs"})},{depth:4,url:"#performance-improvements-during-log-splitting",title:e.jsx(e.Fragment,{children:"Performance Improvements during Log Splitting"})},{depth:4,url:"#enabling-or-disabling-distributed-log-splitting",title:e.jsx(e.Fragment,{children:"Enabling or Disabling Distributed Log Splitting"})},{depth:3,url:"#wal-splitting-based-on-procedurev2",title:e.jsx(e.Fragment,{children:"WAL splitting based on procedureV2"})},{depth:3,url:"#wal-compression",title:e.jsx(e.Fragment,{children:"WAL Compression"})},{depth:3,url:"#durability",title:e.jsx(e.Fragment,{children:"Durability"})},{depth:3,url:"#custom-wal-directory",title:e.jsx(e.Fragment,{children:"Custom WAL Directory"})},{depth:3,url:"#disabling-the-wal",title:e.jsx(e.Fragment,{children:"Disabling the WAL"})}];function o(s){const i={a:"a",br:"br",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",h5:"h5",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",ul:"ul",...s.components},{Callout:t,Step:n,Steps:r}=i;return t||a("Callout"),n||a("Step"),r||a("Steps"),e.jsxs(e.Fragment,{children:[e.jsxs(i.p,{children:[e.jsx(i.code,{children:"HRegionServer"})," is the RegionServer implementation. It is responsible for serving and managing regions. In a distributed cluster, a RegionServer runs on a ",e.jsx(i.a,{href:"/docs/architecture/hdfs#hdfs-datanode",children:"DataNode"}),"."]}),`
`,e.jsx(i.h2,{id:"architecture-regionserver-interface",children:"Interface"}),`
`,e.jsxs(i.p,{children:["The methods exposed by ",e.jsx(i.code,{children:"HRegionRegionInterface"})," contain both data-oriented and region-maintenance methods:"]}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsx(i.li,{children:"Data (get, put, delete, next, etc.)"}),`
`,e.jsxs(i.li,{children:["Region (splitRegion, compactRegion, etc.) For example, when the ",e.jsx(i.code,{children:"Admin"})," method ",e.jsx(i.code,{children:"majorCompact"})," is invoked on a table, the client is actually iterating through all regions for the specified table and requesting a major compaction directly to each region."]}),`
`]}),`
`,e.jsx(i.h2,{id:"architecture-regionserver-processes",children:"Processes"}),`
`,e.jsx(i.p,{children:"The RegionServer runs a variety of background threads:"}),`
`,e.jsx(i.h3,{id:"compactsplitthread",children:"CompactSplitThread"}),`
`,e.jsx(i.p,{children:"Checks for splits and handle minor compactions."}),`
`,e.jsx(i.h3,{id:"majorcompactionchecker",children:"MajorCompactionChecker"}),`
`,e.jsx(i.p,{children:"Checks for major compactions."}),`
`,e.jsx(i.h3,{id:"memstoreflusher",children:"MemStoreFlusher"}),`
`,e.jsx(i.p,{children:"Periodically flushes in-memory writes in the MemStore to StoreFiles."}),`
`,e.jsx(i.h3,{id:"logroller",children:"LogRoller"}),`
`,e.jsx(i.p,{children:"Periodically checks the RegionServer's WAL."}),`
`,e.jsx(i.h2,{id:"coprocessors",children:"Coprocessors"}),`
`,e.jsxs(i.p,{children:["Coprocessors were added in 0.92. There is a thorough ",e.jsx(i.a,{href:"https://blogs.apache.org/hbase/entry/coprocessor_introduction",children:"Blog Overview of CoProcessors"})," posted. Documentation will eventually move to this reference guide, but the blog is the most current information available at this time."]}),`
`,e.jsx(i.h2,{id:"architecture-regionserver-block-cache",children:"Block Cache"}),`
`,e.jsxs(i.p,{children:["HBase provides two different BlockCache implementations to cache data read from HDFS: the default on-heap ",e.jsx(i.code,{children:"LruBlockCache"})," and the ",e.jsx(i.code,{children:"BucketCache"}),", which is (usually) off-heap. This section discusses benefits and drawbacks of each implementation, how to choose the appropriate option, and configuration options for each."]}),`
`,e.jsx(t,{type:"info",title:"Block Cache Reporting: UI",children:e.jsx(i.p,{children:`See the RegionServer UI for detail on caching deploy. See configurations, sizings, current usage,
time-in-the-cache, and even detail on block counts and types.`})}),`
`,e.jsx(i.h3,{id:"cache-choices",children:"Cache Choices"}),`
`,e.jsxs(i.p,{children:[e.jsx(i.code,{children:"LruBlockCache"})," is the original implementation, and is entirely within the Java heap. ",e.jsx(i.code,{children:"BucketCache"})," is optional and mainly intended for keeping block cache data off-heap, although ",e.jsx(i.code,{children:"BucketCache"})," can also be a file-backed cache. In file-backed we can either use it in the file mode or the mmaped mode. We also have pmem mode where the bucket cache resides on the persistent memory device."]}),`
`,e.jsxs(i.p,{children:['When you enable BucketCache, you are enabling a two tier caching system. We used to describe the tiers as "L1" and "L2" but have deprecated this terminology as of hbase-2.0.0. The "L1" cache referred to an instance of LruBlockCache and "L2" to an off-heap BucketCache. Instead, when BucketCache is enabled, all DATA blocks are kept in the BucketCache tier and meta blocks — INDEX and BLOOM blocks — are on-heap in the ',e.jsx(i.code,{children:"LruBlockCache"}),". Management of these two tiers and the policy that dictates how blocks move between them is done by ",e.jsx(i.code,{children:"CombinedBlockCache"}),"."]}),`
`,e.jsx(i.h3,{id:"general-cache-configurations",children:"General Cache Configurations"}),`
`,e.jsxs(i.p,{children:["Apart from the cache implementation itself, you can set some general configuration options to control how the cache performs. See ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CacheConfig.html",children:"CacheConfig"}),". After setting any of these options, restart or rolling restart your cluster for the configuration to take effect. Check logs for errors or unexpected behavior."]}),`
`,e.jsxs(i.p,{children:["See also ",e.jsx(i.a,{href:"/docs/performance#prefetch-option-for-blockcache",children:"Prefetch Option for Blockcache"}),", which discusses a new option introduced in ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-9857",children:"HBASE-9857"}),"."]}),`
`,e.jsx(i.h3,{id:"lrublockcache-design",children:"LruBlockCache Design"}),`
`,e.jsx(i.p,{children:"The LruBlockCache is an LRU cache that contains three levels of block priority to allow for scan-resistance and in-memory ColumnFamilies:"}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsx(i.li,{children:"Single access priority: The first time a block is loaded from HDFS it normally has this priority and it will be part of the first group to be considered during evictions. The advantage is that scanned blocks are more likely to get evicted than blocks that are getting more usage."}),`
`,e.jsx(i.li,{children:"Multi access priority: If a block in the previous priority group is accessed again, it upgrades to this priority. It is thus part of the second group considered during evictions."}),`
`,e.jsxs(i.li,{children:[`In-memory access priority: If the block's family was configured to be "in-memory", it will be part of this priority disregarding the number of times it was accessed. Catalog tables are configured like this. This group is the last one considered during evictions.`,e.jsx(i.br,{}),`
`,"To mark a column family as in-memory, call",`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsx(i.code,{children:e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"HColumnDescriptor."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"setInMemory"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"("}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"true"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:");"})]})})})}),`
`,"if creating a table from java, or set ",e.jsx(i.code,{children:"IN_MEMORY ⇒ true"})," when creating or altering a table in the shell: e.g.",`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsx(i.code,{children:e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"hbase"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(main)"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"003"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"0"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:">"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" create  "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'t'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", {NAME "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'f'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", IN_MEMORY "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'true'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"}"})]})})})}),`
`,"For more information, see the LruBlockCache source"]}),`
`]}),`
`,e.jsx(i.h3,{id:"lrublockcache-usage",children:"LruBlockCache Usage"}),`
`,e.jsxs(i.p,{children:["Block caching is enabled by default for all the user tables which means that any read operation will load the LRU cache. This might be good for a large number of use cases, but further tunings are usually required in order to achieve better performance. An important concept is the ",e.jsx(i.a,{href:"http://en.wikipedia.org/wiki/Working_set_size",children:"working set size"}),`, or WSS, which is: "the amount of memory needed to compute the answer to a problem". For a website, this would be the data that's needed to answer the queries over a short amount of time.`]}),`
`,e.jsx(i.p,{children:"The way to calculate how much memory is available in HBase for caching is:"}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsx(i.code,{children:e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"number of region servers "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"*"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" heap size "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"*"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" hfile.block.cache.size "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"*"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:" 0.99"})]})})})}),`
`,e.jsx(i.p,{children:"The default value for the block cache is 0.4 which represents 40% of the available heap. The last value (99%) is the default acceptable loading factor in the LRU cache after which eviction is started. The reason it is included in this equation is that it would be unrealistic to say that it is possible to use 100% of the available memory since this would make the process blocking from the point where it loads new blocks. Here are some examples:"}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsx(i.li,{children:"One region server with the heap size set to 1 GB and the default block cache size will have 405 MB of block cache available."}),`
`,e.jsx(i.li,{children:"20 region servers with the heap size set to 8 GB and a default block cache size will have 63.3 GB of block cache."}),`
`,e.jsx(i.li,{children:"100 region servers with the heap size set to 24 GB and a block cache size of 0.5 will have about 1.16 TB of block cache."}),`
`]}),`
`,e.jsx(i.p,{children:"Your data is not the only resident of the block cache. Here are others that you may have to take into account:"}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:[`
`,e.jsxs(i.p,{children:[e.jsx(i.strong,{children:"Catalog Tables"}),e.jsx(i.br,{}),`
`,"The ",e.jsx(i.code,{children:"hbase:meta"})," table is forced into the block cache and have the in-memory priority which means that they are harder to evict."]}),`
`,e.jsx(t,{type:"info",children:e.jsx(i.p,{children:"The hbase:meta tables can occupy a few MBs depending on the number of regions."})}),`
`]}),`
`,e.jsxs(i.li,{children:[`
`,e.jsxs(i.p,{children:[e.jsx(i.strong,{children:"HFiles Indexes"}),e.jsx(i.br,{}),`
`,"An ",e.jsx(i.em,{children:"HFile"})," is the file format that HBase uses to store data in HDFS. It contains a multi-layered index which allows HBase to seek the data without having to read the whole file. The size of those indexes is a factor of the block size (64KB by default), the size of your keys and the amount of data you are storing. For big data sets it's not unusual to see numbers around 1GB per region server, although not all of it will be in cache because the LRU will evict indexes that aren't used."]}),`
`]}),`
`,e.jsxs(i.li,{children:[`
`,e.jsxs(i.p,{children:[e.jsx(i.strong,{children:"Keys"}),e.jsx(i.br,{}),`
`,"The values that are stored are only half the picture, since each value is stored along with its keys (row key, family qualifier, and timestamp). See ",e.jsx(i.a,{href:"/docs/regionserver-sizing#try-to-minimize-row-and-column-sizes",children:"Try to minimize row and column sizes"}),"."]}),`
`]}),`
`,e.jsxs(i.li,{children:[`
`,e.jsxs(i.p,{children:[e.jsx(i.strong,{children:"Bloom Filters"}),e.jsx(i.br,{}),`
`,"Just like the HFile indexes, those data structures (when enabled) are stored in the LRU."]}),`
`]}),`
`]}),`
`,e.jsxs(i.p,{children:["Currently the recommended way to measure HFile indexes and bloom filters sizes is to look at the region server web UI and checkout the relevant metrics. For keys, sampling can be done by using the HFile command line tool and look for the average key size metric. Since HBase 0.98.3, you can view details on BlockCache stats and metrics in a special Block Cache section in the UI. As of HBase 2.4.14, you can estimate HFile indexes and bloom filters vs other DATA blocks using blockCacheCount and blockCacheDataBlockCount in JMX. The formula ",e.jsx(i.code,{children:"(blockCacheCount - blockCacheDataBlockCount) * blockSize"})," will give you an estimate which can be useful when trying to enable the BucketCache. You should make sure the post-BucketCache config gives enough memory to the on-heap LRU cache to hold at least the same number of non-DATA blocks from pre-BucketCache. Once BucketCache is enabled, the L1 metrics like l1CacheSize, l1CacheCount, and l1CacheEvictionCount can help you further tune the size."]}),`
`,e.jsx(i.p,{children:"It's generally bad to use block caching when the WSS doesn't fit in memory. This is the case when you have for example 40GB available across all your region servers' block caches but you need to process 1TB of data. One of the reasons is that the churn generated by the evictions will trigger more garbage collections unnecessarily. Here are two use cases:"}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:["Fully random reading pattern: This is a case where you almost never access the same row twice within a short amount of time such that the chance of hitting a cached block is close to 0. Setting block caching on such a table is a waste of memory and CPU cycles, more so that it will generate more garbage to pick up by the JVM. For more information on monitoring GC, see ",e.jsx(i.a,{href:"/docs/troubleshooting#jvm-garbage-collection-logs",children:"JVM Garbage Collection Logs"}),"."]}),`
`,e.jsx(i.li,{children:"Mapping a table: In a typical MapReduce job that takes a table in input, every row will be read only once so there's no need to put them into the block cache. The Scan object has the option of turning this off via the setCacheBlocks method (set it to false). You can still keep block caching turned on on this table if you need fast random read access. An example would be counting the number of rows in a table that serves live traffic, caching every block of that table would create massive churn and would surely evict data that's currently in use."}),`
`]}),`
`,e.jsx(i.h4,{id:"caching-meta-blocks-only-data-blocks-in-fscache",children:"Caching META blocks only (DATA blocks in fscache)"}),`
`,e.jsxs(i.p,{children:["An interesting setup is one where we cache META blocks only and we read DATA blocks in on each access. If the DATA blocks fit inside fscache, this alternative may make sense when access is completely random across a very large dataset. To enable this setup, alter your table and for each column family set ",e.jsx(i.code,{children:"BLOCKCACHE ⇒ 'false'"}),". You are 'disabling' the BlockCache for this column family only. You can never disable the caching of META blocks. Since ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-4683",children:"HBASE-4683 Always cache index and bloom blocks"}),", we will cache META blocks even if the BlockCache is disabled."]}),`
`,e.jsx(i.h3,{id:"off-heap-block-cache",children:"Off-heap Block Cache"}),`
`,e.jsx(i.h4,{id:"how-to-enable-bucketcache",children:"How to Enable BucketCache"}),`
`,e.jsxs(i.p,{children:["The usual deployment of BucketCache is via a managing class that sets up two caching tiers: an on-heap cache implemented by LruBlockCache and a second cache implemented with BucketCache. The managing class is ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CombinedBlockCache.html",children:"CombinedBlockCache"})," by default. The previous link describes the caching 'policy' implemented by CombinedBlockCache. In short, it works by keeping meta blocks — INDEX and BLOOM in the on-heap LruBlockCache tier — and DATA blocks are kept in the BucketCache tier."]}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:[`
`,e.jsx(i.p,{children:e.jsx(i.strong,{children:"Pre-hbase-2.0.0 versions"})}),`
`,e.jsxs(i.p,{children:["Fetching will always be slower when fetching from BucketCache in pre-hbase-2.0.0, as compared to the native on-heap LruBlockCache. However, latencies tend to be less erratic across time, because there is less garbage collection when you use BucketCache since it is managing BlockCache allocations, not the GC. If the BucketCache is deployed in off-heap mode, this memory is not managed by the GC at all. This is why you'd use BucketCache in pre-2.0.0, so your latencies are less erratic, to mitigate GCs and heap fragmentation, and so you can safely use more memory. See Nick Dimiduk's ",e.jsx(i.a,{href:"http://www.n10k.com/blog/blockcache-101/",children:"BlockCache 101"})," for comparisons running on-heap vs off-heap tests. Also see ",e.jsx(i.a,{href:"https://web.archive.org/web/20231109025243/http://people.apache.org/~stack/bc/",children:"Comparing BlockCache Deploys"})," which finds that if your dataset fits inside your LruBlockCache deploy, use it otherwise if you are experiencing cache churn (or you want your cache to exist beyond the vagaries of java GC), use BucketCache."]}),`
`,e.jsxs(i.p,{children:["In pre-2.0.0, one can configure the BucketCache so it receives the ",e.jsx(i.code,{children:"victim"})," of an LruBlockCache eviction. All Data and index blocks are cached in L1 first. When eviction happens from L1, the blocks (or ",e.jsx(i.code,{children:"victims"}),") will get moved to L2. Set ",e.jsx(i.code,{children:"cacheDataInL1"})," via ",e.jsx(i.code,{children:"(HColumnDescriptor.setCacheDataInL1(true)"})," or in the shell, creating or amending column families setting ",e.jsx(i.code,{children:"CACHE_DATA_IN_L1"})," to true: e.g."]}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsx(i.code,{children:e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"hbase"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(main)"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"003"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"0"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:">"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" create "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'t'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", {NAME "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 't'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", CONFIGURATION "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" {CACHE_DATA_IN_L1 "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'true'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"}}"})]})})})}),`
`]}),`
`,e.jsxs(i.li,{children:[`
`,e.jsx(i.p,{children:e.jsx(i.strong,{children:"hbase-2.0.0+ versions"})}),`
`,e.jsxs(i.p,{children:["HBASE-11425 changed the HBase read path so it could hold the read-data off-heap avoiding copying of cached data on to the java heap. See ",e.jsx(i.a,{href:"/docs/offheap-read-write#offheap-read-path",children:"Offheap read-path"}),". In hbase-2.0.0, off-heap latencies approach those of on-heap cache latencies with the added benefit of NOT provoking GC."]}),`
`,e.jsxs(i.p,{children:["From HBase 2.0.0 onwards, the notions of L1 and L2 have been deprecated. When BucketCache is turned on, the DATA blocks will always go to BucketCache and INDEX/BLOOM blocks go to on heap LRUBlockCache. ",e.jsx(i.code,{children:"cacheDataInL1"})," support has been removed."]}),`
`]}),`
`]}),`
`,e.jsx(i.h4,{id:"bucketcache-deploy-modes",children:"BucketCache Deploy Modes"}),`
`,e.jsxs(i.p,{children:["The BucketCache Block Cache can be deployed ",e.jsx(i.em,{children:"offheap"}),", ",e.jsx(i.em,{children:"file"})," or ",e.jsx(i.em,{children:"mmaped"})," file mode."]}),`
`,e.jsxs(i.p,{children:["You set which via the ",e.jsx(i.code,{children:"hbase.bucketcache.ioengine"})," setting. Setting it to ",e.jsx(i.code,{children:"offheap"})," will have BucketCache make its allocations off-heap, and an ioengine setting of ",e.jsx(i.code,{children:"file:PATH_TO_FILE"})," will direct BucketCache to use file caching (Useful in particular if you have some fast I/O attached to the box such as SSDs). From 2.0.0, it is possible to have more than one file backing the BucketCache. This is very useful especially when the Cache size requirement is high. For multiple backing files, configure ioengine as ",e.jsx(i.code,{children:"files:PATH_TO_FILE1,PATH_TO_FILE2,PATH_TO_FILE3"}),". BucketCache can be configured to use an mmapped file also. Configure ioengine as ",e.jsx(i.code,{children:"mmap:PATH_TO_FILE"})," for this."]}),`
`,e.jsxs(i.p,{children:["It is possible to deploy a tiered setup where we bypass the CombinedBlockCache policy and have BucketCache working as a strict L2 cache to the L1 LruBlockCache. For such a setup, set ",e.jsx(i.code,{children:"hbase.bucketcache.combinedcache.enabled"})," to ",e.jsx(i.code,{children:"false"}),". In this mode, on eviction from L1, blocks go to L2. When a block is cached, it is cached first in L1. When we go to look for a cached block, we look first in L1 and if none found, then search L2. Let us call this deploy format, ",e.jsx(i.em,{children:"Raw L1+L2"}),". NOTE: This L1+L2 mode is removed from 2.0.0. When BucketCache is used, it will be strictly the DATA cache and the LruBlockCache will cache INDEX/META blocks."]}),`
`,e.jsxs(i.p,{children:["Other BucketCache configs include: specifying a location to persist cache to across restarts, how many threads to use writing the cache, etc. See the ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CacheConfig.html",children:"CacheConfig.html"})," class for configuration options and descriptions."]}),`
`,e.jsx(i.p,{children:"To check it enabled, look for the log line describing cache setup; it will detail how BucketCache has been deployed. Also see the UI. It will detail the cache tiering and their configuration."}),`
`,e.jsx(i.h4,{id:"bucketcache-example-configuration",children:"BucketCache Example Configuration"}),`
`,e.jsx(i.p,{children:"This sample provides a configuration for a 4 GB off-heap BucketCache with a 1 GB on-heap cache."}),`
`,e.jsx(i.p,{children:"Configuration is performed on the RegionServer."}),`
`,e.jsxs(i.p,{children:["Setting ",e.jsx(i.code,{children:"hbase.bucketcache.ioengine"})," and ",e.jsx(i.code,{children:"hbase.bucketcache.size"})," > 0 enables ",e.jsx(i.code,{children:"CombinedBlockCache"}),". Let us presume that the RegionServer has been set to run with a 5G heap: i.e. ",e.jsx(i.code,{children:"HBASE_HEAPSIZE=5g"}),"."]}),`
`,e.jsxs(i.ol,{children:[`
`,e.jsxs(i.li,{children:[`
`,e.jsxs(i.p,{children:["First, edit the RegionServer's ",e.jsx(i.em,{children:"hbase-env.sh"})," and set ",e.jsx(i.code,{children:"HBASE_OFFHEAPSIZE"})," to a value greater than the off-heap size wanted, in this case, 4 GB (expressed as 4G). Let's set it to 5G. That'll be 4G for our off-heap cache and 1G for any other uses of off-heap memory (there are other users of off-heap memory other than BlockCache; e.g. DFSClient in RegionServer can make use of off-heap memory). See Direct Memory Usage In HBase below."]}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsx(i.code,{children:e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"HBASE_OFFHEAPSIZE"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"5G"})]})})})}),`
`]}),`
`,e.jsxs(i.li,{children:[`
`,e.jsxs(i.p,{children:["Next, add the following configuration to the RegionServer's ",e.jsx(i.em,{children:"hbase-site.xml"}),"."]}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.bucketcache.ioengine</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">offheap</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hfile.block.cache.size</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">0.2</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.bucketcache.size</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">4196</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`]}),`
`,e.jsxs(i.li,{children:[`
`,e.jsx(i.p,{children:"Restart or rolling restart your cluster, and check the logs for any issues."}),`
`]}),`
`]}),`
`,e.jsx(i.p,{children:"In the above, we set the BucketCache to be 4G. We configured the on-heap LruBlockCache have 20% (0.2) of the RegionServer's heap size (0.2 * 5G = 1G). In other words, you configure the L1 LruBlockCache as you would normally (as if there were no L2 cache present)."}),`
`,e.jsxs(i.p,{children:[e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-10641",children:"HBASE-10641"})," introduced the ability to configure multiple sizes for the buckets of the BucketCache, in HBase 0.98 and newer. To configurable multiple bucket sizes, configure the new property ",e.jsx(i.code,{children:"hbase.bucketcache.bucket.sizes"})," to a comma-separated list of block sizes, ordered from smallest to largest, with no spaces. The goal is to optimize the bucket sizes based on your data access patterns. The following example configures buckets of size 4096 and 8192."]}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.bucketcache.bucket.sizes</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">4096,8192</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsxs(t,{type:"info",title:"Direct Memory Usage In HBase",children:[e.jsxs(i.p,{children:["The default maximum direct memory varies by JVM. Traditionally it is 64M or some relation to allocated heap size (-Xmx) or no limit at all (JDK7 apparently). HBase servers use direct memory, in particular short-circuit reading (See ",e.jsx(i.a,{href:"/docs/performance#leveraging-local-data",children:"Leveraging local data"}),"), the hosted DFSClient will allocate direct memory buffers. How much the DFSClient uses is not easy to quantify; it is the number of open HFiles * ",e.jsx(i.code,{children:"hbase.dfs.client.read.shortcircuit.buffer.size"})," where ",e.jsx(i.code,{children:"hbase.dfs.client.read.shortcircuit.buffer.size"})," is set to 128k in HBase — see ",e.jsx(i.em,{children:"hbase-default.xml"})," default configurations. If you do off-heap block caching, you'll be making use of direct memory. The RPCServer uses a ByteBuffer pool. From 2.0.0, these buffers are off-heap ByteBuffers. Starting your JVM, make sure the ",e.jsx(i.code,{children:"-XX:MaxDirectMemorySize"})," setting in ",e.jsx(i.em,{children:"conf/hbase-env.sh"})," considers off-heap BlockCache (",e.jsx(i.code,{children:"hbase.bucketcache.size"}),"), DFSClient usage, RPC side ByteBufferPool max size. This has to be bit higher than sum of off heap BlockCache size and max ByteBufferPool size. Allocating an extra of 1-2 GB for the max direct memory size has worked in tests. Direct memory, which is part of the Java process heap, is separate from the object heap allocated by -Xmx. The value allocated by ",e.jsx(i.code,{children:"MaxDirectMemorySize"})," must not exceed physical RAM, and is likely to be less than the total available RAM due to other memory requirements and system constraints."]}),e.jsxs(i.p,{children:["You can see how much memory — on-heap and off-heap/direct — a RegionServer is configured to use and how much it is using at any one time by looking at the ",e.jsx(i.em,{children:"Server Metrics: Memory"})," tab in the UI. It can also be gotten via JMX. In particular the direct memory currently used by the server can be found on the ",e.jsx(i.code,{children:"java.nio.type=BufferPool,name=direct"})," bean. Terracotta has a ",e.jsx(i.a,{href:"https://web.archive.org/web/20170907032911/http://terracotta.org/documentation/4.0/bigmemorygo/configuration/storage-options",children:"good write up"})," on using off-heap memory in Java. It is for their product BigMemory but a lot of the issues noted apply in general to any attempt at going off-heap. Check it out."]})]}),`
`,e.jsxs(t,{type:"info",title:"hbase.bucketcache.percentage.in.combinedcache",children:[e.jsxs(i.p,{children:["This is a pre-HBase 1.0 configuration removed because it was confusing. It was a float that you would set to some value between 0.0 and 1.0. Its default was 0.9. If the deploy was using CombinedBlockCache, then the LruBlockCache L1 size was calculated to be ",e.jsx(i.code,{children:"(1 - hbase.bucketcache.percentage.in.combinedcache) * size-of-bucketcache"})," and the BucketCache size was ",e.jsx(i.code,{children:"hbase.bucketcache.percentage.in.combinedcache * size-of-bucket-cache"}),". where size-of-bucket-cache itself is EITHER the value of the configuration ",e.jsx(i.code,{children:"hbase.bucketcache.size"})," IF it was specified as Megabytes OR ",e.jsx(i.code,{children:"hbase.bucketcache.size"})," * ",e.jsx(i.code,{children:"-XX:MaxDirectMemorySize"})," if ",e.jsx(i.code,{children:"hbase.bucketcache.size"})," is between 0 and 1.0."]}),e.jsxs(i.p,{children:["In 1.0, it should be more straight-forward. Onheap LruBlockCache size is set as a fraction of java heap using ",e.jsx(i.code,{children:"hfile.block.cache.size setting"})," (not the best name) and BucketCache is set as above in absolute Megabytes."]})]}),`
`,e.jsx(i.h3,{id:"time-based-priority-for-bucketcache",children:"Time Based Priority for BucketCache"}),`
`,e.jsxs(i.p,{children:[e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-28463",children:"HBASE-28463"})," introduced time based priority for blocks in BucketCache. It allows for defining an age threshold at individual column families' configuration, whereby blocks older than this configured threshold would be targeted first for eviction."]}),`
`,e.jsx(i.p,{children:"Blocks from column families that don't define the age threshold wouldn't be evaluated by the time based priority, and would only be evicted following the LRU eviction logic."}),`
`,e.jsx(i.p,{children:'This feature is mostly useful for use cases where most recent data is more frequently accessed, and therefore should get higher priority in the cache. Configuring Time Based Priority with the "age" of most accessed data would then give a finer control over blocks allocation in the BucketCache than the built-in LRU eviction logic.'}),`
`,e.jsx(i.p,{children:"Time Based Priority for BucketCache provides three different strategies for defining data age:"}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsx(i.li,{children:"Cell timestamps: Uses the timestamp portion of HBase cells for comparing the data age."}),`
`,e.jsx(i.li,{children:"Custom cell qualifiers: Uses a custom-defined date qualifier for comparing the data age. It uses that value to tier the entire row containing the given qualifier value. This requires that the custom qualifier be a valid Java long timestamp."}),`
`,e.jsx(i.li,{children:"Custom value provider: Allows for defining a pluggable implementation that contains the logic for identifying the date value to be used for comparison. This also provides additional flexibility for different use cases that might have the date stored in other formats or embedded with other data in various portions of a given row."}),`
`]}),`
`,e.jsxs(i.p,{children:["For use cases where priority is determined by the order of record ingestion in HBase (with the most recent being the most relevant), the built-in cell timestamp offers the most convenient and efficient method for configuring age-based priority. See ",e.jsx(i.a,{href:"/docs/architecture/regionserver#using-cell-timestamps-for-time-based-priority",children:"Using Cell timestamps for Time Based Priority"}),"."]}),`
`,e.jsxs(i.p,{children:["Some applications may utilize a custom date column to define the priority of table records. In such instances, a custom cell qualifier-based priority is advisable. See ",e.jsx(i.a,{href:"/docs/architecture/regionserver#using-custom-cell-qualifiers-for-time-based-priority",children:"Using Custom Cell Qualifiers for Time Based Priority"}),"."]}),`
`,e.jsxs(i.p,{children:["Finally, more intricate schemas may incorporate domain-specific logic for defining the age of each record. The custom value provider facilitates the integration of custom code to implement the appropriate parsing of the date value that should be used for the priority comparison. See ",e.jsx(i.a,{href:"/docs/architecture/regionserver#using-a-custom-value-provider-for-time-based-priority",children:"Using a Custom value provider for Time Based Priority"}),"."]}),`
`,e.jsx(i.p,{children:"With Time Based Priority for BucketCache, blocks age is evaluated when deciding if a block should be cached (i.e. during reads, writes, compaction and prefetch), as well as during the cache freeSpace run (mass eviction), prior to executing the LRU logic."}),`
`,e.jsx(i.p,{children:`Because blocks don't hold any specific meta information other than type, it's necessary to group blocks of the same "age group" on separate files, using specialized compaction implementations (see more details in the configuration section below). The time range of all blocks in each file is then appended at the file meta info section, and is used for evaluating the age of blocks that should be considered in the Time Based Priority logic.`}),`
`,e.jsx(i.h4,{id:"configuring-time-based-priority-for-bucketcache",children:"Configuring Time Based Priority for BucketCache"}),`
`,e.jsx(i.p,{children:"Finding the age of each block involves an extra overhead, therefore the feature is disabled by default at a global configuration level."}),`
`,e.jsxs(i.p,{children:["To enable it, the following configuration should be set on RegionServers' ",e.jsx(i.em,{children:"hbase-site.xml"}),":"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.regionserver.datatiering.enable</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">true</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsx(i.p,{children:"Once enabled globally, it's necessary to define the desired strategy-specific settings at the individual column family level."}),`
`,e.jsx(i.h4,{id:"using-cell-timestamps-for-time-based-priority",children:"Using Cell timestamps for Time Based Priority"}),`
`,e.jsx(i.p,{children:"This strategy is the most efficient to run, as it uses the timestamp portion of each cell containing the data for comparing the age of blocks. It requires DateTieredCompaction for splitting the blocks into separate files according to blocks' ages."}),`
`,e.jsx(i.p,{children:"The example below sets the hot age threshold to one week (in milliseconds) for the column family 'cf1' in table 'orders':"}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"hbase"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(main)"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"003"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"0"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:">"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" alter "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'orders'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", {NAME "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'cf1'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  CONFIGURATION "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" {"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'hbase.hstore.datatiering.type'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'TIME_RANGE'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.datatiering.hot.age.millis'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" '604800000'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.engine.class'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.blockingStoreFiles'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" '60'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.compaction.min'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" '2'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.compaction.max'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" '60'"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  }"})}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"}"})})]})})}),`
`,e.jsxs(t,{type:"info",title:"Date Tiered Compaction specific tunings",children:[e.jsx(i.p,{children:"In the example above, the properties governing the number of windows and period of each window in the date tiered compaction were not set. With the default settings, the compaction will create initially four windows of six hours, then four windows of one day each, then another four windows of four days each and so on until the minimum timestamp among the selected files is covered. This can create a large number of files, therefore, additional changes to the 'hbase.hstore.blockingStoreFiles', 'hbase.hstore.compaction.min' and 'hbase.hstore.compaction.max' are recommended."}),e.jsx(i.p,{children:"Alternatively, consider adjusting the initial window size to the same as the hot age threshold, and two windows only per tier:"}),e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"hbase"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(main)"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"003"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"0"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:">"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" alter "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'orders'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", {NAME "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'cf1'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  CONFIGURATION "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" {"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'hbase.hstore.datatiering.type'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'TIME_RANGE'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.datatiering.hot.age.millis'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" '604800000'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.engine.class'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.compaction.date.tiered.base.window.millis'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" '604800000'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.compaction.date.tiered.windows.per.tier'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" '2'"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  }"})}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"}"})})]})})})]}),`
`,e.jsx(i.h4,{id:"using-custom-cell-qualifiers-for-time-based-priority",children:"Using Custom Cell Qualifiers for Time Based Priority"}),`
`,e.jsx(i.p,{children:'This strategy uses a new compaction implementation designed for Time Based Priority. It extends date tiered compaction, but instead of producing multiple tiers of various time windows, it simply splits files into two groups: the "cold" group, where all blocks are older than the defined threshold age, and the "hot" group, where all blocks are newer than the threshold age.'}),`
`,e.jsx(i.p,{children:"The example below defines a cell qualifier 'event_date' to be used for comparing the age of blocks within the custom cell qualifier strategy:"}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"hbase"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(main)"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"003"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"0"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:">"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" alter "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'orders'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", {NAME "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'cf1'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  CONFIGURATION "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" {"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'hbase.hstore.datatiering.type'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'CUSTOM'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'TIERING_CELL_QUALIFIER'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'event_date'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.datatiering.hot.age.millis'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" '604800000'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.engine.class'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'org.apache.hadoop.hbase.regionserver.CustomTieredStoreEngine'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.compaction.date.tiered.custom.age.limit.millis'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" '604800000'"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  }"})}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"}"})})]})})}),`
`,e.jsx(t,{type:"info",title:"Time Based Priority x Compaction Age Threshold Configurations",children:e.jsx(i.p,{children:`Note that there are two different configurations for defining the hot age threshold. This is
because the Time Based Priority enforcer operates independently of the compaction implementation.`})}),`
`,e.jsx(i.h4,{id:"using-a-custom-value-provider-for-time-based-priority",children:"Using a Custom value provider for Time Based Priority"}),`
`,e.jsxs(i.p,{children:["It's also possible to hook in domain-specific logic for defining the data age of each row to be used for comparing blocks priorities. The Custom Time Based Priority framework defines the ",e.jsx(i.code,{children:"CustomTieredCompactor.TieringValueProvider"})," interface, which can be implemented to provide the specific date value to be used by compaction for grouping the blocks according to the threshold age."]}),`
`,e.jsxs(i.p,{children:["In the following example, the ",e.jsx(i.code,{children:"RowKeyPortionTieringValueProvider"})," implements the ",e.jsx(i.code,{children:"getTieringValue"}),' method. This method parses the date from a segment of the row key value, specifically between positions 14 and 29, using the "yyyyMMddHHmmss" format. The parsed date is then returned as a long timestamp, which is then used by custom tiered compaction to group the blocks based on the defined hot age threshold:']}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"public"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" class"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" RowKeyPortionTieringValueProvider"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" implements"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" CustomTieredCompactor.TieringValueProvider"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" {"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"   private"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" SimpleDateFormat sdf "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" SimpleDateFormat"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"("}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:'"yyyyMMddHHmmss"'}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:");"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"   @"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"Override"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"   public"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" void"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" init"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(Configuration "}),e.jsx(i.span,{style:{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},children:"configuration"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:") "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"throws"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" Exception {}"})]}),`
`,e.jsx(i.span,{className:"line"}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"   @"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"Override"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"   public"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" long"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" getTieringValue"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(Cell "}),e.jsx(i.span,{style:{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},children:"cell"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:") {"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"     byte"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"[] rowArray "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" byte"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"[cell."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getRowLength"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"()];"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"     System."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"arraycopy"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(cell."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getRowArray"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(), cell."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getRowOffset"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(), rowArray, "}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"0"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", cell."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getRowLength"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"());"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"     String datePortion "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" Bytes."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"toString"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(rowArray)."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"substring"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"("}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"14"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", "}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"29"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:")."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"trim"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"     try"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" {"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"       return"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" sdf."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"parse"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(datePortion)."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getTime"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"     } "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"catch"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" (ParseException "}),e.jsx(i.span,{style:{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},children:"e"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:") {"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"       //handle error"})}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"     }"})}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"     return"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" Long.MAX_VALUE;"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"   }"})}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"}"})})]})})}),`
`,e.jsx(i.p,{children:"The Tiering Value Provider above can then be configured for Time Based Priority as follows:"}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"hbase"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(main)"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"003"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"0"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:">"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" alter "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'orders'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", {NAME "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'cf1'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  CONFIGURATION "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"=>"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" {"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'hbase.hstore.datatiering.type'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'CUSTOM'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.custom-tiering-value.provider.class'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"      'org.apache.hbase.client.example.RowKeyPortionTieringValueProvider'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.datatiering.hot.age.millis'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" '604800000'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.engine.class'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" 'org.apache.hadoop.hbase.regionserver.CustomTieredStoreEngine'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"    'hbase.hstore.compaction.date.tiered.custom.age.limit.millis'"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" =>"}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:" '604800000'"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  }"})}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"}"})})]})})}),`
`,e.jsx(t,{type:"info",children:e.jsx(i.p,{children:`Upon enabling Custom Time Based Priority (either the custom qualifier or custom value provider) in
the column family configuration, it is imperative that major compaction be executed twice on the
specified tables to ensure the effective application of the newly configured priorities within the
bucket cache.`})}),`
`,e.jsxs(t,{type:"info",children:[e.jsxs(i.p,{children:["Time Based Priority was originally implemented with the cell timestamp strategy only. The original design covering cell timestamp based strategy is available ",e.jsx(i.a,{href:"https://docs.google.com/document/d/1Qd3kvZodBDxHTFCIRtoePgMbvyuUSxeydi2SEWQFQro/edit?tab=t.0#heading=h.gjdgxs",children:"here"}),"."]}),e.jsxs(i.p,{children:["The second phase including the two custom strategies mentioned above is detailed in ",e.jsx(i.a,{href:"https://docs.google.com/document/d/1uBGIO9IQ-FbSrE5dnUMRtQS23NbCbAmRVDkAOADcU_E/edit?tab=t.0",children:"this separate design doc"}),"."]})]}),`
`,e.jsx(i.h3,{id:"compressed-blockcache",children:"Compressed BlockCache"}),`
`,e.jsxs(i.p,{children:[e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-11331",children:"HBASE-11331"})," introduced lazy BlockCache decompression, more simply referred to as compressed BlockCache. When compressed BlockCache is enabled data and encoded data blocks are cached in the BlockCache in their on-disk format, rather than being decompressed and decrypted before caching."]}),`
`,e.jsx(i.p,{children:"For a RegionServer hosting more data than can fit into cache, enabling this feature with SNAPPY compression has been shown to result in 50% increase in throughput and 30% improvement in mean latency while, increasing garbage collection by 80% and increasing overall CPU load by 2%. See HBASE-11331 for more details about how performance was measured and achieved. For a RegionServer hosting data that can comfortably fit into cache, or if your workload is sensitive to extra CPU or garbage-collection load, you may receive less benefit."}),`
`,e.jsxs(i.p,{children:["The compressed BlockCache is disabled by default. To enable it, set ",e.jsx(i.code,{children:"hbase.block.data.cachecompressed"})," to ",e.jsx(i.code,{children:"true"})," in ",e.jsx(i.em,{children:"hbase-site.xml"})," on all RegionServers."]}),`
`,e.jsx(i.h3,{id:"cache-aware-load-balancer",children:"Cache Aware Load Balancer"}),`
`,e.jsxs(i.p,{children:["Depending on the data size and the configured cache size, the cache warm up can take anywhere from a few minutes to a few hours. This becomes even more critical for HBase deployments over cloud storage, where compute is separated from storage. Doing this everytime the region server starts can be a very expensive process. To eliminate this, ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-27313",children:"HBASE-27313"})," implemented the cache persistence feature where the region servers periodically persist the blocks cached in the bucket cache. This persisted information is then used to resurrect the cache in the event of a region server restart because of normal restart or crash."]}),`
`,e.jsxs(i.p,{children:[e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-27999",children:"HBASE-27999"})," implements the cache aware load balancer, which adds to the load balancer the ability to consider the cache allocation of each region on region servers when calculating a new assignment plan, using the region/region server cache allocation information reported by region servers to calculate the percentage of HFiles cached for each region on the hosting server. This information is then used by the balancer as a factor when deciding on an optimal, new assignment plan."]}),`
`,e.jsx(i.p,{children:"The master node captures the caching information from all the region servers and uses this information to decide on new region assignments while ensuring a minimal impact on the current cache allocation. A region is assigned to the region server where it has a better cache ratio as compared to the region server where it is currently hosted."}),`
`,e.jsx(i.p,{children:"The CacheAwareLoadBalancer uses two cost elements for deciding the region allocation. These are described below:"}),`
`,e.jsxs(i.ol,{children:[`
`,e.jsxs(i.li,{children:[e.jsx(i.strong,{children:"Cache Cost"}),e.jsx(i.br,{}),`
`,"The cache cost is calculated as the percentage of data for a region cached on the region server where it is either currently hosted or was previously hosted. A region may have multiple HFiles, each of different sizes. A HFile is considered to be fully prefetched when all the data blocks in this file are in the cache. The region server hosting this region calculates the ratio of number of HFiles fully cached in the cache to the total number of HFiles in the region. This ratio will vary from 0 (region hosted on this server, but none of its HFiles are cached into the cache) to 1 (region hosted on this server and all the HFiles for this region are cached into the cache).",e.jsx(i.br,{}),`
`,"Every region server maintains this information for all the regions currently hosted there. In addition to that, this cache ratio is also maintained for the regions which were previously hosted on this region server giving historical information about the regions."]}),`
`,e.jsx(i.li,{children:"Skewness Cost"}),`
`]}),`
`,e.jsx(i.p,{children:"The cache aware balancer will consider cache cost with the skewness cost to decide on the region assignment plan under following conditions:"}),`
`,e.jsxs(i.ol,{children:[`
`,e.jsx(i.li,{children:"There is an idle server in the cluster. This can happen when an existing server is restarted or a new server is added to the cluster."}),`
`,e.jsxs(i.li,{children:["When the cost of maintaining the balance in the cluster is greater than the minimum threshold defined by the configuration ",e.jsx(i.em,{children:"hbase.master.balancer.stochastic.minCostNeedBalance"}),"."]}),`
`]}),`
`,e.jsx(i.p,{children:"The CacheAwareLoadBalancer can be enabled in the cluster by setting the following configuration properties in the master master configuration:"}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.master.loadbalancer.class</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">org.apache.hadoop.hbase.master.balancer.CacheAwareLoadBalancer</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.bucketcache.persistent.path</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">/path/to/bucketcache_persistent_file</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsxs(i.p,{children:['Within HBASE-29168, the CacheAwareLoadBalancer implements region move throttling. This mitigates the impact of "losing" cache factor when balancing mainly due to region skewness, i.e. when new region servers are added to the cluster, a large bulk of cached regions may move to the new servers at once, which can cause noticeable read performance impacts for cache sensitive use cases. The throttling sleep time is determined by the ',e.jsx(i.strong,{children:"hbase.master.balancer.move.throttlingMillis"})," property, and it defaults to 60000 millis. If a region planned to be moved has a cache ratio on the target server above the thershold configurable by the ",e.jsx(i.strong,{children:"hbase.master.balancer.stochastic.throttling.cacheRatio"})," property (80% by default), no throttling will be applied in this region move."]}),`
`,e.jsx(i.h2,{id:"regionserver-splitting-implementation",children:"RegionServer Splitting Implementation"}),`
`,e.jsxs(i.p,{children:["As write requests are handled by the region server, they accumulate in an in-memory storage system called the ",e.jsx(i.em,{children:"memstore"}),". Once the memstore fills, its content are written to disk as additional store files. This event is called a ",e.jsx(i.em,{children:"memstore flush"}),". As store files accumulate, the RegionServer will ",e.jsx(i.a,{href:"/docs/architecture/regions#compaction",children:"compact"})," them into fewer, larger files. After each flush or compaction finishes, the amount of data stored in the region has changed. The RegionServer consults the region split policy to determine if the region has grown too large or should be split for another policy-specific reason. A region split request is enqueued if the policy recommends it."]}),`
`,e.jsxs(i.p,{children:["Logically, the process of splitting a region is simple. We find a suitable point in the keyspace of the region where we should divide the region in half, then split the region's data into two new regions at that point. The details of the process however are not simple. When a split happens, the newly created ",e.jsx(i.em,{children:"daughter regions"})," do not rewrite all the data into new files immediately. Instead, they create small files similar to symbolic link files, named ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/Reference.html",children:"Reference files"}),", which point to either the top or bottom part of the parent store file according to the split point. The reference file is used just like a regular data file, but only half of the records are considered. The region can only be split if there are no more references to the immutable data files of the parent region. Those reference files are cleaned gradually by compactions, so that the region will stop referring to its parents files, and can be split further."]}),`
`,e.jsxs(i.p,{children:["Although splitting the region is a local decision made by the RegionServer, the split process itself must coordinate with many actors. The RegionServer notifies the Master before and after the split, updates the ",e.jsx(i.code,{children:".META."}),' table so that clients can discover the new daughter regions, and rearranges the directory structure and data files in HDFS. Splitting is a multi-task process. To enable rollback in case of an error, the RegionServer keeps an in-memory journal about the execution state. The steps taken by the RegionServer to execute the split are illustrated in the "RegionServer Split Process" schema below. Each step is labeled with its step number. Actions from RegionServers or Master are shown in red, while actions from the clients are shown in green.']}),`
`,e.jsx(i.p,{children:e.jsx(i.img,{alt:"Region Split Process",src:h,placeholder:"blur"})}),`
`,e.jsxs(i.ol,{children:[`
`,e.jsxs(i.li,{children:["The RegionServer decides locally to split the region, and prepares the split. ",e.jsx(i.strong,{children:"THE SPLIT TRANSACTION IS STARTED."})," As a first step, the RegionServer acquires a shared read lock on the table to prevent schema modifications during the splitting process. Then it creates a znode in zookeeper under ",e.jsx(i.code,{children:"/hbase/region-in-transition/region-name"}),", and sets the znode's state to ",e.jsx(i.code,{children:"SPLITTING"}),"."]}),`
`,e.jsxs(i.li,{children:["The Master learns about this znode, since it has a watcher for the parent ",e.jsx(i.code,{children:"region-in-transition"})," znode."]}),`
`,e.jsxs(i.li,{children:["The RegionServer creates a sub-directory named ",e.jsx(i.code,{children:".splits"})," under the parent's ",e.jsx(i.code,{children:"region"})," directory in HDFS."]}),`
`,e.jsxs(i.li,{children:["The RegionServer closes the parent region and marks the region as offline in its local data structures. ",e.jsx(i.strong,{children:"THE SPLITTING REGION IS NOW OFFLINE."})," At this point, client requests coming to the parent region will throw ",e.jsx(i.code,{children:"NotServingRegionException"}),". The client will retry with some backoff. The closing region is flushed."]}),`
`,e.jsxs(i.li,{children:["The RegionServer creates region directories under the ",e.jsx(i.code,{children:".splits"})," directory, for daughter regions A and B, and creates necessary data structures. Then it splits the store files, in the sense that it creates two Reference files per store file in the parent region. Those reference files will point to the parent region's files."]}),`
`,e.jsx(i.li,{children:"The RegionServer creates the actual region directory in HDFS, and moves the reference files for each daughter."}),`
`,e.jsxs(i.li,{children:["The RegionServer sends a ",e.jsx(i.code,{children:"Put"})," request to the ",e.jsx(i.code,{children:".META."})," table, to set the parent as offline in the ",e.jsx(i.code,{children:".META."})," table and add information about daughter regions. At this point, there won't be individual entries in ",e.jsx(i.code,{children:".META."})," for the daughters. Clients will see that the parent region is split if they scan ",e.jsx(i.code,{children:".META."}),", but won't know about the daughters until they appear in ",e.jsx(i.code,{children:".META."}),". Also, if this ",e.jsx(i.code,{children:"Put"})," to ",e.jsx(i.code,{children:".META"}),". succeeds, the parent will be effectively split. If the RegionServer fails before this RPC succeeds, Master and the next Region Server opening the region will clean dirty state about the region split. After the ",e.jsx(i.code,{children:".META."})," update, though, the region split will be rolled-forward by Master."]}),`
`,e.jsx(i.li,{children:"The RegionServer opens daughters A and B in parallel."}),`
`,e.jsxs(i.li,{children:["The RegionServer adds the daughters A and B to ",e.jsx(i.code,{children:".META."}),", together with information that it hosts the regions. ",e.jsx(i.strong,{children:"THE SPLIT REGIONS (DAUGHTERS WITH REFERENCES TO PARENT) ARE NOW ONLINE."})," After this point, clients can discover the new regions and issue requests to them. Clients cache the ",e.jsx(i.code,{children:".META."})," entries locally, but when they make requests to the RegionServer or ",e.jsx(i.code,{children:".META."}),", their caches will be invalidated, and they will learn about the new regions from ",e.jsx(i.code,{children:".META."}),"."]}),`
`,e.jsxs(i.li,{children:["The RegionServer updates znode ",e.jsx(i.code,{children:"/hbase/region-in-transition/region-name"})," in ZooKeeper to state ",e.jsx(i.code,{children:"SPLIT"}),", so that the master can learn about it. The balancer can freely re-assign the daughter regions to other region servers if necessary. ",e.jsx(i.strong,{children:"THE SPLIT TRANSACTION IS NOW FINISHED."})]}),`
`,e.jsxs(i.li,{children:["After the split, ",e.jsx(i.code,{children:".META."})," and HDFS will still contain references to the parent region. Those references will be removed when compactions in daughter regions rewrite the data files. Garbage collection tasks in the master periodically check whether the daughter regions still refer to the parent region's files. If not, the parent region will be removed."]}),`
`]}),`
`,e.jsx(i.h2,{id:"write-ahead-log-wal",children:"Write Ahead Log (WAL)"}),`
`,e.jsx(i.h3,{id:"purpose",children:"Purpose"}),`
`,e.jsxs(i.p,{children:["The ",e.jsx(i.em,{children:"Write Ahead Log (WAL)"})," records all changes to data in HBase, to file-based storage. Under normal operations, the WAL is not needed because data changes move from the MemStore to StoreFiles. However, if a RegionServer crashes or becomes unavailable before the MemStore is flushed, the WAL ensures that the changes to the data can be replayed. If writing to the WAL fails, the entire operation to modify the data fails."]}),`
`,e.jsxs(i.p,{children:["HBase uses an implementation of the ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/wal/WAL.html",children:"WAL"})," interface. Usually, there is only one instance of a WAL per RegionServer. An exception is the RegionServer that is carrying ",e.jsx(i.em,{children:"hbase:meta"}),"; the ",e.jsx(i.em,{children:"meta"})," table gets its own dedicated WAL. The RegionServer records Puts and Deletes to its WAL, before recording them these Mutations ",e.jsx(i.a,{href:"/docs/architecture/regions#memstore",children:"MemStore"})," for the affected ",e.jsx(i.a,{href:"/docs/architecture/regions#store",children:"Store"}),"."]}),`
`,e.jsx(t,{type:"info",title:"The HLog",children:e.jsxs(i.p,{children:["Prior to 2.0, the interface for WALs in HBase was named ",e.jsx(i.code,{children:"HLog"}),`. In 0.94, HLog was the name of the
implementation of the WAL. You will likely find references to the HLog in documentation tailored
to these older versions.`]})}),`
`,e.jsxs(i.p,{children:["The WAL resides in HDFS in the ",e.jsx(i.em,{children:"/hbase/WALs/"})," directory, with subdirectories per RegionServer."]}),`
`,e.jsxs(i.p,{children:["For more general information about the concept of write ahead logs, see the Wikipedia ",e.jsx(i.a,{href:"http://en.wikipedia.org/wiki/Write-ahead_logging",children:"Write-Ahead Log"})," article."]}),`
`,e.jsx(i.h3,{id:"wal-providers",children:"WAL Providers"}),`
`,e.jsxs(i.p,{children:["In HBase, there are a number of WAL implementations (or 'Providers'). Each is known by a short name label (that unfortunately is not always descriptive). You set the provider in ",e.jsx(i.em,{children:"hbase-site.xml"})," passing the WAL provider short-name as the value on the ",e.jsx(i.em,{children:"hbase.wal.provider"})," property (Set the provider for ",e.jsx(i.em,{children:"hbase:meta"})," using the ",e.jsx(i.em,{children:"hbase.wal.meta_provider"})," property, otherwise it uses the same provider configured by ",e.jsx(i.em,{children:"hbase.wal.provider"}),")."]}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:[e.jsx(i.em,{children:"asyncfs"}),": The ",e.jsx(i.strong,{children:"default"}),". New since hbase-2.0.0 (HBASE-15536, HBASE-14790). This ",e.jsx(i.em,{children:"AsyncFSWAL"}),' provider, as it identifies itself in RegionServer logs, is built on a new non-blocking dfsclient implementation. It is currently resident in the hbase codebase but intent is to move it back up into HDFS itself. WALs edits are written concurrently ("fan-out") style to each of the WAL-block replicas on each DataNode rather than in a chained pipeline as the default client does. Latencies should be better. See ',e.jsx(i.a,{href:"https://www.slideshare.net/HBaseCon/apache-hbase-improvements-and-practices-at-xiaomi",children:"Apache HBase Improvements and Practices at Xiaomi"})," at slide 14 onward for more detail on implementation."]}),`
`,e.jsxs(i.li,{children:[e.jsx(i.em,{children:"filesystem"}),": This was the default in hbase-1.x releases. It is built on the blocking ",e.jsx(i.em,{children:"DFSClient"})," and writes to replicas in classic ",e.jsx(i.em,{children:"DFSCLient"})," pipeline mode. In logs it identifies as ",e.jsx(i.em,{children:"FSHLog"})," or ",e.jsx(i.em,{children:"FSHLogProvider"}),"."]}),`
`,e.jsxs(i.li,{children:[e.jsx(i.em,{children:"multiwal"}),": This provider is made of multiple instances of ",e.jsx(i.em,{children:"asyncfs"})," or ",e.jsx(i.em,{children:"filesystem"}),". See the next section for more on ",e.jsx(i.em,{children:"multiwal"}),"."]}),`
`]}),`
`,e.jsx(i.p,{children:"Look for the lines like the below in the RegionServer log to see which provider is in place (The below shows the default AsyncFSWALProvider):"}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsx(i.code,{children:e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"2018"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"-"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"04"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"-"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"02"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:" 13"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"22"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"37"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"983"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" INFO"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  [regionserver"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"/"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"ve0528"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"16020"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"] wal.WALFactory"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:":"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" Instantiating WALProvider of type "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"class"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" org"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:".apache.hadoop.hbase.wal.AsyncFSWALProvider"})]})})})}),`
`,e.jsx(t,{type:"info",children:e.jsxs(i.p,{children:["As the ",e.jsx(i.em,{children:"AsyncFSWAL"}),` hacks into the internal of DFSClient implementation, it will be easily broken
by upgrading the hadoop dependencies, even for a simple patch release. So if you do not specify
the wal provider explicitly, we will first try to use the `,e.jsx(i.em,{children:"asyncfs"}),`, if failed, we will fall back
to use `,e.jsx(i.em,{children:"filesystem"}),`. And notice that this may not always work, so if you still have problem
starting HBase due to the problem of starting `,e.jsx(i.em,{children:"AsyncFSWAL"}),", please specify ",e.jsx(i.em,{children:"filesystem"}),` explicitly
in the config file.`]})}),`
`,e.jsx(t,{type:"info",children:e.jsxs(i.p,{children:[`EC support has been added to hadoop-3.x, and it is incompatible with WAL as the EC output stream
does not support hflush/hsync. In order to create a non-EC file in an EC directory, we need to use
the new builder-based create API for `,e.jsx(i.em,{children:"FileSystem"}),`, but it is only introduced in hadoop-2.9+ and
for HBase we still need to support hadoop-2.7.x. So please do not enable EC for the WAL directory
until we find a way to deal with it.`]})}),`
`,e.jsx(i.h3,{id:"multiwal",children:"MultiWAL"}),`
`,e.jsx(i.p,{children:"With a single WAL per RegionServer, the RegionServer must write to the WAL serially, because HDFS files must be sequential. This causes the WAL to be a performance bottleneck."}),`
`,e.jsxs(i.p,{children:["HBase 1.0 introduces support MultiWal in ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-5699",children:"HBASE-5699"}),". MultiWAL allows a RegionServer to write multiple WAL streams in parallel, by using multiple pipelines in the underlying HDFS instance, which increases total throughput during writes. This parallelization is done by partitioning incoming edits by their Region. Thus, the current implementation will not help with increasing the throughput to a single Region."]}),`
`,e.jsx(i.p,{children:"RegionServers using the original WAL implementation and those using the MultiWAL implementation can each handle recovery of either set of WALs, so a zero-downtime configuration update is possible through a rolling restart."}),`
`,e.jsx(i.h4,{id:"configure-multiwal",children:"Configure MultiWAL"}),`
`,e.jsxs(i.p,{children:["To configure MultiWAL for a RegionServer, set the value of the property ",e.jsx(i.code,{children:"hbase.wal.provider"})," to ",e.jsx(i.code,{children:"multiwal"})," by pasting in the following XML:"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.wal.provider</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">multiwal</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsx(i.p,{children:"Restart the RegionServer for the changes to take effect."}),`
`,e.jsx(i.p,{children:"To disable MultiWAL for a RegionServer, unset the property and restart the RegionServer."}),`
`,e.jsx(i.h3,{id:"wal-flushing",children:"WAL Flushing"}),`
`,e.jsx(i.p,{children:"TODO (describe)."}),`
`,e.jsx(i.h3,{id:"wal-splitting",children:"WAL Splitting"}),`
`,e.jsxs(i.p,{children:["A RegionServer serves many regions. All of the regions in a region server share the same active WAL file. Each edit in the WAL file includes information about which region it belongs to. When a region is opened, the edits in the WAL file which belong to that region need to be replayed. Therefore, edits in the WAL file must be grouped by region so that particular sets can be replayed to regenerate the data in a particular region. The process of grouping the WAL edits by region is called ",e.jsx(i.em,{children:"log splitting"}),". It is a critical process for recovering data if a region server fails."]}),`
`,e.jsx(i.p,{children:"Log splitting is done by the HMaster during cluster start-up or by the ServerShutdownHandler as a region server shuts down. So that consistency is guaranteed, affected regions are unavailable until data is restored. All WAL edits need to be recovered and replayed before a given region can become available again. As a result, regions affected by log splitting are unavailable until the process completes."}),`
`,e.jsx(i.h4,{id:"procedure-log-splitting-step-by-step",children:"Procedure: Log Splitting, Step by Step"}),`
`,e.jsxs(r,{children:[e.jsxs(n,{children:[e.jsxs(i.h5,{id:"the-hbasewalshostportstartcode-directory-is-renamed",children:["The ",e.jsx(i.code,{children:"/hbase/WALs/HOST,PORT,STARTCODE"})," directory is renamed"]}),e.jsx(i.p,{children:"Renaming the directory is important because a RegionServer may still be up and accepting requests even if the HMaster thinks it is down. If the RegionServer does not respond immediately and does not heartbeat its ZooKeeper session, the HMaster may interpret this as a RegionServer failure. Renaming the logs directory ensures that existing, valid WAL files which are still in use by an active but busy RegionServer are not written to by accident."}),e.jsx(i.p,{children:"The new directory is named according to the following pattern:"}),e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsx(i.code,{children:e.jsx(i.span,{className:"line",children:e.jsx(i.span,{children:"/hbase/WALs/HOST,PORT,STARTCODE-splitting"})})})})}),e.jsx(i.p,{children:"An example of such a renamed directory might look like the following:"}),e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsx(i.code,{children:e.jsx(i.span,{className:"line",children:e.jsx(i.span,{children:"/hbase/WALs/srv.example.com,60020,1254173957298-splitting"})})})})})]}),e.jsxs(n,{children:[e.jsx(i.h5,{id:"each-log-file-is-split-one-at-a-time",children:"Each log file is split, one at a time"}),e.jsx(i.p,{children:"The log splitter reads the log file one edit entry at a time and puts each edit entry into the buffer corresponding to the edit's region. At the same time, the splitter starts several writer threads. Writer threads pick up a corresponding buffer and write the edit entries in the buffer to a temporary recovered edit file. The temporary edit file is stored to disk with the following naming pattern:"}),e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsx(i.code,{children:e.jsx(i.span,{className:"line",children:e.jsx(i.span,{children:"/hbase/TABLE_NAME/REGION_ID/recovered.edits/.temp"})})})})}),e.jsxs(i.p,{children:["This file is used to store all the edits in the WAL log for this region. After log splitting completes, the ",e.jsx(i.em,{children:".temp"})," file is renamed to the sequence ID of the first log written to the file."]}),e.jsx(i.p,{children:"To determine whether all edits have been written, the sequence ID is compared to the sequence of the last edit that was written to the HFile. If the sequence of the last edit is greater than or equal to the sequence ID included in the file name, it is clear that all writes from the edit file have been completed."})]}),e.jsxs(n,{children:[e.jsx(i.h5,{id:"after-log-splitting-is-complete-each-affected-region-is-assigned-to-a-regionserver",children:"After log splitting is complete, each affected region is assigned to a RegionServer"}),e.jsxs(i.p,{children:["When the region is opened, the ",e.jsx(i.em,{children:"recovered.edits"})," folder is checked for recovered edits files. If any such files are present, they are replayed by reading the edits and saving them to the MemStore. After all edit files are replayed, the contents of the MemStore are written to disk (HFile) and the edit files are deleted."]})]})]}),`
`,e.jsx(i.h4,{id:"handling-of-errors-during-log-splitting",children:"Handling of Errors During Log Splitting"}),`
`,e.jsxs(i.p,{children:["If you set the ",e.jsx(i.code,{children:"hbase.hlog.split.skip.errors"})," option to ",e.jsx(i.code,{children:"true"}),", errors are treated as follows:"]}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsx(i.li,{children:"Any error encountered during splitting will be logged."}),`
`,e.jsxs(i.li,{children:["The problematic WAL log will be moved into the ",e.jsx(i.em,{children:".corrupt"})," directory under the hbase ",e.jsx(i.code,{children:"rootdir"}),","]}),`
`,e.jsx(i.li,{children:"Processing of the WAL will continue"}),`
`]}),`
`,e.jsxs(i.p,{children:["If the ",e.jsx(i.code,{children:"hbase.hlog.split.skip.errors"})," option is set to ",e.jsx(i.code,{children:"false"}),", the default, the exception will be propagated and the split will be logged as failed. See ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-2958",children:"HBASE-2958 When hbase.hlog.split.skip.errors is set to false, we fail the split but that's it"}),". We need to do more than just fail split if this flag is set."]}),`
`,e.jsx(i.h4,{id:"how-eofexceptions-are-treated-when-splitting-a-crashed-regionservers-wals",children:"How EOFExceptions are treated when splitting a crashed RegionServer's WALs"}),`
`,e.jsxs(i.p,{children:["If an EOFException occurs while splitting logs, the split proceeds even when ",e.jsx(i.code,{children:"hbase.hlog.split.skip.errors"})," is set to ",e.jsx(i.code,{children:"false"}),". An EOFException while reading the last log in the set of files to split is likely, because the RegionServer was likely in the process of writing a record at the time of a crash. For background, see ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-2643",children:"HBASE-2643 Figure how to deal with eof splitting logs"})]}),`
`,e.jsx(i.h4,{id:"performance-improvements-during-log-splitting",children:"Performance Improvements during Log Splitting"}),`
`,e.jsxs(i.p,{children:["WAL log splitting and recovery can be resource intensive and take a long time, depending on the number of RegionServers involved in the crash and the size of the regions. ",e.jsx(i.a,{href:"/docs/architecture/regionserver#enabling-or-disabling-distributed-log-splitting",children:"Distributed log splitting"})," was developed to improve performance during log splitting."]}),`
`,e.jsx(i.h4,{id:"enabling-or-disabling-distributed-log-splitting",children:"Enabling or Disabling Distributed Log Splitting"}),`
`,e.jsxs(i.p,{children:["Distributed log processing is enabled by default since HBase 0.92. The setting is controlled by the ",e.jsx(i.code,{children:"hbase.master.distributed.log.splitting"})," property, which can be set to ",e.jsx(i.code,{children:"true"})," or ",e.jsx(i.code,{children:"false"}),", but defaults to ",e.jsx(i.code,{children:"true"}),"."]}),`
`,e.jsx(i.h3,{id:"wal-splitting-based-on-procedurev2",children:"WAL splitting based on procedureV2"}),`
`,e.jsx(i.p,{children:"After HBASE-20610, we introduce a new way to do WAL splitting coordination by procedureV2 framework. This can simplify the process of WAL splitting and no need to connect zookeeper any more."}),`
`,e.jsx(i.h4,{id:"background-toc",children:"Background"}),`
`,e.jsx(i.p,{children:"Currently, splitting WAL processes are coordinated by zookeeper. Each region server are trying to grab tasks from zookeeper. And the burden becomes heavier when the number of region server increase."}),`
`,e.jsx(i.h4,{id:"implementation-on-master-side-toc",children:"Implementation on Master side"}),`
`,e.jsx(i.p,{children:"During ServerCrashProcedure, SplitWALManager will create one SplitWALProcedure for each WAL file which should be split. Then each SplitWALProcedure will spawn a SplitWalRemoteProcedure to send the request to region server. SplitWALProcedure is a StateMachineProcedure and here is the state transfer diagram."}),`
`,e.jsx(i.p,{children:e.jsx(i.img,{alt:"WAL splitting",src:l,placeholder:"blur"})}),`
`,e.jsx(i.h4,{id:"implementation-on-region-server-side-toc",children:"Implementation on Region Server side"}),`
`,e.jsx(i.p,{children:"Region Server will receive a SplitWALCallable and execute it, which is much more straightforward than before. It will return null if success and return exception if there is any error."}),`
`,e.jsx(i.h4,{id:"architecture-regionserver-wal-splitting-based-on-procedurev2-performance",children:"Performance"}),`
`,e.jsx(i.p,{children:"According to tests on a cluster which has 5 regionserver and 1 master. procedureV2 coordinated WAL splitting has a better performance than ZK coordinated WAL splitting no master when restarting the whole cluster or one region server crashing."}),`
`,e.jsx(i.h4,{id:"enable-this-feature-toc",children:"Enable this feature"}),`
`,e.jsx(i.p,{children:"To enable this feature, first we should ensure our package of HBase already contains these code. If not, please upgrade the package of HBase cluster without any configuration change first. Then change configuration 'hbase.split.wal.zk.coordinated' to false. Rolling upgrade the master with new configuration. Now WAL splitting are handled by our new implementation. But region server are still trying to grab tasks from zookeeper, we can rolling upgrade the region servers with the new configuration to stop that."}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:["Steps as follows:",`
`,e.jsxs(i.ul,{children:[`
`,e.jsx(i.li,{children:"Upgrade whole cluster to get the new Implementation."}),`
`,e.jsx(i.li,{children:"Upgrade Master with new configuration 'hbase.split.wal.zk.coordinated'=false."}),`
`,e.jsx(i.li,{children:"Upgrade region server to stop grab tasks from zookeeper."}),`
`]}),`
`]}),`
`]}),`
`,e.jsx(i.h3,{id:"wal-compression",children:"WAL Compression"}),`
`,e.jsxs(i.p,{children:["The content of the WAL can be compressed using LRU Dictionary compression. This can be used to speed up WAL replication to different datanodes. The dictionary can store up to 2",e.jsx("sup",{children:"15"})," elements; eviction starts after this number is exceeded."]}),`
`,e.jsxs(i.p,{children:["To enable WAL compression, set the ",e.jsx(i.code,{children:"hbase.regionserver.wal.enablecompression"})," property to ",e.jsx(i.code,{children:"true"}),". The default value for this property is ",e.jsx(i.code,{children:"false"}),". By default, WAL tag compression is turned on when WAL compression is enabled. You can turn off WAL tag compression by setting the ",e.jsx(i.code,{children:"hbase.regionserver.wal.tags.enablecompression"})," property to 'false'."]}),`
`,e.jsx(i.p,{children:"A possible downside to WAL compression is that we lose more data from the last block in the WAL if it is ill-terminated mid-write. If entries in this last block were added with new dictionary entries but we failed persist the amended dictionary because of an abrupt termination, a read of this last block may not be able to resolve last-written entries."}),`
`,e.jsx(i.h3,{id:"durability",children:"Durability"}),`
`,e.jsxs(i.p,{children:["It is possible to set ",e.jsx(i.em,{children:"durability"})," on each Mutation or on a Table basis. Options include:"]}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:[e.jsx(i.em,{children:"SKIP_WAL"}),": Do not write Mutations to the WAL (See the next section, ",e.jsx(i.a,{href:"/docs/architecture/regionserver#disabling-the-wal",children:"Disabling the WAL"}),")."]}),`
`,e.jsxs(i.li,{children:[e.jsx(i.em,{children:"ASYNC_WAL"}),": Write the WAL asynchronously; do not hold-up clients waiting on the sync of their write to the filesystem but return immediately. The edit becomes visible. Meanwhile, in the background, the Mutation will be flushed to the WAL at some time later. This option currently may lose data. See HBASE-16689."]}),`
`,e.jsxs(i.li,{children:[e.jsx(i.em,{children:"SYNC_WAL"}),": The ",e.jsx(i.strong,{children:"default"}),". Each edit is sync'd to HDFS before we return success to the client."]}),`
`,e.jsxs(i.li,{children:[e.jsx(i.em,{children:"FSYNC_WAL"}),": Each edit is fsync'd to HDFS and the filesystem before we return success to the client."]}),`
`]}),`
`,e.jsxs(i.p,{children:["Do not confuse the ",e.jsx(i.em,{children:"ASYNC_WAL"})," option on a Mutation or Table with the ",e.jsx(i.em,{children:"AsyncFSWAL"})," writer; they are distinct options unfortunately closely named"]}),`
`,e.jsx(i.h3,{id:"custom-wal-directory",children:"Custom WAL Directory"}),`
`,e.jsx(i.p,{children:"HBASE-17437 added support for specifying a WAL directory outside the HBase root directory or even in a different FileSystem since 1.3.3/2.0+. Some FileSystems (such as Amazon S3) don't support append or consistent writes, in such scenario WAL directory needs to be configured in a different FileSystem to avoid loss of writes."}),`
`,e.jsx(i.p,{children:"Following configurations are added to accomplish this:"}),`
`,e.jsxs(i.ol,{children:[`
`,e.jsxs(i.li,{children:[e.jsx(i.code,{children:"hbase.wal.dir"}),e.jsx(i.br,{}),`
`,"This defines where the root WAL directory is located, could be on a different FileSystem than the root directory. WAL directory can not be set to a subdirectory of the root directory. The default value of this is the root directory if unset."]}),`
`,e.jsxs(i.li,{children:[e.jsx(i.code,{children:"hbase.rootdir.perms"}),e.jsx(i.br,{}),`
`,"Configures FileSystem permissions to set on the root directory. This is '700' by default."]}),`
`,e.jsxs(i.li,{children:[e.jsx(i.code,{children:"hbase.wal.dir.perms"}),e.jsx(i.br,{}),`
`,"Configures FileSystem permissions to set on the WAL directory FileSystem. This is '700' by default."]}),`
`]}),`
`,e.jsx(t,{type:"info",children:e.jsx(i.p,{children:`While migrating to custom WAL dir (outside the HBase root directory or a different FileSystem)
existing WAL files must be copied manually to new WAL dir, otherwise it may lead to data
loss/inconsistency as HMaster has no information about previous WAL directory.`})}),`
`,e.jsx(i.h3,{id:"disabling-the-wal",children:"Disabling the WAL"}),`
`,e.jsx(i.p,{children:"It is possible to disable the WAL, to improve performance in certain specific situations. However, disabling the WAL puts your data at risk. The only situation where this is recommended is during a bulk load. This is because, in the event of a problem, the bulk load can be re-run with no risk of data loss."}),`
`,e.jsxs(i.p,{children:["The WAL is disabled by calling the HBase client field ",e.jsx(i.code,{children:"Mutation.writeToWAL(false)"}),". Use the ",e.jsx(i.code,{children:"Mutation.setDurability(Durability.SKIP_WAL)"})," and Mutation.getDurability() methods to set and get the field's value. There is no way to disable the WAL for only a specific table."]}),`
`,e.jsx(t,{type:"warn",children:e.jsx(i.p,{children:"If you disable the WAL for anything other than bulk loads, your data is at risk."})})]})}function k(s={}){const{wrapper:i}=s.components||{};return i?e.jsx(i,{...s,children:e.jsx(o,{...s})}):o(s)}function a(s,i){throw new Error("Expected component `"+s+"` to be defined: you likely forgot to import, pass, or provide it.")}export{g as _markdown,k as default,u as extractedReferences,p as frontmatter,m as structuredData,f as toc};
