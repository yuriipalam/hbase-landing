import{j as e}from"./chunk-OIYGIGL5-BFuAKb0n.js";let o=`The HBase client finds the RegionServers that are serving the particular row range of interest. It does this by querying the \`hbase:meta\` table. See [hbase:meta](/docs/architecture/catalog-tables#hbasemeta) for details. After locating the required region(s), the client contacts the RegionServer serving that region, rather than going through the master, and issues the read or write request. This information is cached in the client so that subsequent requests need not go through the lookup process. Should a region be reassigned either by the master load balancer or because a RegionServer has died, the client will requery the catalog tables to determine the new location of the user region.

See [Runtime Impact](/docs/architecture/master#runtime-impact) for more information about the impact of the Master on HBase Client communication.

Administrative functions are done via an instance of [Admin](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Admin.html)

## Cluster Connections

The API changed in HBase 1.0. For connection configuration information, see [Client configuration and dependencies connecting to an HBase cluster](/docs/configuration/default#client-configuration-and-dependencies-connecting-to-an-hbase-cluster).

### API as of HBase 1.0.0

It's been cleaned up and users are returned Interfaces to work against rather than particular types. In HBase 1.0, obtain a \`Connection\` object from \`ConnectionFactory\` and thereafter, get from it instances of \`Table\`, \`Admin\`, and \`RegionLocator\` on an as-need basis. When done, close the obtained instances. Finally, be sure to cleanup your \`Connection\` instance before exiting. \`Connections\` are heavyweight objects but thread-safe so you can create one for your application and keep the instance around. \`Table\`, \`Admin\` and \`RegionLocator\` instances are lightweight. Create as you go and then let go as soon as you are done by closing them. See the [Client Package Javadoc Description](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/package-summary.html) for example usage of the new HBase 1.0 API.

### API before HBase 1.0.0

Instances of \`HTable\` are the way to interact with an HBase cluster earlier than 1.0.0. *[Table](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html) instances are not thread-safe*. Only one thread can use an instance of Table at any given time. When creating Table instances, it is advisable to use the same [HBaseConfiguration](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/HBaseConfiguration) instance. This will ensure sharing of ZooKeeper and socket instances to the RegionServers which is usually what you want. For example, this is preferred:

\`\`\`java
HBaseConfiguration conf = HBaseConfiguration.create();
HTable table1 = new HTable(conf, "myTable");
HTable table2 = new HTable(conf, "myTable");
\`\`\`

as opposed to this:

\`\`\`java
HBaseConfiguration conf1 = HBaseConfiguration.create();
HTable table1 = new HTable(conf1, "myTable");
HBaseConfiguration conf2 = HBaseConfiguration.create();
HTable table2 = new HTable(conf2, "myTable");
\`\`\`

For more information about how connections are handled in the HBase client, see [ConnectionFactory](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ConnectionFactory.html).

#### Connection Pooling

For applications which require high-end multithreaded access (e.g., web-servers or application servers that may serve many application threads in a single JVM), you can pre-create a \`Connection\`, as shown in the following example:

**Example 24. Pre-Creating a \`Connection\`**

\`\`\`java
// Create a connection to the cluster.
Configuration conf = HBaseConfiguration.create();
try (Connection connection = ConnectionFactory.createConnection(conf);
     Table table = connection.getTable(TableName.valueOf(tablename))) {
  // use table as needed, the table returned is lightweight
}
\`\`\`

<Callout type="warn" title="HTablePool is Deprecated">
  Previous versions of this guide discussed \`HTablePool\`, which was deprecated in HBase 0.94, 0.95,
  and 0.96, and removed in 0.98.1, by
  [HBASE-6580](https://issues.apache.org/jira/browse/HBASE-6580), or \`HConnection\`, which is
  deprecated in HBase 1.0 by \`Connection\`. Please use
  [Connection](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Connection.html)
  instead.
</Callout>

## WriteBuffer and Batch Methods

In HBase 1.0 and later, [HTable](https://hbase.apache.org/1.4/devapidocs/org/apache/hadoop/hbase/client/HTable.html) is deprecated in favor of [Table](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html). \`Table\` does not use autoflush. To do buffered writes, use the BufferedMutator class.

In HBase 2.0 and later, [HTable](https://hbase.apache.org/2.6/devapidocs/org/apache/hadoop/hbase/client/HTable.html) does not use BufferedMutator to execute the \`Put\` operation. Refer to [HBASE-18500](https://issues.apache.org/jira/browse/HBASE-18500) for more information.

For additional information on write durability, review the [ACID semantics](/acid-semantics) page.

For fine-grained control of batching of \`Put\`s or \`Delete\`s, see the [batch](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html#batch\\(java.util.List,java.lang.Object%5B%5D\\)) methods on Table.

## Asynchronous Client

It is a new API introduced in HBase 2.0 which aims to provide the ability to access HBase asynchronously.

You can obtain an \`AsyncConnection\` from \`ConnectionFactory\`, and then get a asynchronous table instance from it to access HBase. When done, close the \`AsyncConnection\` instance(usually when your program exits).

For the asynchronous table, most methods have the same meaning with the old \`Table\` interface, expect that the return value is wrapped with a CompletableFuture usually. We do not have any buffer here so there is no close method for asynchronous table, you do not need to close it. And it is thread safe.

There are several differences for scan:

* There is still a \`getScanner\` method which returns a \`ResultScanner\`. You can use it in the old way and it works like the old \`ClientAsyncPrefetchScanner\`.
* There is a \`scanAll\` method which will return all the results at once. It aims to provide a simpler way for small scans which you want to get the whole results at once usually.
* The Observer Pattern. There is a scan method which accepts a \`ScanResultConsumer\` as a parameter. It will pass the results to the consumer.

Notice that \`AsyncTable\` interface is templatized. The template parameter specifies the type of \`ScanResultConsumerBase\` used by scans, which means the observer style scan APIs are different. The two types of scan consumers are - \`ScanResultConsumer\` and \`AdvancedScanResultConsumer\`.

\`ScanResultConsumer\` needs a separate thread pool which is used to execute the callbacks registered to the returned CompletableFuture. Because the use of separate thread pool frees up RPC threads, callbacks are free to do anything. Use this if the callbacks are not quick, or when in doubt.

\`AdvancedScanResultConsumer\` executes callbacks inside the framework thread. It is not allowed to do time consuming work in the callbacks else it will likely block the framework threads and cause very bad performance impact. As its name, it is designed for advanced users who want to write high performance code. See \`org.apache.hadoop.hbase.client.example.HttpProxyExample\` for how to write fully asynchronous code with it.

## Asynchronous Admin

You can obtain an \`AsyncConnection\` from \`ConnectionFactory\`, and then get a \`AsyncAdmin\` instance from it to access HBase. Notice that there are two \`getAdmin\` methods to get a \`AsyncAdmin\` instance. One method has one extra thread pool parameter which is used to execute callbacks. It is designed for normal users. Another method doesn't need a thread pool and all the callbacks are executed inside the framework thread so it is not allowed to do time consuming works in the callbacks. It is designed for advanced users.

The default \`getAdmin\` methods will return a \`AsyncAdmin\` instance which use default configs. If you want to customize some configs, you can use \`getAdminBuilder\` methods to get a \`AsyncAdminBuilder\` for creating \`AsyncAdmin\` instance. Users are free to only set the configs they care about to create a new \`AsyncAdmin\` instance.

For the \`AsyncAdmin\` interface, most methods have the same meaning with the old \`Admin\` interface, expect that the return value is wrapped with a CompletableFuture usually.

For most admin operations, when the returned CompletableFuture is done, it means the admin operation has also been done. But for compact operation, it only means the compact request was sent to HBase and may need some time to finish the compact operation. For \`rollWALWriter\` method, it only means the rollWALWriter request was sent to the region server and may need some time to finish the \`rollWALWriter\` operation.

For region name, we only accept \`byte[]\` as the parameter type and it may be a full region name or a encoded region name. For server name, we only accept \`ServerName\` as the parameter type. For table name, we only accept \`TableName\` as the parameter type. For \`list*\` operations, we only accept \`Pattern\` as the parameter type if you want to do regex matching.

## External Clients

Information on non-Java clients and custom protocols is covered in [Apache HBase External APIs](/docs/external-apis)

## Master Registry (new as of 2.3.0)

Starting from 2.5.0, MasterRegistry is deprecated. It's functionality is completely superseded by the RpcConnectionRegistry. Please see [Rpc Connection Registry (new as of 2.5.0)](/docs/architecture/client#rpc-connection-registry-new-as-of-250) for more details.

Client internally works with a *connection registry* to fetch the metadata needed by connections. This connection registry implementation is responsible for fetching the following metadata.

* Active master address
* Current meta region(s) locations
* Cluster ID (unique to this cluster)

This information is needed as a part of various client operations like connection set up, scans, gets, etc. Traditionally, the connection registry implementation has been based on ZooKeeper as the source of truth and clients fetched the metadata directly from the ZooKeeper quorum. HBase 2.3.0 introduces a new connection registry implementation based on direct communication with the Masters. With this implementation, clients now fetch required metadata via master RPC end points instead of maintaining connections to ZooKeeper. This change was done for the following reasons.

* Reduce load on ZooKeeper since that is critical for cluster operation.
* Holistic client timeout and retry configurations since the new registry brings all the client operations under HBase rpc framework.
* Remove the ZooKeeper client dependency on HBase client library.

This means:

* At least a single active or stand by master is needed for cluster connection setup. Refer to [Runtime Impact](/docs/architecture/master#runtime-impact) for more details.
* Master can be in a critical path of read/write operations, especially if the client metadata cache is empty or stale.
* There is higher connection load on the masters that before since the clients talk directly to HMasters instead of ZooKeeper ensemble\\\`

To reduce hot-spotting on a single master, all the masters (active & stand-by) expose the needed service to fetch the connection metadata. This lets the client connect to any master (not just active). Both ZooKeeper-based and Master-based connection registry implementations are available in 2.3+. For 2.x and earlier, the ZooKeeper-based implementation remains the default configuration. For 3.0.0, RpcConnectionRegistry becomes the default configuration, as the alternate to MasterRegistry.

Change the connection registry implementation by updating the value configured for \`hbase.client.registry.impl\`. To explicitly enable the ZooKeeper-based registry, use

\`\`\`xml
<property>
  <name>hbase.client.registry.impl</name>
  <value>org.apache.hadoop.hbase.client.ZKConnectionRegistry</value>
</property>
\`\`\`

To explicitly enable the Master-based registry, use

\`\`\`xml
<property>
  <name>hbase.client.registry.impl</name>
  <value>org.apache.hadoop.hbase.client.MasterRegistry</value>
</property>
\`\`\`

### MasterRegistry RPC hedging

MasterRegistry implements hedging of connection registry RPCs across active and stand-by masters. This lets the client make the same request to multiple servers and which ever responds first is returned back to the client immediately. This improves performance, especially when a subset of servers are under load. The hedging fan out size is configurable, meaning the number of requests that are hedged in a single attempt, using the configuration key *hbase.client.master\\_registry.hedged.fanout* in the client configuration. It defaults to 2. With this default, the RPCs are tried in batches of 2. The hedging policy is still primitive and does not adapt to any sort of live rpc performance metrics.

### Additional Notes

* Clients hedge the requests in a randomized order to avoid hot-spotting a single master.
* Cluster internal connections (masters ↔ regionservers) still use ZooKeeper based connection registry.
* Cluster internal state is still tracked in Zookeeper, hence ZK availability requirements are same as before.
* Inter cluster replication still uses ZooKeeper based connection registry to simplify configuration management.

For more implementation details, please refer to the [design doc](https://github.com/apache/hbase/tree/master/dev-support/design-docs) and [HBASE-18095](https://issues.apache.org/jira/browse/HBASE-18095).

## Rpc Connection Registry (new as of 2.5.0)

As said in the [Master Registry (new as of 2.3.0)](/docs/architecture/client#masterregistry-rpc-hedging) section, there are some disadvantages and limitations for MasterRegistry, especially that it puts master in the critical path of read/write operations. In order to address these problems, we introduced a more generic RpcConnectionRegistry.

It is also rpc based, like MasterRegistry, with several differences

1. Region server also implements the necessary rpc service, so you can config any nodes in the cluster as bootstrap nodes, not only masters
2. Support refreshing bootstrap nodes, for spreading loads across the nodes in the cluster, and also remove the dead nodes in bootstrap nodes.

To explicitly enable the rpc-based registry, use

\`\`\`xml
<property>
  <name>hbase.client.registry.impl</name>
  <value>org.apache.hadoop.hbase.client.RpcConnectionRegistry</value>
</property>
\`\`\`

To configure the bootstrap nodes, use

\`\`\`xml
<property>
  <name>hbase.client.bootstrap.servers</name>
  <value>server1:16020,server2:16020,server3:16020</value>
</property>
\`\`\`

If not configured, we will fallback to use master addresses as the bootstrap nodes.

RpcConnectionRegistry is available in 2.5+, and becomes the default client registry implementation in 3.0.0.

### RpcConnectionRegistry RPC hedging

Hedged read is still supported, the configuration key is now *hbase.client.bootstrap.hedged.fanout*, and its default value is still 2.

### RpcConnectionRegistry bootstrap nodes refreshing

There are basically two reasons for us to refresh the bootstrap nodes

* Periodically. This is for spreading loads across the nodes in the cluster. There are two configurations
  1. *hbase.client.bootstrap.refresh\\_interval\\_secs*: the refresh interval in seconds, default 300. A value less than or equal to zero means disable refreshing.
  2. *hbase.client.bootstrap.initial\\_refresh\\_delay\\_secs*: the initial refresh interval in seconds, the default value is 1/10 of *hbase.client.bootstrap.refresh\\_interval\\_secs*. The reason why we want to introduce a separated configuration for the delay for first refreshing is that, as end users could configure any nodes in a cluster as the initial bootstrap nodes, it is possible that different end users will configure the same machine which makes the machine over load. So we should have a shorter delay for the initial refresh, to let users quickly switch to the bootstrap nodes we want them to connect to.

* When there is a connection error while requesting the nodes, we will refresh immediately, to remove the dead nodes. To avoid putting too much pressure to the cluster, there is a configuration *hbase.client.bootstrap.min\\_secs\\_between\\_refreshes*, to control the minimum interval between two refreshings. The default value is 60, but notice that, if you change *hbase.client.bootstrap.refresh\\_interval\\_secs* to a small value, you need to make sure to also change *hbase.client.bootstrap.min\\_secs\\_between\\_refreshes* to a value smaller than *hbase.client.bootstrap.refresh\\_interval\\_secs*, otherwise an IllegalArgumentException will be thrown.

<Callout type="info">
  (Advanced) In case of any issues with the rpc/master based registry, use the following
  configuration to fallback to the ZooKeeper based connection registry implementation.
</Callout>

\`\`\`xml
<property>
  <name>hbase.client.registry.impl</name>
  <value>org.apache.hadoop.hbase.client.ZKConnectionRegistry</value>
</property>
\`\`\`

## Connection URI

Starting from 2.7.0, we add the support for specifying the connection information for a HBase cluster through an URI, which we call a "connection URI". And we've added several methods in *ConnectionFactory* to let you get a connection to the cluster specified by the URI. It looks like:

\`\`\`java
URI uri = new URI("hbase+rpc://server1:16020,server2:16020,server3:16020");
try (Connection conn = ConnectionFactory.createConnection(uri)) {
  ...
}
\`\`\`

### Supported Schemes

Currently there are two schemes supported, *hbase+rpc* for *RpcConnectionRegistry* and *hbase+zk* for *ZKConnectionRegistry*. *MasterRegistry* is deprecated so we do not expose it through connection URI.

For *hbase+rpc*, it looks like

\`\`\`shell
hbase+rpc://server1:16020,server2:16020,server3:16020
\`\`\`

The authority part *server1:16020,server2:16020,server3:16020* specifies the bootstrap nodes and their rpc ports, i.e, the configuration value for *hbase.client.bootstrap.servers* in the past.

For *hbase+zk*, it looks like

\`\`\`shell
hbase+zk://zk1:2181,zk2:2181,zk3:2181/hbase
\`\`\`

The authority part *zk1:2181,zk2:2181,zk3:2181* is the zk quorum, i.e, the configuration value for *hbase.zookeeper.quorum* in the past. The path part */hbase* is the znode parent, i.e, the configuration value for *zookeeper.znode.parent* in the past.

### Specify Configuration through URI Queries

To let users fully specify the connection information through a connection URI, we support specifying configuration values through URI Queries. It looks like:

\`\`\`shell
hbase+rpc://server1:16020?hbase.client.operation.timeout=10000
\`\`\`

In this way you can set the operation timeout to 10 seconds. Notice that, the configuration values specified in the connection URI will override the ones in the configuration file.

### Implement Your Own Connection Registry

We use *ServiceLoader* to load different connection registry implementations, the entry point is *org.apache.hadoop.hbase.client.ConnectionRegistryURIFactory*. So if you implement your own *ConnectionRegistryURIFactory* which has a different scheme, and register it in the services file, we can load it at runtime.

Connection URI is still a very new feature which has not been used extensively in production, so we do not want to expose the ability to customize *ConnectionRegistryURIFactory* yet as the API may be changed frequently in the beginning.

If you really want to implement your own connection registry, you can use the above way but take your own risk.
`,h={title:"Client",description:"HBase client architecture, connection management, metadata caching, and client-side configuration for optimal performance."},c=[{href:"/docs/architecture/catalog-tables#hbasemeta"},{href:"/docs/architecture/master#runtime-impact"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Admin.html"},{href:"/docs/configuration/default#client-configuration-and-dependencies-connecting-to-an-hbase-cluster"},{href:"https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/package-summary.html"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/HBaseConfiguration"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ConnectionFactory.html"},{href:"https://issues.apache.org/jira/browse/HBASE-6580"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Connection.html"},{href:"https://hbase.apache.org/1.4/devapidocs/org/apache/hadoop/hbase/client/HTable.html"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html"},{href:"https://hbase.apache.org/2.6/devapidocs/org/apache/hadoop/hbase/client/HTable.html"},{href:"https://issues.apache.org/jira/browse/HBASE-18500"},{href:"/acid-semantics"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html#batch(java.util.List,java.lang.Object%5B%5D)"},{href:"/docs/external-apis"},{href:"/docs/architecture/client#rpc-connection-registry-new-as-of-250"},{href:"/docs/architecture/master#runtime-impact"},{href:"https://github.com/apache/hbase/tree/master/dev-support/design-docs"},{href:"https://issues.apache.org/jira/browse/HBASE-18095"},{href:"/docs/architecture/client#masterregistry-rpc-hedging"}],l={contents:[{heading:void 0,content:"The HBase client finds the RegionServers that are serving the particular row range of interest. It does this by querying the hbase:meta table. See hbase:meta for details. After locating the required region(s), the client contacts the RegionServer serving that region, rather than going through the master, and issues the read or write request. This information is cached in the client so that subsequent requests need not go through the lookup process. Should a region be reassigned either by the master load balancer or because a RegionServer has died, the client will requery the catalog tables to determine the new location of the user region."},{heading:void 0,content:"See Runtime Impact for more information about the impact of the Master on HBase Client communication."},{heading:void 0,content:"Administrative functions are done via an instance of Admin"},{heading:"cluster-connections",content:"The API changed in HBase 1.0. For connection configuration information, see Client configuration and dependencies connecting to an HBase cluster."},{heading:"api-as-of-hbase-100",content:"It's been cleaned up and users are returned Interfaces to work against rather than particular types. In HBase 1.0, obtain a Connection object from ConnectionFactory and thereafter, get from it instances of Table, Admin, and RegionLocator on an as-need basis. When done, close the obtained instances. Finally, be sure to cleanup your Connection instance before exiting. Connections are heavyweight objects but thread-safe so you can create one for your application and keep the instance around. Table, Admin and RegionLocator instances are lightweight. Create as you go and then let go as soon as you are done by closing them. See the Client Package Javadoc Description for example usage of the new HBase 1.0 API."},{heading:"api-before-hbase-100",content:"Instances of HTable are the way to interact with an HBase cluster earlier than 1.0.0. Table instances are not thread-safe. Only one thread can use an instance of Table at any given time. When creating Table instances, it is advisable to use the same HBaseConfiguration instance. This will ensure sharing of ZooKeeper and socket instances to the RegionServers which is usually what you want. For example, this is preferred:"},{heading:"api-before-hbase-100",content:"as opposed to this:"},{heading:"api-before-hbase-100",content:"For more information about how connections are handled in the HBase client, see ConnectionFactory."},{heading:"connection-pooling",content:"For applications which require high-end multithreaded access (e.g., web-servers or application servers that may serve many application threads in a single JVM), you can pre-create a Connection, as shown in the following example:"},{heading:"connection-pooling",content:"Example 24. Pre-Creating a Connection"},{heading:"connection-pooling",content:"type: warn"},{heading:"connection-pooling",content:"title: HTablePool is Deprecated"},{heading:"connection-pooling",content:`Previous versions of this guide discussed HTablePool, which was deprecated in HBase 0.94, 0.95,
and 0.96, and removed in 0.98.1, by
HBASE-6580, or HConnection, which is
deprecated in HBase 1.0 by Connection. Please use
Connection
instead.`},{heading:"writebuffer-and-batch-methods",content:"In HBase 1.0 and later, HTable is deprecated in favor of Table. Table does not use autoflush. To do buffered writes, use the BufferedMutator class."},{heading:"writebuffer-and-batch-methods",content:"In HBase 2.0 and later, HTable does not use BufferedMutator to execute the Put operation. Refer to HBASE-18500 for more information."},{heading:"writebuffer-and-batch-methods",content:"For additional information on write durability, review the ACID semantics page."},{heading:"writebuffer-and-batch-methods",content:"For fine-grained control of batching of Puts or Deletes, see the batch methods on Table."},{heading:"asynchronous-client",content:"It is a new API introduced in HBase 2.0 which aims to provide the ability to access HBase asynchronously."},{heading:"asynchronous-client",content:"You can obtain an AsyncConnection from ConnectionFactory, and then get a asynchronous table instance from it to access HBase. When done, close the AsyncConnection instance(usually when your program exits)."},{heading:"asynchronous-client",content:"For the asynchronous table, most methods have the same meaning with the old Table interface, expect that the return value is wrapped with a CompletableFuture usually. We do not have any buffer here so there is no close method for asynchronous table, you do not need to close it. And it is thread safe."},{heading:"asynchronous-client",content:"There are several differences for scan:"},{heading:"asynchronous-client",content:"There is still a getScanner method which returns a ResultScanner. You can use it in the old way and it works like the old ClientAsyncPrefetchScanner."},{heading:"asynchronous-client",content:"There is a scanAll method which will return all the results at once. It aims to provide a simpler way for small scans which you want to get the whole results at once usually."},{heading:"asynchronous-client",content:"The Observer Pattern. There is a scan method which accepts a ScanResultConsumer as a parameter. It will pass the results to the consumer."},{heading:"asynchronous-client",content:"Notice that AsyncTable interface is templatized. The template parameter specifies the type of ScanResultConsumerBase used by scans, which means the observer style scan APIs are different. The two types of scan consumers are - ScanResultConsumer and AdvancedScanResultConsumer."},{heading:"asynchronous-client",content:"ScanResultConsumer needs a separate thread pool which is used to execute the callbacks registered to the returned CompletableFuture. Because the use of separate thread pool frees up RPC threads, callbacks are free to do anything. Use this if the callbacks are not quick, or when in doubt."},{heading:"asynchronous-client",content:"AdvancedScanResultConsumer executes callbacks inside the framework thread. It is not allowed to do time consuming work in the callbacks else it will likely block the framework threads and cause very bad performance impact. As its name, it is designed for advanced users who want to write high performance code. See org.apache.hadoop.hbase.client.example.HttpProxyExample for how to write fully asynchronous code with it."},{heading:"asynchronous-admin",content:"You can obtain an AsyncConnection from ConnectionFactory, and then get a AsyncAdmin instance from it to access HBase. Notice that there are two getAdmin methods to get a AsyncAdmin instance. One method has one extra thread pool parameter which is used to execute callbacks. It is designed for normal users. Another method doesn't need a thread pool and all the callbacks are executed inside the framework thread so it is not allowed to do time consuming works in the callbacks. It is designed for advanced users."},{heading:"asynchronous-admin",content:"The default getAdmin methods will return a AsyncAdmin instance which use default configs. If you want to customize some configs, you can use getAdminBuilder methods to get a AsyncAdminBuilder for creating AsyncAdmin instance. Users are free to only set the configs they care about to create a new AsyncAdmin instance."},{heading:"asynchronous-admin",content:"For the AsyncAdmin interface, most methods have the same meaning with the old Admin interface, expect that the return value is wrapped with a CompletableFuture usually."},{heading:"asynchronous-admin",content:"For most admin operations, when the returned CompletableFuture is done, it means the admin operation has also been done. But for compact operation, it only means the compact request was sent to HBase and may need some time to finish the compact operation. For rollWALWriter method, it only means the rollWALWriter request was sent to the region server and may need some time to finish the rollWALWriter operation."},{heading:"asynchronous-admin",content:"For region name, we only accept byte[] as the parameter type and it may be a full region name or a encoded region name. For server name, we only accept ServerName as the parameter type. For table name, we only accept TableName as the parameter type. For list* operations, we only accept Pattern as the parameter type if you want to do regex matching."},{heading:"external-clients",content:"Information on non-Java clients and custom protocols is covered in Apache HBase External APIs"},{heading:"master-registry-new-as-of-230",content:"Starting from 2.5.0, MasterRegistry is deprecated. It's functionality is completely superseded by the RpcConnectionRegistry. Please see Rpc Connection Registry (new as of 2.5.0) for more details."},{heading:"master-registry-new-as-of-230",content:"Client internally works with a connection registry to fetch the metadata needed by connections. This connection registry implementation is responsible for fetching the following metadata."},{heading:"master-registry-new-as-of-230",content:"Active master address"},{heading:"master-registry-new-as-of-230",content:"Current meta region(s) locations"},{heading:"master-registry-new-as-of-230",content:"Cluster ID (unique to this cluster)"},{heading:"master-registry-new-as-of-230",content:"This information is needed as a part of various client operations like connection set up, scans, gets, etc. Traditionally, the connection registry implementation has been based on ZooKeeper as the source of truth and clients fetched the metadata directly from the ZooKeeper quorum. HBase 2.3.0 introduces a new connection registry implementation based on direct communication with the Masters. With this implementation, clients now fetch required metadata via master RPC end points instead of maintaining connections to ZooKeeper. This change was done for the following reasons."},{heading:"master-registry-new-as-of-230",content:"Reduce load on ZooKeeper since that is critical for cluster operation."},{heading:"master-registry-new-as-of-230",content:"Holistic client timeout and retry configurations since the new registry brings all the client operations under HBase rpc framework."},{heading:"master-registry-new-as-of-230",content:"Remove the ZooKeeper client dependency on HBase client library."},{heading:"master-registry-new-as-of-230",content:"This means:"},{heading:"master-registry-new-as-of-230",content:"At least a single active or stand by master is needed for cluster connection setup. Refer to Runtime Impact for more details."},{heading:"master-registry-new-as-of-230",content:"Master can be in a critical path of read/write operations, especially if the client metadata cache is empty or stale."},{heading:"master-registry-new-as-of-230",content:"There is higher connection load on the masters that before since the clients talk directly to HMasters instead of ZooKeeper ensemble`"},{heading:"master-registry-new-as-of-230",content:"To reduce hot-spotting on a single master, all the masters (active & stand-by) expose the needed service to fetch the connection metadata. This lets the client connect to any master (not just active). Both ZooKeeper-based and Master-based connection registry implementations are available in 2.3+. For 2.x and earlier, the ZooKeeper-based implementation remains the default configuration. For 3.0.0, RpcConnectionRegistry becomes the default configuration, as the alternate to MasterRegistry."},{heading:"master-registry-new-as-of-230",content:"Change the connection registry implementation by updating the value configured for hbase.client.registry.impl. To explicitly enable the ZooKeeper-based registry, use"},{heading:"master-registry-new-as-of-230",content:"To explicitly enable the Master-based registry, use"},{heading:"masterregistry-rpc-hedging",content:"MasterRegistry implements hedging of connection registry RPCs across active and stand-by masters. This lets the client make the same request to multiple servers and which ever responds first is returned back to the client immediately. This improves performance, especially when a subset of servers are under load. The hedging fan out size is configurable, meaning the number of requests that are hedged in a single attempt, using the configuration key hbase.client.master_registry.hedged.fanout in the client configuration. It defaults to 2. With this default, the RPCs are tried in batches of 2. The hedging policy is still primitive and does not adapt to any sort of live rpc performance metrics."},{heading:"additional-notes",content:"Clients hedge the requests in a randomized order to avoid hot-spotting a single master."},{heading:"additional-notes",content:"Cluster internal connections (masters ↔ regionservers) still use ZooKeeper based connection registry."},{heading:"additional-notes",content:"Cluster internal state is still tracked in Zookeeper, hence ZK availability requirements are same as before."},{heading:"additional-notes",content:"Inter cluster replication still uses ZooKeeper based connection registry to simplify configuration management."},{heading:"additional-notes",content:"For more implementation details, please refer to the design doc and HBASE-18095."},{heading:"rpc-connection-registry-new-as-of-250",content:"As said in the Master Registry (new as of 2.3.0) section, there are some disadvantages and limitations for MasterRegistry, especially that it puts master in the critical path of read/write operations. In order to address these problems, we introduced a more generic RpcConnectionRegistry."},{heading:"rpc-connection-registry-new-as-of-250",content:"It is also rpc based, like MasterRegistry, with several differences"},{heading:"rpc-connection-registry-new-as-of-250",content:"Region server also implements the necessary rpc service, so you can config any nodes in the cluster as bootstrap nodes, not only masters"},{heading:"rpc-connection-registry-new-as-of-250",content:"Support refreshing bootstrap nodes, for spreading loads across the nodes in the cluster, and also remove the dead nodes in bootstrap nodes."},{heading:"rpc-connection-registry-new-as-of-250",content:"To explicitly enable the rpc-based registry, use"},{heading:"rpc-connection-registry-new-as-of-250",content:"To configure the bootstrap nodes, use"},{heading:"rpc-connection-registry-new-as-of-250",content:"If not configured, we will fallback to use master addresses as the bootstrap nodes."},{heading:"rpc-connection-registry-new-as-of-250",content:"RpcConnectionRegistry is available in 2.5+, and becomes the default client registry implementation in 3.0.0."},{heading:"rpcconnectionregistry-rpc-hedging",content:"Hedged read is still supported, the configuration key is now hbase.client.bootstrap.hedged.fanout, and its default value is still 2."},{heading:"rpcconnectionregistry-bootstrap-nodes-refreshing",content:"There are basically two reasons for us to refresh the bootstrap nodes"},{heading:"rpcconnectionregistry-bootstrap-nodes-refreshing",content:"Periodically. This is for spreading loads across the nodes in the cluster. There are two configurations"},{heading:"rpcconnectionregistry-bootstrap-nodes-refreshing",content:"hbase.client.bootstrap.refresh_interval_secs: the refresh interval in seconds, default 300. A value less than or equal to zero means disable refreshing."},{heading:"rpcconnectionregistry-bootstrap-nodes-refreshing",content:"hbase.client.bootstrap.initial_refresh_delay_secs: the initial refresh interval in seconds, the default value is 1/10 of hbase.client.bootstrap.refresh_interval_secs. The reason why we want to introduce a separated configuration for the delay for first refreshing is that, as end users could configure any nodes in a cluster as the initial bootstrap nodes, it is possible that different end users will configure the same machine which makes the machine over load. So we should have a shorter delay for the initial refresh, to let users quickly switch to the bootstrap nodes we want them to connect to."},{heading:"rpcconnectionregistry-bootstrap-nodes-refreshing",content:"When there is a connection error while requesting the nodes, we will refresh immediately, to remove the dead nodes. To avoid putting too much pressure to the cluster, there is a configuration hbase.client.bootstrap.min_secs_between_refreshes, to control the minimum interval between two refreshings. The default value is 60, but notice that, if you change hbase.client.bootstrap.refresh_interval_secs to a small value, you need to make sure to also change hbase.client.bootstrap.min_secs_between_refreshes to a value smaller than hbase.client.bootstrap.refresh_interval_secs, otherwise an IllegalArgumentException will be thrown."},{heading:"rpcconnectionregistry-bootstrap-nodes-refreshing",content:"type: info"},{heading:"rpcconnectionregistry-bootstrap-nodes-refreshing",content:`(Advanced) In case of any issues with the rpc/master based registry, use the following
configuration to fallback to the ZooKeeper based connection registry implementation.`},{heading:"connection-uri",content:`Starting from 2.7.0, we add the support for specifying the connection information for a HBase cluster through an URI, which we call a "connection URI". And we've added several methods in ConnectionFactory to let you get a connection to the cluster specified by the URI. It looks like:`},{heading:"supported-schemes",content:"Currently there are two schemes supported, hbase+rpc for RpcConnectionRegistry and hbase+zk for ZKConnectionRegistry. MasterRegistry is deprecated so we do not expose it through connection URI."},{heading:"supported-schemes",content:"For hbase+rpc, it looks like"},{heading:"supported-schemes",content:"The authority part server1:16020,server2:16020,server3:16020 specifies the bootstrap nodes and their rpc ports, i.e, the configuration value for hbase.client.bootstrap.servers in the past."},{heading:"supported-schemes",content:"For hbase+zk, it looks like"},{heading:"supported-schemes",content:"The authority part zk1:2181,zk2:2181,zk3:2181 is the zk quorum, i.e, the configuration value for hbase.zookeeper.quorum in the past. The path part /hbase is the znode parent, i.e, the configuration value for zookeeper.znode.parent in the past."},{heading:"specify-configuration-through-uri-queries",content:"To let users fully specify the connection information through a connection URI, we support specifying configuration values through URI Queries. It looks like:"},{heading:"specify-configuration-through-uri-queries",content:"In this way you can set the operation timeout to 10 seconds. Notice that, the configuration values specified in the connection URI will override the ones in the configuration file."},{heading:"implement-your-own-connection-registry",content:"We use ServiceLoader to load different connection registry implementations, the entry point is org.apache.hadoop.hbase.client.ConnectionRegistryURIFactory. So if you implement your own ConnectionRegistryURIFactory which has a different scheme, and register it in the services file, we can load it at runtime."},{heading:"implement-your-own-connection-registry",content:"Connection URI is still a very new feature which has not been used extensively in production, so we do not want to expose the ability to customize ConnectionRegistryURIFactory yet as the API may be changed frequently in the beginning."},{heading:"implement-your-own-connection-registry",content:"If you really want to implement your own connection registry, you can use the above way but take your own risk."}],headings:[{id:"cluster-connections",content:"Cluster Connections"},{id:"api-as-of-hbase-100",content:"API as of HBase 1.0.0"},{id:"api-before-hbase-100",content:"API before HBase 1.0.0"},{id:"connection-pooling",content:"Connection Pooling"},{id:"writebuffer-and-batch-methods",content:"WriteBuffer and Batch Methods"},{id:"asynchronous-client",content:"Asynchronous Client"},{id:"asynchronous-admin",content:"Asynchronous Admin"},{id:"external-clients",content:"External Clients"},{id:"master-registry-new-as-of-230",content:"Master Registry (new as of 2.3.0)"},{id:"masterregistry-rpc-hedging",content:"MasterRegistry RPC hedging"},{id:"additional-notes",content:"Additional Notes"},{id:"rpc-connection-registry-new-as-of-250",content:"Rpc Connection Registry (new as of 2.5.0)"},{id:"rpcconnectionregistry-rpc-hedging",content:"RpcConnectionRegistry RPC hedging"},{id:"rpcconnectionregistry-bootstrap-nodes-refreshing",content:"RpcConnectionRegistry bootstrap nodes refreshing"},{id:"connection-uri",content:"Connection URI"},{id:"supported-schemes",content:"Supported Schemes"},{id:"specify-configuration-through-uri-queries",content:"Specify Configuration through URI Queries"},{id:"implement-your-own-connection-registry",content:"Implement Your Own Connection Registry"}]};const d=[{depth:2,url:"#cluster-connections",title:e.jsx(e.Fragment,{children:"Cluster Connections"})},{depth:3,url:"#api-as-of-hbase-100",title:e.jsx(e.Fragment,{children:"API as of HBase 1.0.0"})},{depth:3,url:"#api-before-hbase-100",title:e.jsx(e.Fragment,{children:"API before HBase 1.0.0"})},{depth:4,url:"#connection-pooling",title:e.jsx(e.Fragment,{children:"Connection Pooling"})},{depth:2,url:"#writebuffer-and-batch-methods",title:e.jsx(e.Fragment,{children:"WriteBuffer and Batch Methods"})},{depth:2,url:"#asynchronous-client",title:e.jsx(e.Fragment,{children:"Asynchronous Client"})},{depth:2,url:"#asynchronous-admin",title:e.jsx(e.Fragment,{children:"Asynchronous Admin"})},{depth:2,url:"#external-clients",title:e.jsx(e.Fragment,{children:"External Clients"})},{depth:2,url:"#master-registry-new-as-of-230",title:e.jsx(e.Fragment,{children:"Master Registry (new as of 2.3.0)"})},{depth:3,url:"#masterregistry-rpc-hedging",title:e.jsx(e.Fragment,{children:"MasterRegistry RPC hedging"})},{depth:3,url:"#additional-notes",title:e.jsx(e.Fragment,{children:"Additional Notes"})},{depth:2,url:"#rpc-connection-registry-new-as-of-250",title:e.jsx(e.Fragment,{children:"Rpc Connection Registry (new as of 2.5.0)"})},{depth:3,url:"#rpcconnectionregistry-rpc-hedging",title:e.jsx(e.Fragment,{children:"RpcConnectionRegistry RPC hedging"})},{depth:3,url:"#rpcconnectionregistry-bootstrap-nodes-refreshing",title:e.jsx(e.Fragment,{children:"RpcConnectionRegistry bootstrap nodes refreshing"})},{depth:2,url:"#connection-uri",title:e.jsx(e.Fragment,{children:"Connection URI"})},{depth:3,url:"#supported-schemes",title:e.jsx(e.Fragment,{children:"Supported Schemes"})},{depth:3,url:"#specify-configuration-through-uri-queries",title:e.jsx(e.Fragment,{children:"Specify Configuration through URI Queries"})},{depth:3,url:"#implement-your-own-connection-registry",title:e.jsx(e.Fragment,{children:"Implement Your Own Connection Registry"})}];function s(t){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",ul:"ul",...t.components},{Callout:i}=n;return i||a("Callout"),e.jsxs(e.Fragment,{children:[e.jsxs(n.p,{children:["The HBase client finds the RegionServers that are serving the particular row range of interest. It does this by querying the ",e.jsx(n.code,{children:"hbase:meta"})," table. See ",e.jsx(n.a,{href:"/docs/architecture/catalog-tables#hbasemeta",children:"hbase:meta"})," for details. After locating the required region(s), the client contacts the RegionServer serving that region, rather than going through the master, and issues the read or write request. This information is cached in the client so that subsequent requests need not go through the lookup process. Should a region be reassigned either by the master load balancer or because a RegionServer has died, the client will requery the catalog tables to determine the new location of the user region."]}),`
`,e.jsxs(n.p,{children:["See ",e.jsx(n.a,{href:"/docs/architecture/master#runtime-impact",children:"Runtime Impact"})," for more information about the impact of the Master on HBase Client communication."]}),`
`,e.jsxs(n.p,{children:["Administrative functions are done via an instance of ",e.jsx(n.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Admin.html",children:"Admin"})]}),`
`,e.jsx(n.h2,{id:"cluster-connections",children:"Cluster Connections"}),`
`,e.jsxs(n.p,{children:["The API changed in HBase 1.0. For connection configuration information, see ",e.jsx(n.a,{href:"/docs/configuration/default#client-configuration-and-dependencies-connecting-to-an-hbase-cluster",children:"Client configuration and dependencies connecting to an HBase cluster"}),"."]}),`
`,e.jsx(n.h3,{id:"api-as-of-hbase-100",children:"API as of HBase 1.0.0"}),`
`,e.jsxs(n.p,{children:["It's been cleaned up and users are returned Interfaces to work against rather than particular types. In HBase 1.0, obtain a ",e.jsx(n.code,{children:"Connection"})," object from ",e.jsx(n.code,{children:"ConnectionFactory"})," and thereafter, get from it instances of ",e.jsx(n.code,{children:"Table"}),", ",e.jsx(n.code,{children:"Admin"}),", and ",e.jsx(n.code,{children:"RegionLocator"})," on an as-need basis. When done, close the obtained instances. Finally, be sure to cleanup your ",e.jsx(n.code,{children:"Connection"})," instance before exiting. ",e.jsx(n.code,{children:"Connections"})," are heavyweight objects but thread-safe so you can create one for your application and keep the instance around. ",e.jsx(n.code,{children:"Table"}),", ",e.jsx(n.code,{children:"Admin"})," and ",e.jsx(n.code,{children:"RegionLocator"})," instances are lightweight. Create as you go and then let go as soon as you are done by closing them. See the ",e.jsx(n.a,{href:"https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/package-summary.html",children:"Client Package Javadoc Description"})," for example usage of the new HBase 1.0 API."]}),`
`,e.jsx(n.h3,{id:"api-before-hbase-100",children:"API before HBase 1.0.0"}),`
`,e.jsxs(n.p,{children:["Instances of ",e.jsx(n.code,{children:"HTable"})," are the way to interact with an HBase cluster earlier than 1.0.0. ",e.jsxs(n.em,{children:[e.jsx(n.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html",children:"Table"})," instances are not thread-safe"]}),". Only one thread can use an instance of Table at any given time. When creating Table instances, it is advisable to use the same ",e.jsx(n.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/HBaseConfiguration",children:"HBaseConfiguration"})," instance. This will ensure sharing of ZooKeeper and socket instances to the RegionServers which is usually what you want. For example, this is preferred:"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(n.code,{children:[e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"HBaseConfiguration conf "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" HBaseConfiguration."}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"create"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"HTable table1 "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" HTable"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(conf, "}),e.jsx(n.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:'"myTable"'}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:");"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"HTable table2 "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" HTable"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(conf, "}),e.jsx(n.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:'"myTable"'}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:");"})]})]})})}),`
`,e.jsx(n.p,{children:"as opposed to this:"}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(n.code,{children:[e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"HBaseConfiguration conf1 "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" HBaseConfiguration."}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"create"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"HTable table1 "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" HTable"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(conf1, "}),e.jsx(n.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:'"myTable"'}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:");"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"HBaseConfiguration conf2 "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" HBaseConfiguration."}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"create"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"HTable table2 "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" HTable"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(conf2, "}),e.jsx(n.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:'"myTable"'}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:");"})]})]})})}),`
`,e.jsxs(n.p,{children:["For more information about how connections are handled in the HBase client, see ",e.jsx(n.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ConnectionFactory.html",children:"ConnectionFactory"}),"."]}),`
`,e.jsx(n.h4,{id:"connection-pooling",children:"Connection Pooling"}),`
`,e.jsxs(n.p,{children:["For applications which require high-end multithreaded access (e.g., web-servers or application servers that may serve many application threads in a single JVM), you can pre-create a ",e.jsx(n.code,{children:"Connection"}),", as shown in the following example:"]}),`
`,e.jsx(n.p,{children:e.jsxs(n.strong,{children:["Example 24. Pre-Creating a ",e.jsx(n.code,{children:"Connection"})]})}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(n.code,{children:[e.jsx(n.span,{className:"line",children:e.jsx(n.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"// Create a connection to the cluster."})}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"Configuration conf "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" HBaseConfiguration."}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"create"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"try"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" (Connection connection "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" ConnectionFactory."}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"createConnection"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(conf);"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"     Table table "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" connection."}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getTable"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(TableName."}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"valueOf"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(tablename))) {"})]}),`
`,e.jsx(n.span,{className:"line",children:e.jsx(n.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"  // use table as needed, the table returned is lightweight"})}),`
`,e.jsx(n.span,{className:"line",children:e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"}"})})]})})}),`
`,e.jsx(i,{type:"warn",title:"HTablePool is Deprecated",children:e.jsxs(n.p,{children:["Previous versions of this guide discussed ",e.jsx(n.code,{children:"HTablePool"}),`, which was deprecated in HBase 0.94, 0.95,
and 0.96, and removed in 0.98.1, by
`,e.jsx(n.a,{href:"https://issues.apache.org/jira/browse/HBASE-6580",children:"HBASE-6580"}),", or ",e.jsx(n.code,{children:"HConnection"}),`, which is
deprecated in HBase 1.0 by `,e.jsx(n.code,{children:"Connection"}),`. Please use
`,e.jsx(n.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Connection.html",children:"Connection"}),`
instead.`]})}),`
`,e.jsx(n.h2,{id:"writebuffer-and-batch-methods",children:"WriteBuffer and Batch Methods"}),`
`,e.jsxs(n.p,{children:["In HBase 1.0 and later, ",e.jsx(n.a,{href:"https://hbase.apache.org/1.4/devapidocs/org/apache/hadoop/hbase/client/HTable.html",children:"HTable"})," is deprecated in favor of ",e.jsx(n.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html",children:"Table"}),". ",e.jsx(n.code,{children:"Table"})," does not use autoflush. To do buffered writes, use the BufferedMutator class."]}),`
`,e.jsxs(n.p,{children:["In HBase 2.0 and later, ",e.jsx(n.a,{href:"https://hbase.apache.org/2.6/devapidocs/org/apache/hadoop/hbase/client/HTable.html",children:"HTable"})," does not use BufferedMutator to execute the ",e.jsx(n.code,{children:"Put"})," operation. Refer to ",e.jsx(n.a,{href:"https://issues.apache.org/jira/browse/HBASE-18500",children:"HBASE-18500"})," for more information."]}),`
`,e.jsxs(n.p,{children:["For additional information on write durability, review the ",e.jsx(n.a,{href:"/acid-semantics",children:"ACID semantics"})," page."]}),`
`,e.jsxs(n.p,{children:["For fine-grained control of batching of ",e.jsx(n.code,{children:"Put"}),"s or ",e.jsx(n.code,{children:"Delete"}),"s, see the ",e.jsx(n.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html#batch(java.util.List,java.lang.Object%5B%5D)",children:"batch"})," methods on Table."]}),`
`,e.jsx(n.h2,{id:"asynchronous-client",children:"Asynchronous Client"}),`
`,e.jsx(n.p,{children:"It is a new API introduced in HBase 2.0 which aims to provide the ability to access HBase asynchronously."}),`
`,e.jsxs(n.p,{children:["You can obtain an ",e.jsx(n.code,{children:"AsyncConnection"})," from ",e.jsx(n.code,{children:"ConnectionFactory"}),", and then get a asynchronous table instance from it to access HBase. When done, close the ",e.jsx(n.code,{children:"AsyncConnection"})," instance(usually when your program exits)."]}),`
`,e.jsxs(n.p,{children:["For the asynchronous table, most methods have the same meaning with the old ",e.jsx(n.code,{children:"Table"})," interface, expect that the return value is wrapped with a CompletableFuture usually. We do not have any buffer here so there is no close method for asynchronous table, you do not need to close it. And it is thread safe."]}),`
`,e.jsx(n.p,{children:"There are several differences for scan:"}),`
`,e.jsxs(n.ul,{children:[`
`,e.jsxs(n.li,{children:["There is still a ",e.jsx(n.code,{children:"getScanner"})," method which returns a ",e.jsx(n.code,{children:"ResultScanner"}),". You can use it in the old way and it works like the old ",e.jsx(n.code,{children:"ClientAsyncPrefetchScanner"}),"."]}),`
`,e.jsxs(n.li,{children:["There is a ",e.jsx(n.code,{children:"scanAll"})," method which will return all the results at once. It aims to provide a simpler way for small scans which you want to get the whole results at once usually."]}),`
`,e.jsxs(n.li,{children:["The Observer Pattern. There is a scan method which accepts a ",e.jsx(n.code,{children:"ScanResultConsumer"})," as a parameter. It will pass the results to the consumer."]}),`
`]}),`
`,e.jsxs(n.p,{children:["Notice that ",e.jsx(n.code,{children:"AsyncTable"})," interface is templatized. The template parameter specifies the type of ",e.jsx(n.code,{children:"ScanResultConsumerBase"})," used by scans, which means the observer style scan APIs are different. The two types of scan consumers are - ",e.jsx(n.code,{children:"ScanResultConsumer"})," and ",e.jsx(n.code,{children:"AdvancedScanResultConsumer"}),"."]}),`
`,e.jsxs(n.p,{children:[e.jsx(n.code,{children:"ScanResultConsumer"})," needs a separate thread pool which is used to execute the callbacks registered to the returned CompletableFuture. Because the use of separate thread pool frees up RPC threads, callbacks are free to do anything. Use this if the callbacks are not quick, or when in doubt."]}),`
`,e.jsxs(n.p,{children:[e.jsx(n.code,{children:"AdvancedScanResultConsumer"})," executes callbacks inside the framework thread. It is not allowed to do time consuming work in the callbacks else it will likely block the framework threads and cause very bad performance impact. As its name, it is designed for advanced users who want to write high performance code. See ",e.jsx(n.code,{children:"org.apache.hadoop.hbase.client.example.HttpProxyExample"})," for how to write fully asynchronous code with it."]}),`
`,e.jsx(n.h2,{id:"asynchronous-admin",children:"Asynchronous Admin"}),`
`,e.jsxs(n.p,{children:["You can obtain an ",e.jsx(n.code,{children:"AsyncConnection"})," from ",e.jsx(n.code,{children:"ConnectionFactory"}),", and then get a ",e.jsx(n.code,{children:"AsyncAdmin"})," instance from it to access HBase. Notice that there are two ",e.jsx(n.code,{children:"getAdmin"})," methods to get a ",e.jsx(n.code,{children:"AsyncAdmin"})," instance. One method has one extra thread pool parameter which is used to execute callbacks. It is designed for normal users. Another method doesn't need a thread pool and all the callbacks are executed inside the framework thread so it is not allowed to do time consuming works in the callbacks. It is designed for advanced users."]}),`
`,e.jsxs(n.p,{children:["The default ",e.jsx(n.code,{children:"getAdmin"})," methods will return a ",e.jsx(n.code,{children:"AsyncAdmin"})," instance which use default configs. If you want to customize some configs, you can use ",e.jsx(n.code,{children:"getAdminBuilder"})," methods to get a ",e.jsx(n.code,{children:"AsyncAdminBuilder"})," for creating ",e.jsx(n.code,{children:"AsyncAdmin"})," instance. Users are free to only set the configs they care about to create a new ",e.jsx(n.code,{children:"AsyncAdmin"})," instance."]}),`
`,e.jsxs(n.p,{children:["For the ",e.jsx(n.code,{children:"AsyncAdmin"})," interface, most methods have the same meaning with the old ",e.jsx(n.code,{children:"Admin"})," interface, expect that the return value is wrapped with a CompletableFuture usually."]}),`
`,e.jsxs(n.p,{children:["For most admin operations, when the returned CompletableFuture is done, it means the admin operation has also been done. But for compact operation, it only means the compact request was sent to HBase and may need some time to finish the compact operation. For ",e.jsx(n.code,{children:"rollWALWriter"})," method, it only means the rollWALWriter request was sent to the region server and may need some time to finish the ",e.jsx(n.code,{children:"rollWALWriter"})," operation."]}),`
`,e.jsxs(n.p,{children:["For region name, we only accept ",e.jsx(n.code,{children:"byte[]"})," as the parameter type and it may be a full region name or a encoded region name. For server name, we only accept ",e.jsx(n.code,{children:"ServerName"})," as the parameter type. For table name, we only accept ",e.jsx(n.code,{children:"TableName"})," as the parameter type. For ",e.jsx(n.code,{children:"list*"})," operations, we only accept ",e.jsx(n.code,{children:"Pattern"})," as the parameter type if you want to do regex matching."]}),`
`,e.jsx(n.h2,{id:"external-clients",children:"External Clients"}),`
`,e.jsxs(n.p,{children:["Information on non-Java clients and custom protocols is covered in ",e.jsx(n.a,{href:"/docs/external-apis",children:"Apache HBase External APIs"})]}),`
`,e.jsx(n.h2,{id:"master-registry-new-as-of-230",children:"Master Registry (new as of 2.3.0)"}),`
`,e.jsxs(n.p,{children:["Starting from 2.5.0, MasterRegistry is deprecated. It's functionality is completely superseded by the RpcConnectionRegistry. Please see ",e.jsx(n.a,{href:"/docs/architecture/client#rpc-connection-registry-new-as-of-250",children:"Rpc Connection Registry (new as of 2.5.0)"})," for more details."]}),`
`,e.jsxs(n.p,{children:["Client internally works with a ",e.jsx(n.em,{children:"connection registry"})," to fetch the metadata needed by connections. This connection registry implementation is responsible for fetching the following metadata."]}),`
`,e.jsxs(n.ul,{children:[`
`,e.jsx(n.li,{children:"Active master address"}),`
`,e.jsx(n.li,{children:"Current meta region(s) locations"}),`
`,e.jsx(n.li,{children:"Cluster ID (unique to this cluster)"}),`
`]}),`
`,e.jsx(n.p,{children:"This information is needed as a part of various client operations like connection set up, scans, gets, etc. Traditionally, the connection registry implementation has been based on ZooKeeper as the source of truth and clients fetched the metadata directly from the ZooKeeper quorum. HBase 2.3.0 introduces a new connection registry implementation based on direct communication with the Masters. With this implementation, clients now fetch required metadata via master RPC end points instead of maintaining connections to ZooKeeper. This change was done for the following reasons."}),`
`,e.jsxs(n.ul,{children:[`
`,e.jsx(n.li,{children:"Reduce load on ZooKeeper since that is critical for cluster operation."}),`
`,e.jsx(n.li,{children:"Holistic client timeout and retry configurations since the new registry brings all the client operations under HBase rpc framework."}),`
`,e.jsx(n.li,{children:"Remove the ZooKeeper client dependency on HBase client library."}),`
`]}),`
`,e.jsx(n.p,{children:"This means:"}),`
`,e.jsxs(n.ul,{children:[`
`,e.jsxs(n.li,{children:["At least a single active or stand by master is needed for cluster connection setup. Refer to ",e.jsx(n.a,{href:"/docs/architecture/master#runtime-impact",children:"Runtime Impact"})," for more details."]}),`
`,e.jsx(n.li,{children:"Master can be in a critical path of read/write operations, especially if the client metadata cache is empty or stale."}),`
`,e.jsx(n.li,{children:"There is higher connection load on the masters that before since the clients talk directly to HMasters instead of ZooKeeper ensemble`"}),`
`]}),`
`,e.jsx(n.p,{children:"To reduce hot-spotting on a single master, all the masters (active & stand-by) expose the needed service to fetch the connection metadata. This lets the client connect to any master (not just active). Both ZooKeeper-based and Master-based connection registry implementations are available in 2.3+. For 2.x and earlier, the ZooKeeper-based implementation remains the default configuration. For 3.0.0, RpcConnectionRegistry becomes the default configuration, as the alternate to MasterRegistry."}),`
`,e.jsxs(n.p,{children:["Change the connection registry implementation by updating the value configured for ",e.jsx(n.code,{children:"hbase.client.registry.impl"}),". To explicitly enable the ZooKeeper-based registry, use"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(n.code,{children:[e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.client.registry.impl</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">org.apache.hadoop.hbase.client.ZKConnectionRegistry</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsx(n.p,{children:"To explicitly enable the Master-based registry, use"}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(n.code,{children:[e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.client.registry.impl</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">org.apache.hadoop.hbase.client.MasterRegistry</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsx(n.h3,{id:"masterregistry-rpc-hedging",children:"MasterRegistry RPC hedging"}),`
`,e.jsxs(n.p,{children:["MasterRegistry implements hedging of connection registry RPCs across active and stand-by masters. This lets the client make the same request to multiple servers and which ever responds first is returned back to the client immediately. This improves performance, especially when a subset of servers are under load. The hedging fan out size is configurable, meaning the number of requests that are hedged in a single attempt, using the configuration key ",e.jsx(n.em,{children:"hbase.client.master_registry.hedged.fanout"})," in the client configuration. It defaults to 2. With this default, the RPCs are tried in batches of 2. The hedging policy is still primitive and does not adapt to any sort of live rpc performance metrics."]}),`
`,e.jsx(n.h3,{id:"additional-notes",children:"Additional Notes"}),`
`,e.jsxs(n.ul,{children:[`
`,e.jsx(n.li,{children:"Clients hedge the requests in a randomized order to avoid hot-spotting a single master."}),`
`,e.jsx(n.li,{children:"Cluster internal connections (masters ↔ regionservers) still use ZooKeeper based connection registry."}),`
`,e.jsx(n.li,{children:"Cluster internal state is still tracked in Zookeeper, hence ZK availability requirements are same as before."}),`
`,e.jsx(n.li,{children:"Inter cluster replication still uses ZooKeeper based connection registry to simplify configuration management."}),`
`]}),`
`,e.jsxs(n.p,{children:["For more implementation details, please refer to the ",e.jsx(n.a,{href:"https://github.com/apache/hbase/tree/master/dev-support/design-docs",children:"design doc"})," and ",e.jsx(n.a,{href:"https://issues.apache.org/jira/browse/HBASE-18095",children:"HBASE-18095"}),"."]}),`
`,e.jsx(n.h2,{id:"rpc-connection-registry-new-as-of-250",children:"Rpc Connection Registry (new as of 2.5.0)"}),`
`,e.jsxs(n.p,{children:["As said in the ",e.jsx(n.a,{href:"/docs/architecture/client#masterregistry-rpc-hedging",children:"Master Registry (new as of 2.3.0)"})," section, there are some disadvantages and limitations for MasterRegistry, especially that it puts master in the critical path of read/write operations. In order to address these problems, we introduced a more generic RpcConnectionRegistry."]}),`
`,e.jsx(n.p,{children:"It is also rpc based, like MasterRegistry, with several differences"}),`
`,e.jsxs(n.ol,{children:[`
`,e.jsx(n.li,{children:"Region server also implements the necessary rpc service, so you can config any nodes in the cluster as bootstrap nodes, not only masters"}),`
`,e.jsx(n.li,{children:"Support refreshing bootstrap nodes, for spreading loads across the nodes in the cluster, and also remove the dead nodes in bootstrap nodes."}),`
`]}),`
`,e.jsx(n.p,{children:"To explicitly enable the rpc-based registry, use"}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(n.code,{children:[e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.client.registry.impl</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">org.apache.hadoop.hbase.client.RpcConnectionRegistry</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsx(n.p,{children:"To configure the bootstrap nodes, use"}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(n.code,{children:[e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.client.bootstrap.servers</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">server1:16020,server2:16020,server3:16020</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsx(n.p,{children:"If not configured, we will fallback to use master addresses as the bootstrap nodes."}),`
`,e.jsx(n.p,{children:"RpcConnectionRegistry is available in 2.5+, and becomes the default client registry implementation in 3.0.0."}),`
`,e.jsx(n.h3,{id:"rpcconnectionregistry-rpc-hedging",children:"RpcConnectionRegistry RPC hedging"}),`
`,e.jsxs(n.p,{children:["Hedged read is still supported, the configuration key is now ",e.jsx(n.em,{children:"hbase.client.bootstrap.hedged.fanout"}),", and its default value is still 2."]}),`
`,e.jsx(n.h3,{id:"rpcconnectionregistry-bootstrap-nodes-refreshing",children:"RpcConnectionRegistry bootstrap nodes refreshing"}),`
`,e.jsx(n.p,{children:"There are basically two reasons for us to refresh the bootstrap nodes"}),`
`,e.jsxs(n.ul,{children:[`
`,e.jsxs(n.li,{children:[`
`,e.jsx(n.p,{children:"Periodically. This is for spreading loads across the nodes in the cluster. There are two configurations"}),`
`,e.jsxs(n.ol,{children:[`
`,e.jsxs(n.li,{children:[e.jsx(n.em,{children:"hbase.client.bootstrap.refresh_interval_secs"}),": the refresh interval in seconds, default 300. A value less than or equal to zero means disable refreshing."]}),`
`,e.jsxs(n.li,{children:[e.jsx(n.em,{children:"hbase.client.bootstrap.initial_refresh_delay_secs"}),": the initial refresh interval in seconds, the default value is 1/10 of ",e.jsx(n.em,{children:"hbase.client.bootstrap.refresh_interval_secs"}),". The reason why we want to introduce a separated configuration for the delay for first refreshing is that, as end users could configure any nodes in a cluster as the initial bootstrap nodes, it is possible that different end users will configure the same machine which makes the machine over load. So we should have a shorter delay for the initial refresh, to let users quickly switch to the bootstrap nodes we want them to connect to."]}),`
`]}),`
`]}),`
`,e.jsxs(n.li,{children:[`
`,e.jsxs(n.p,{children:["When there is a connection error while requesting the nodes, we will refresh immediately, to remove the dead nodes. To avoid putting too much pressure to the cluster, there is a configuration ",e.jsx(n.em,{children:"hbase.client.bootstrap.min_secs_between_refreshes"}),", to control the minimum interval between two refreshings. The default value is 60, but notice that, if you change ",e.jsx(n.em,{children:"hbase.client.bootstrap.refresh_interval_secs"})," to a small value, you need to make sure to also change ",e.jsx(n.em,{children:"hbase.client.bootstrap.min_secs_between_refreshes"})," to a value smaller than ",e.jsx(n.em,{children:"hbase.client.bootstrap.refresh_interval_secs"}),", otherwise an IllegalArgumentException will be thrown."]}),`
`]}),`
`]}),`
`,e.jsx(i,{type:"info",children:e.jsx(n.p,{children:`(Advanced) In case of any issues with the rpc/master based registry, use the following
configuration to fallback to the ZooKeeper based connection registry implementation.`})}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(n.code,{children:[e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">hbase.client.registry.impl</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">org.apache.hadoop.hbase.client.ZKConnectionRegistry</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(n.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsx(n.h2,{id:"connection-uri",children:"Connection URI"}),`
`,e.jsxs(n.p,{children:[`Starting from 2.7.0, we add the support for specifying the connection information for a HBase cluster through an URI, which we call a "connection URI". And we've added several methods in `,e.jsx(n.em,{children:"ConnectionFactory"})," to let you get a connection to the cluster specified by the URI. It looks like:"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(n.code,{children:[e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"URI uri "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" URI"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"("}),e.jsx(n.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:'"hbase+rpc://server1:16020,server2:16020,server3:16020"'}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:");"})]}),`
`,e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"try"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" (Connection conn "}),e.jsx(n.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" ConnectionFactory."}),e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"createConnection"}),e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(uri)) {"})]}),`
`,e.jsx(n.span,{className:"line",children:e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  ..."})}),`
`,e.jsx(n.span,{className:"line",children:e.jsx(n.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"}"})})]})})}),`
`,e.jsx(n.h3,{id:"supported-schemes",children:"Supported Schemes"}),`
`,e.jsxs(n.p,{children:["Currently there are two schemes supported, ",e.jsx(n.em,{children:"hbase+rpc"})," for ",e.jsx(n.em,{children:"RpcConnectionRegistry"})," and ",e.jsx(n.em,{children:"hbase+zk"})," for ",e.jsx(n.em,{children:"ZKConnectionRegistry"}),". ",e.jsx(n.em,{children:"MasterRegistry"})," is deprecated so we do not expose it through connection URI."]}),`
`,e.jsxs(n.p,{children:["For ",e.jsx(n.em,{children:"hbase+rpc"}),", it looks like"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z" fill="currentColor" /></svg>',children:e.jsx(n.code,{children:e.jsx(n.span,{className:"line",children:e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"hbase+rpc://server1:16020,server2:16020,server3:16020"})})})})}),`
`,e.jsxs(n.p,{children:["The authority part ",e.jsx(n.em,{children:"server1:16020,server2:16020,server3:16020"})," specifies the bootstrap nodes and their rpc ports, i.e, the configuration value for ",e.jsx(n.em,{children:"hbase.client.bootstrap.servers"})," in the past."]}),`
`,e.jsxs(n.p,{children:["For ",e.jsx(n.em,{children:"hbase+zk"}),", it looks like"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z" fill="currentColor" /></svg>',children:e.jsx(n.code,{children:e.jsx(n.span,{className:"line",children:e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"hbase+zk://zk1:2181,zk2:2181,zk3:2181/hbase"})})})})}),`
`,e.jsxs(n.p,{children:["The authority part ",e.jsx(n.em,{children:"zk1:2181,zk2:2181,zk3:2181"})," is the zk quorum, i.e, the configuration value for ",e.jsx(n.em,{children:"hbase.zookeeper.quorum"})," in the past. The path part ",e.jsx(n.em,{children:"/hbase"})," is the znode parent, i.e, the configuration value for ",e.jsx(n.em,{children:"zookeeper.znode.parent"})," in the past."]}),`
`,e.jsx(n.h3,{id:"specify-configuration-through-uri-queries",children:"Specify Configuration through URI Queries"}),`
`,e.jsx(n.p,{children:"To let users fully specify the connection information through a connection URI, we support specifying configuration values through URI Queries. It looks like:"}),`
`,e.jsx(e.Fragment,{children:e.jsx(n.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z" fill="currentColor" /></svg>',children:e.jsx(n.code,{children:e.jsxs(n.span,{className:"line",children:[e.jsx(n.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"hbase+rpc://server1:16020?hbase.client.operation.timeout"}),e.jsx(n.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"=10000"})]})})})}),`
`,e.jsx(n.p,{children:"In this way you can set the operation timeout to 10 seconds. Notice that, the configuration values specified in the connection URI will override the ones in the configuration file."}),`
`,e.jsx(n.h3,{id:"implement-your-own-connection-registry",children:"Implement Your Own Connection Registry"}),`
`,e.jsxs(n.p,{children:["We use ",e.jsx(n.em,{children:"ServiceLoader"})," to load different connection registry implementations, the entry point is ",e.jsx(n.em,{children:"org.apache.hadoop.hbase.client.ConnectionRegistryURIFactory"}),". So if you implement your own ",e.jsx(n.em,{children:"ConnectionRegistryURIFactory"})," which has a different scheme, and register it in the services file, we can load it at runtime."]}),`
`,e.jsxs(n.p,{children:["Connection URI is still a very new feature which has not been used extensively in production, so we do not want to expose the ability to customize ",e.jsx(n.em,{children:"ConnectionRegistryURIFactory"})," yet as the API may be changed frequently in the beginning."]}),`
`,e.jsx(n.p,{children:"If you really want to implement your own connection registry, you can use the above way but take your own risk."})]})}function p(t={}){const{wrapper:n}=t.components||{};return n?e.jsx(n,{...t,children:e.jsx(s,{...t})}):s(t)}function a(t,n){throw new Error("Expected component `"+t+"` to be defined: you likely forgot to import, pass, or provide it.")}export{o as _markdown,p as default,c as extractedReferences,h as frontmatter,l as structuredData,d as toc};
