import{j as e}from"./chunk-OIYGIGL5-BFuAKb0n.js";let r=`## Operating System

### Memory

RAM, RAM, RAM. Don't starve HBase.

### 64-bit

Use a 64-bit platform (and 64-bit JVM).

### Swapping

Watch out for swapping. Set \`swappiness\` to 0.

### CPU

Make sure you have set up your Hadoop to use native, hardware checksumming. See [hadoop.native.lib](/docs/compression#making-use-of-hadoop-native-libraries-in-hbase).

## Network

Perhaps the most important factor in avoiding network issues degrading Hadoop and HBase performance is the switching hardware that is used, decisions made early in the scope of the project can cause major problems when you double or triple the size of your cluster (or more).

Important items to consider:

* Switching capacity of the device
* Number of systems connected
* Uplink capacity

### Single Switch

The single most important factor in this configuration is that the switching capacity of the hardware is capable of handling the traffic which can be generated by all systems connected to the switch. Some lower priced commodity hardware can have a slower switching capacity than could be utilized by a full switch.

### Multiple Switches

Multiple switches are a potential pitfall in the architecture. The most common configuration of lower priced hardware is a simple 1Gbps uplink from one switch to another. This often overlooked pinch point can easily become a bottleneck for cluster communication. Especially with MapReduce jobs that are both reading and writing a lot of data the communication across this uplink could be saturated.

Mitigation of this issue is fairly simple and can be accomplished in multiple ways:

* Use appropriate hardware for the scale of the cluster which you're attempting to build.
* Use larger single switch configurations i.e. single 48 port as opposed to 2x 24 port
* Configure port trunking for uplinks to utilize multiple interfaces to increase cross switch bandwidth.

### Multiple Racks

Multiple rack configurations carry the same potential issues as multiple switches, and can suffer performance degradation from two main areas:

* Poor switch capacity performance
* Insufficient uplink to another rack

If the switches in your rack have appropriate switching capacity to handle all the hosts at full speed, the next most likely issue will be caused by homing more of your cluster across racks. The easiest way to avoid issues when spanning multiple racks is to use port trunking to create a bonded uplink to other racks. The downside of this method however, is in the overhead of ports that could potentially be used. An example of this is, creating an 8Gbps port channel from rack A to rack B, using 8 of your 24 ports to communicate between racks gives you a poor ROI, using too few however can mean you're not getting the most out of your cluster.

Using 10Gbe links between racks will greatly increase performance, and assuming your switches support a 10Gbe uplink or allow for an expansion card will allow you to save your ports for machines as opposed to uplinks.

### Network Interfaces

Are all the network interfaces functioning correctly? Are you sure? See the Troubleshooting Case Study in [Case Study #1 (Performance Issue On A Single Node)](/docs/case-studies#case-study-1-performance-issue-on-a-single-node).

### Network Consistency and Partition Tolerance

The [CAP Theorem](http://en.wikipedia.org/wiki/CAP_theorem) states that a distributed system can maintain two out of the following three characteristics:

* ***C***&#x6F;nsistency — all nodes see the same data.
* ***A***&#x76;ailability — every request receives a response about whether it succeeded or failed.
* ***P***&#x61;rtition tolerance — the system continues to operate even if some of its components become unavailable to the others.

HBase favors consistency and partition tolerance, where a decision has to be made. Coda Hale explains why partition tolerance is so important, in [http://codahale.com/you-cant-sacrifice-partition-tolerance/](http://codahale.com/you-cant-sacrifice-partition-tolerance/).

Robert Yokota used an automated testing framework called [Jepson](https://aphyr.com/tags/jepsen) to test HBase's partition tolerance in the face of network partitions, using techniques modeled after Aphyr's [Call Me Maybe](https://aphyr.com/posts/281-call-me-maybe-carly-rae-jepsen-and-the-perils-of-network-partitions) series. The results, available as a [blog post](https://rayokota.wordpress.com/2015/09/30/call-me-maybe-hbase/) and an [addendum](https://rayokota.wordpress.com/2015/09/30/call-me-maybe-hbase-addendum/), show that HBase performs correctly.

## Java

### The Garbage Collector and Apache HBase

#### Long GC pauses

In his presentation, [Avoiding Full GCs with MemStore-Local Allocation Buffers](http://www.slideshare.net/cloudera/hbase-hug-presentation), Todd Lipcon describes two cases of stop-the-world garbage collections common in HBase, especially during loading; CMS failure modes and old generation heap fragmentation brought.

To address the first, start the CMS earlier than default by adding \`-XX:CMSInitiatingOccupancyFraction\` and setting it down from defaults. Start at 60 or 70 percent (The lower you bring down the threshold, the more GCing is done, the more CPU used). To address the second fragmentation issue, Todd added an experimental facility, (MSLAB), that must be explicitly enabled in Apache HBase 0.90.x (It's defaulted to be *on* in Apache 0.92.x HBase). Set \`hbase.hregion.memstore.mslab.enabled\` to true in your \`Configuration\`. See the cited slides for background and detail. The latest JVMs do better regards fragmentation so make sure you are running a recent release. Read down in the message, [Identifying concurrent mode failures caused by fragmentation](http://osdir.com/ml/hotspot-gc-use/2011-11/msg00002.html). Be aware that when enabled, each MemStore instance will occupy at least an MSLAB instance of memory. If you have thousands of regions or lots of regions each with many column families, this allocation of MSLAB may be responsible for a good portion of your heap allocation and in an extreme case cause you to OOME. Disable MSLAB in this case, or lower the amount of memory it uses or float less regions per server.

If you have a write-heavy workload, check out [HBASE-8163 MemStoreChunkPool: An improvement for JAVA GC when using MSLAB](https://issues.apache.org/jira/browse/HBASE-8163). It describes configurations to lower the amount of young GC during write-heavy loadings. If you do not have HBASE-8163 installed, and you are trying to improve your young GC times, one trick to consider — courtesy of our Liang Xie — is to set the GC config \`-XX:PretenureSizeThreshold\` in *hbase-env.sh* to be just smaller than the size of \`hbase.hregion.memstore.mslab.chunksize\` so MSLAB allocations happen in the tenured space directly rather than first in the young gen. You'd do this because these MSLAB allocations are going to likely make it to the old gen anyways and rather than pay the price of a copies between s0 and s1 in eden space followed by the copy up from young to old gen after the MSLABs have achieved sufficient tenure, save a bit of YGC churn and allocate in the old gen directly.

Other sources of long GCs can be the JVM itself logging. See [Eliminating Large JVM GC Pauses Caused by Background IO Traffic](https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-traffic)

For more information about GC logs, see [JVM Garbage Collection Logs](/docs/troubleshooting#jvm-garbage-collection-logs).

Consider also enabling the off-heap Block Cache. This has been shown to mitigate GC pause times. See [Block Cache](/docs/architecture/regionserver#architecture-regionserver-block-cache)

## HBase Configurations

See [Recommended Configurations](/docs/configuration/important#recommended-configurations).

### Improving the 99th Percentile

Try [hedged\\_reads](/docs/performance#hedged-reads).

### Managing Compactions

For larger systems, managing [compactions and splits](/docs/configuration/important#managed-compactions) may be something you want to consider.

### \`hbase.regionserver.handler.count\`

See [hbase.regionserver.handler.count](/docs/configuration/default#hbaseregionserverhandlercount-toc).

### \`hfile.block.cache.size\`

See [hfile.block.cache.size](/docs/configuration/default#hfileblockcachesize-toc). A memory setting for the RegionServer process.

### Prefetch Option for Blockcache

[HBASE-9857](https://issues.apache.org/jira/browse/HBASE-9857) adds a new option to prefetch HFile contents when opening the BlockCache, if a Column family or RegionServer property is set. This option is available for HBase 0.98.3 and later. The purpose is to warm the BlockCache as rapidly as possible after the cache is opened, using in-memory table data, and not counting the prefetching as cache misses. This is great for fast reads, but is not a good idea if the data to be preloaded will not fit into the BlockCache. It is useful for tuning the IO impact of prefetching versus the time before all data blocks are in cache.

To enable prefetching on a given column family, you can use HBase Shell or use the API.

**Enable Prefetch Using HBase Shell**

\`\`\`ruby
hbase> create 'MyTable', { NAME => 'myCF', PREFETCH_BLOCKS_ON_OPEN => 'true' }
\`\`\`

**Enable Prefetch Using the API**

\`\`\`java
// ...
HTableDescriptor tableDesc = new HTableDescriptor("myTable");
HColumnDescriptor cfDesc = new HColumnDescriptor("myCF");
cfDesc.setPrefetchBlocksOnOpen(true);
tableDesc.addFamily(cfDesc);
// ...
\`\`\`

See the API documentation for [CacheConfig](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CacheConfig.html).

To see prefetch in operation, enable TRACE level logging on \`org.apache.hadoop.hbase.io.hfile.HFileReaderImpl\` in hbase-2.0+ or on \`org.apache.hadoop.hbase.io.hfile.HFileReaderV2\` in earlier versions, hbase-1.x, of HBase.

### \`hbase.regionserver.global.memstore.size\`

See [hbase.regionserver.global.memstore.size](/docs/configuration/default#hbaseregionserverglobalmemstoresize-toc). This memory setting is often adjusted for the RegionServer process depending on needs.

### \`hbase.regionserver.global.memstore.size.lower.limit\`

See [hbase.regionserver.global.memstore.size.lower.limit](/docs/configuration/default#hbaseregionserverglobalmemstoresizelowerlimit-toc). This memory setting is often adjusted for the RegionServer process depending on needs.

### \`hbase.hstore.blockingStoreFiles\`

See [hbase.hstore.blockingStoreFiles](/docs/configuration/default#hbasehstoreblockingStoreFiles-toc). If there is blocking in the RegionServer logs, increasing this can help.

### \`hbase.hregion.memstore.block.multiplier\`

See [hbase.hregion.memstore.block.multiplier](/docs/configuration/default#hbasehregionmemstoreblockmultiplier-toc). If there is enough RAM, increasing this can help.

### \`hbase.regionserver.checksum.verify\`

Have HBase write the checksum into the datablock and save having to do the checksum seek whenever you read.

See [hbase.regionserver.checksum.verify](/docs/configuration/default#hbaseregionserverchecksumverify-toc), [hbase.hstore.bytes.per.checksum](/docs/configuration/default#hbasehstorebytesperchecksum-toc) and [hbase.hstore.checksum.algorithm](/docs/configuration/default#hbasehstorechecksumalgorithm-toc). For more information see the release note on [HBASE-5074 support checksums in HBase block cache](https://issues.apache.org/jira/browse/HBASE-5074).

### Tuning \`callQueue\` Options

[HBASE-11355](https://issues.apache.org/jira/browse/HBASE-11355) introduces several callQueue tuning mechanisms which can increase performance. See the JIRA for some benchmarking information.

To increase the number of callqueues, set \`hbase.ipc.server.num.callqueue\` to a value greater than \`1\`. To split the callqueue into separate read and write queues, set \`hbase.ipc.server.callqueue.read.ratio\` to a value between \`0\` and \`1\`. This factor weights the queues toward writes (if below .5) or reads (if above .5). Another way to say this is that the factor determines what percentage of the split queues are used for reads. The following examples illustrate some of the possibilities. Note that you always have at least one write queue, no matter what setting you use.

* The default value of \`0\` does not split the queue.
* A value of \`.3\` uses 30% of the queues for reading and 70% for writing. Given a value of \`10\` for \`hbase.ipc.server.num.callqueue\`, 3 queues would be used for reads and 7 for writes.
* A value of \`.5\` uses the same number of read queues and write queues. Given a value of \`10\` for \`hbase.ipc.server.num.callqueue\`, 5 queues would be used for reads and 5 for writes.
* A value of \`.6\` uses 60% of the queues for reading and 40% for writing. Given a value of \`10\` for \`hbase.ipc.server.num.callqueue\`, 6 queues would be used for reads and 4 for writes.
* A value of \`1.0\` uses one queue to process write requests, and all other queues process read requests. A value higher than \`1.0\` has the same effect as a value of \`1.0\`. Given a value of \`10\` for \`hbase.ipc.server.num.callqueue\`, 9 queues would be used for reads and 1 for writes.

You can also split the read queues so that separate queues are used for short reads (from Get operations) and long reads (from Scan operations), by setting the \`hbase.ipc.server.callqueue.scan.ratio\` option. This option is a factor between 0 and 1, which determine the ratio of read queues used for Gets and Scans. More queues are used for Gets if the value is below \`.5\` and more are used for scans if the value is above \`.5\`. No matter what setting you use, at least one read queue is used for Get operations.

* A value of \`0\` does not split the read queue.
* A value of \`.3\` uses 70% of the read queues for Gets and 30% for Scans. Given a value of \`20\` for \`hbase.ipc.server.num.callqueue\` and a value of \`.5\` for \`hbase.ipc.server.callqueue.read.ratio\`, 10 queues would be used for reads, out of those 10, 7 would be used for Gets and 3 for Scans.
* A value of \`.5\` uses half the read queues for Gets and half for Scans. Given a value of \`20\` for \`hbase.ipc.server.num.callqueue\` and a value of \`.5\` for \`hbase.ipc.server.callqueue.read.ratio\`, 10 queues would be used for reads, out of those 10, 5 would be used for Gets and 5 for Scans.
* A value of \`.7\` uses 30% of the read queues for Gets and 70% for Scans. Given a value of \`20\` for \`hbase.ipc.server.num.callqueue\` and a value of \`.5\` for \`hbase.ipc.server.callqueue.read.ratio\`, 10 queues would be used for reads, out of those 10, 3 would be used for Gets and 7 for Scans.
* A value of \`1.0\` uses all but one of the read queues for Scans. Given a value of \`20\` for \`hbase.ipc.server.num.callqueue\` and a value of \`.5\` for \`hbase.ipc.server.callqueue.read.ratio\`, 10 queues would be used for reads, out of those 10, 1 would be used for Gets and 9 for Scans.

You can use the new option \`hbase.ipc.server.callqueue.handler.factor\` to programmatically tune the number of queues:

* A value of \`0\` uses a single shared queue between all the handlers.
* A value of \`1\` uses a separate queue for each handler.
* A value between \`0\` and \`1\` tunes the number of queues against the number of handlers. For instance, a value of \`.5\` shares one queue between each two handlers.\\
  Having more queues, such as in a situation where you have one queue per handler, reduces contention when adding a task to a queue or selecting it from a queue. The trade-off is that if you have some queues with long-running tasks, a handler may end up waiting to execute from that queue rather than processing another queue which has waiting tasks.

For these values to take effect on a given RegionServer, the RegionServer must be restarted. These parameters are intended for testing purposes and should be used carefully.

## ZooKeeper

See [ZooKeeper](/docs/zookeeper) for information on configuring ZooKeeper, and see the part about having a dedicated disk.

## Schema Design

### Number of Column Families

See [On the number of column families](/docs/regionserver-sizing#on-the-number-of-column-families).

### Key and Attribute Lengths

See [Try to minimize row and column sizes](/docs/regionserver-sizing#try-to-minimize-row-and-column-sizes). See also [However...](/docs/performance#however) for compression caveats.

### Table RegionSize

The regionsize can be set on a per-table basis via \`setMaxFileSize\` on [TableDescriptorBuilder](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/TableDescriptorBuilder.html) in the event where certain tables require different regionsizes than the configured default regionsize.

See [Determining region count and size](/docs/operational-management/region-and-capacity#determining-region-count-and-size) for more information.

### Bloom Filters

A Bloom filter, named for its creator, Burton Howard Bloom, is a data structure which is designed to predict whether a given element is a member of a set of data. A positive result from a Bloom filter is not always accurate, but a negative result is guaranteed to be accurate. Bloom filters are designed to be "accurate enough" for sets of data which are so large that conventional hashing mechanisms would be impractical. For more information about Bloom filters in general, refer to [http://en.wikipedia.org/wiki/Bloom\\_filter](http://en.wikipedia.org/wiki/Bloom_filter).

In terms of HBase, Bloom filters provide a lightweight in-memory structure to reduce the number of disk reads for a given Get operation (Bloom filters do not work with Scans) to only the StoreFiles likely to contain the desired Row. The potential performance gain increases with the number of parallel reads.

The Bloom filters themselves are stored in the metadata of each HFile and never need to be updated. When an HFile is opened because a region is deployed to a RegionServer, the Bloom filter is loaded into memory.

HBase includes some tuning mechanisms for folding the Bloom filter to reduce the size and keep the false positive rate within a desired range.

Bloom filters were introduced in [HBASE-1200](https://issues.apache.org/jira/browse/HBASE-1200). Since HBase 0.96, row-based Bloom filters are enabled by default. ([HBASE-8450](https://issues.apache.org/jira/browse/HBASE-8450))

For more information on Bloom filters in relation to HBase, see [Bloom Filters](/docs/performance#bloom-filters) for more information, or the following Quora discussion: [How are bloom filters used in HBase?](http://www.quora.com/How-are-bloom-filters-used-in-HBase).

#### When To Use Bloom Filters

Since HBase 0.96, row-based Bloom filters are enabled by default. You may choose to disable them or to change some tables to use row+column Bloom filters, depending on the characteristics of your data and how it is loaded into HBase.

To determine whether Bloom filters could have a positive impact, check the value of \`blockCacheHitRatio\` in the RegionServer metrics. If Bloom filters are enabled, the value of \`blockCacheHitRatio\` should increase, because the Bloom filter is filtering out blocks that are definitely not needed.

You can choose to enable Bloom filters for a row or for a row+column combination. If you generally scan entire rows, the row+column combination will not provide any benefit. A row-based Bloom filter can operate on a row+column Get, but not the other way around. However, if you have a large number of column-level Puts, such that a row may be present in every StoreFile, a row-based filter will always return a positive result and provide no benefit. Unless you have one column per row, row+column Bloom filters require more space, in order to store more keys. Bloom filters work best when the size of each data entry is at least a few kilobytes in size.

Overhead will be reduced when your data is stored in a few larger StoreFiles, to avoid extra disk IO during low-level scans to find a specific row.

Bloom filters need to be rebuilt upon deletion, so may not be appropriate in environments with a large number of deletions.

#### Enabling Bloom Filters

Bloom filters are enabled on a Column Family. You can do this by using the setBloomFilterType method of HColumnDescriptor or using the HBase API. Valid values are \`NONE\`, \`ROW\` (default), or \`ROWCOL\`. See [When To Use Bloom Filters](/docs/performance#when-to-use-bloom-filters) for more information on \`ROW\` versus \`ROWCOL\`. See also the API documentation for [ColumnFamilyDescriptorBuilder](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.html).

The following example creates a table and enables a ROWCOL Bloom filter on the \`colfam1\` column family.

\`\`\`ruby
hbase> create 'mytable',{NAME => 'colfam1', BLOOMFILTER => 'ROWCOL'}
\`\`\`

#### Configuring Server-Wide Behavior of Bloom Filters

You can configure the following settings in the *hbase-site.xml*.

| Parameter                                | Default   | Description                                                                                                                                                    |
| ---------------------------------------- | --------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| io.storefile.bloom.enabled               | yes       | Set to no to kill bloom filters server-wide if something goes wrong                                                                                            |
| io.storefile.bloom.error.rate            | .01       | The average false positive rate for bloom filters. Folding is used to maintain the false positive rate. Expressed as a decimal representation of a percentage. |
| io.storefile.bloom.max.fold              | 7         | The guaranteed maximum fold rate. Changing this setting should not be necessary and is not recommended.                                                        |
| io.storefile.bloom.max.keys              | 128000000 | For default (single-block) Bloom filters, this specifies the maximum number of keys.                                                                           |
| io.storefile.delete.family.bloom.enabled | true      | Master switch to enable Delete Family Bloom filters and store them in the StoreFile.                                                                           |
| io.storefile.bloom.block.size            | 131072    | Target Bloom block size. Bloom filter blocks of approximately this size are interleaved with data blocks.                                                      |
| hfile.block.bloom.cacheonwrite           | false     | Enables cache-on-write for inline blocks of a compound Bloom filter.                                                                                           |

### ColumnFamily BlockSize

The blocksize can be configured for each ColumnFamily in a table, and defaults to 64k. Larger cell values require larger blocksizes. There is an inverse relationship between blocksize and the resulting StoreFile indexes (i.e., if the blocksize is doubled then the resulting indexes should be roughly halved).

See [ColumnFamilyDescriptorBuilder](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.html) and [Store](/docs/architecture/regions#store) for more information.

### In-Memory ColumnFamilies

ColumnFamilies can optionally be defined as in-memory. Data is still persisted to disk, just like any other ColumnFamily. In-memory blocks have the highest priority in the [Block Cache](/docs/architecture/regionserver#architecture-regionserver-block-cache), but it is not a guarantee that the entire table will be in memory.

See [ColumnFamilyDescriptorBuilder](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.html) for more information.

### Compression

Production systems should use compression with their ColumnFamily definitions. See [Compression and Data Block Encoding In HBase](/docs/compression) for more information.

#### However...

Compression deflates data *on disk*. When it's in-memory (e.g., in the MemStore) or on the wire (e.g., transferring between RegionServer and Client) it's inflated. So while using ColumnFamily compression is a best practice, but it's not going to completely eliminate the impact of over-sized Keys, over-sized ColumnFamily names, or over-sized Column names.

See [Try to minimize row and column sizes](/docs/regionserver-sizing#try-to-minimize-row-and-column-sizes) on for schema design tips, and [KeyValue](/docs/architecture/regions#keyvalue) for more information on HBase stores data internally.

## HBase General Patterns

### Constants

When people get started with HBase they have a tendency to write code that looks like this:

\`\`\`java
Get get = new Get(rowkey);
Result r = table.get(get);
byte[] b = r.getValue(Bytes.toBytes("cf"), Bytes.toBytes("attr"));  // returns current version of value
\`\`\`

But especially when inside loops (and MapReduce jobs), converting the columnFamily and column-names to byte-arrays repeatedly is surprisingly expensive. It's better to use constants for the byte-arrays, like this:

\`\`\`java
public static final byte[] CF = "cf".getBytes();
public static final byte[] ATTR = "attr".getBytes();
...
Get get = new Get(rowkey);
Result r = table.get(get);
byte[] b = r.getValue(CF, ATTR);  // returns current version of value
\`\`\`

## Writing to HBase

### Batch Loading

Use the bulk load tool if you can. See [Bulk Loading](/docs/architecture/bulk-loading). Otherwise, pay attention to the below.

### Table Creation: Pre-Creating Regions

Tables in HBase are initially created with one region by default. For bulk imports, this means that all clients will write to the same region until it is large enough to split and become distributed across the cluster. A useful pattern to speed up the bulk import process is to pre-create empty regions. Be somewhat conservative in this, because too-many regions can actually degrade performance.

There are two different approaches to pre-creating splits using the HBase API. The first approach is to rely on the default \`Admin\` strategy (which is implemented in \`Bytes.split\`)...

\`\`\`java
byte[] startKey = ...;      // your lowest key
byte[] endKey = ...;        // your highest key
int numberOfRegions = ...;  // # of regions to create
admin.createTable(table, startKey, endKey, numberOfRegions);
\`\`\`

And the other approach, using the HBase API, is to define the splits yourself...

\`\`\`java
byte[][] splits = ...;   // create your own splits
admin.createTable(table, splits);
\`\`\`

You can achieve a similar effect using the HBase Shell to create tables by specifying split options.

\`\`\`ruby
# create table with specific split points
hbase>create 't1','f1',SPLITS => ['\\x10\\x00', '\\x20\\x00', '\\x30\\x00', '\\x40\\x00']

# create table with four regions based on random bytes keys
hbase>create 't2','f1', { NUMREGIONS => 4 , SPLITALGO => 'UniformSplit' }

# create table with five regions based on hex keys
create 't3','f1', { NUMREGIONS => 5, SPLITALGO => 'HexStringSplit' }
\`\`\`

See [Relationship Between RowKeys and Region Splits](/docs/regionserver-sizing#relationship-between-rowkeys-and-region-splits) for issues related to understanding your keyspace and pre-creating regions. See [manual region splitting decisions](/docs/architecture/regions#manual-region-splitting) for discussion on manually pre-splitting regions. See [Pre-splitting tables with the HBase Shell](/docs/shell#pre-splitting-tables-with-the-hbase-shell) for more details of using the HBase Shell to pre-split tables.

### Table Creation: Deferred Log Flush

The default behavior for Puts using the Write Ahead Log (WAL) is that \`WAL\` edits will be written immediately. If deferred log flush is used, WAL edits are kept in memory until the flush period. The benefit is aggregated and asynchronous \`WAL\`- writes, but the potential downside is that if the RegionServer goes down the yet-to-be-flushed edits are lost. This is safer, however, than not using WAL at all with Puts.

Deferred log flush can be configured on tables via [TableDescriptorBuilder](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/TableDescriptorBuilder.html). The default value of \`hbase.regionserver.optionallogflushinterval\` is 1000ms.

### HBase Client: Turn off WAL on Puts

A frequent request is to disable the WAL to increase performance of Puts. This is only appropriate for bulk loads, as it puts your data at risk by removing the protection of the WAL in the event of a region server crash. Bulk loads can be re-run in the event of a crash, with little risk of data loss.

<Callout type="warn">
  If you disable the WAL for anything other than bulk loads, your data is at risk.
</Callout>

In general, it is best to use WAL for Puts, and where loading throughput is a concern to use bulk loading techniques instead. For normal Puts, you are not likely to see a performance improvement which would outweigh the risk. To disable the WAL, see [Disabling the WAL](/docs/architecture/regionserver#disabling-the-wal).

### HBase Client: Group Puts by RegionServer

In addition to using the writeBuffer, grouping \`Put\`s by RegionServer can reduce the number of client RPC calls per writeBuffer flush. There is a utility \`HTableUtil\` currently on MASTER that does this, but you can either copy that or implement your own version for those still on 0.90.x or earlier.

### MapReduce: Skip The Reducer

When writing a lot of data to an HBase table from a MR job (e.g., with [TableOutputFormat](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html)), and specifically where Puts are being emitted from the Mapper, skip the Reducer step. When a Reducer step is used, all of the output (Puts) from the Mapper will get spooled to disk, then sorted/shuffled to other Reducers that will most likely be off-node. It's far more efficient to just write directly to HBase.

For summary jobs where HBase is used as a source and a sink, then writes will be coming from the Reducer step (e.g., summarize values then write out result). This is a different processing problem than from the above case.

### Anti-Pattern: One Hot Region

If all your data is being written to one region at a time, then re-read the section on processing timeseries data.

Also, if you are pre-splitting regions and all your data is *still* winding up in a single region even though your keys aren't monotonically increasing, confirm that your keyspace actually works with the split strategy. There are a variety of reasons that regions may appear "well split" but won't work with your data. As the HBase client communicates directly with the RegionServers, this can be obtained via [RegionLocator.getRegionLocation](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/RegionLocator.html#getRegionLocation\\(byte%5B%5D\\)).

See [Table Creation: Pre-Creating Regions](/docs/performance#table-creation-pre-creating-regions), as well as [HBase Configurations](/docs/performance#hbase-configurations)

## Reading from HBase

The mailing list can help if you are having performance issues.

### Scan Caching

If HBase is used as an input source for a MapReduce job, for example, make sure that the input [Scan](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Scan.html) instance to the MapReduce job has \`setCaching\` set to something greater than the default (which is 1). Using the default value means that the map-task will make call back to the region-server for every record processed. Setting this value to 500, for example, will transfer 500 rows at a time to the client to be processed. There is a cost/benefit to have the cache value be large because it costs more in memory for both client and RegionServer, so bigger isn't always better.

#### Scan Caching in MapReduce Jobs

Scan settings in MapReduce jobs deserve special attention. Timeouts can result (e.g., UnknownScannerException) in Map tasks if it takes longer to process a batch of records before the client goes back to the RegionServer for the next set of data. This problem can occur because there is non-trivial processing occurring per row. If you process rows quickly, set caching higher. If you process rows more slowly (e.g., lots of transformations per row, writes), then set caching lower.

Timeouts can also happen in a non-MapReduce use case (i.e., single threaded HBase client doing a Scan), but the processing that is often performed in MapReduce jobs tends to exacerbate this issue.

### Scan Attribute Selection

Whenever a Scan is used to process large numbers of rows (and especially when used as a MapReduce source), be aware of which attributes are selected. If \`scan.addFamily\` is called then *all* of the attributes in the specified ColumnFamily will be returned to the client. If only a small number of the available attributes are to be processed, then only those attributes should be specified in the input scan because attribute over-selection is a non-trivial performance penalty over large datasets.

### Avoid scan seeks

When columns are selected explicitly with \`scan.addColumn\`, HBase will schedule seek operations to seek between the selected columns. When rows have few columns and each column has only a few versions this can be inefficient. A seek operation is generally slower if does not seek at least past 5-10 columns/versions or 512-1024 bytes.

In order to opportunistically look ahead a few columns/versions to see if the next column/version can be found that way before a seek operation is scheduled, a new attribute \`Scan.HINT_LOOKAHEAD\` can be set on the Scan object. The following code instructs the RegionServer to attempt two iterations of next before a seek is scheduled:

\`\`\`java
Scan scan = new Scan();
scan.addColumn(...);
scan.setAttribute(Scan.HINT_LOOKAHEAD, Bytes.toBytes(2));
table.getScanner(scan);
\`\`\`

### MapReduce - Input Splits

For MapReduce jobs that use HBase tables as a source, if there a pattern where the "slow" map tasks seem to have the same Input Split (i.e., the RegionServer serving the data), see the Troubleshooting Case Study in [Case Study #1 (Performance Issue On A Single Node)](/docs/case-studies#case-study-1-performance-issue-on-a-single-node).

### Close ResultScanners

This isn't so much about improving performance but rather *avoiding* performance problems. If you forget to close [ResultScanners](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ResultScanner.html) you can cause problems on the RegionServers. Always have ResultScanner processing enclosed in try/catch blocks.

\`\`\`java
Scan scan = new Scan();
// set attrs...
ResultScanner rs = table.getScanner(scan);
try {
  for (Result r = rs.next(); r != null; r = rs.next()) {
    // process result...
  }
} finally {
  rs.close();  // always close the ResultScanner!
}
table.close();
\`\`\`

### Block Cache

[Scan](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Scan.html) instances can be set to use the block cache in the RegionServer via the \`setCacheBlocks\` method. For input Scans to MapReduce jobs, this should be \`false\`. For frequently accessed rows, it is advisable to use the block cache.

Cache more data by moving your Block Cache off-heap. See [Off-heap Block Cache](/docs/architecture/regionserver#off-heap-block-cache)

### Optimal Loading of Row Keys

When performing a table [scan](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Scan.html) where only the row keys are needed (no families, qualifiers, values or timestamps), add a FilterList with a \`MUST_PASS_ALL\` operator to the scanner using \`setFilter\`. The filter list should include both a [FirstKeyOnlyFilter](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html) and a [KeyOnlyFilter](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/filter/KeyOnlyFilter.html). Using this filter combination will result in a worst case scenario of a RegionServer reading a single value from disk and minimal network traffic to the client for a single row.

### Concurrency: Monitor Data Spread

When performing a high number of concurrent reads, monitor the data spread of the target tables. If the target table(s) have too few regions then the reads could likely be served from too few nodes.

See [Table Creation: Pre-Creating Regions](/docs/performance#table-creation-pre-creating-regions), as well as [HBase Configurations](/docs/performance#hbase-configurations)

### Bloom Filters

Enabling Bloom Filters can save your having to go to disk and can help improve read latencies.

[Bloom filters](http://en.wikipedia.org/wiki/Bloom_filter) were developed over in [HBase-1200 Add bloomfilters](https://issues.apache.org/jira/browse/HBASE-1200). For description of the development process — why static blooms rather than dynamic — and for an overview of the unique properties that pertain to blooms in HBase, as well as possible future directions, see the *Development Process* section of the document [BloomFilters in HBase](https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf) attached to [HBASE-1200](https://issues.apache.org/jira/browse/HBASE-1200). The bloom filters described here are actually version two of blooms in HBase. In versions up to 0.19.x, HBase had a dynamic bloom option based on work done by the [European Commission One-Lab Project 034819](http://www.onelab.org). The core of the HBase bloom work was later pulled up into Hadoop to implement org.apache.hadoop.io.BloomMapFile. Version 1 of HBase blooms never worked that well. Version 2 is a rewrite from scratch though again it starts with the one-lab work.

See also [Bloom Filters](/docs/performance#bloom-filters).

#### Bloom StoreFile footprint

Bloom filters add an entry to the \`StoreFile\` general \`FileInfo\` data structure and then two extra entries to the \`StoreFile\` metadata section.

**BloomFilter in the \`StoreFile\` \`FileInfo\` data structure**\\
\`FileInfo\` has a \`BLOOM_FILTER_TYPE\` entry which is set to \`NONE\`, \`ROW\` or \`ROWCOL.\`

**BloomFilter entries in \`StoreFile\` metadata**\\
\`BLOOM_FILTER_META\` holds Bloom Size, Hash Function used, etc. It's small in size and is cached on \`StoreFile.Reader\` load.\\
\`BLOOM_FILTER_DATA\` is the actual bloomfilter data. Obtained on-demand. Stored in the LRU cache, if it is enabled (It's enabled by default).

#### Bloom Filter Configuration

**\`io.storefile.bloom.enabled\` global kill switch**\\
\`io.storefile.bloom.enabled\` in \`Configuration\` serves as the kill switch in case something goes wrong. Default = \`true\`.

\`io.storefile.bloom.error.rate\`\\
\`io.storefile.bloom.error.rate\` = average false positive rate. Default = 1%. Decrease rate by ½ (e.g. to .5%) == +1 bit per bloom entry.

\`io.storefile.bloom.max.fold\`\\
\`io.storefile.bloom.max.fold\` = guaranteed minimum fold rate. Most people should leave this alone. Default = 7, or can collapse to at least 1/128th of original size. See the *Development Process* section of the document [BloomFilters in HBase](https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf) for more on what this option means.

### Hedged Reads

Hedged reads are a feature of HDFS, introduced in Hadoop 2.4.0 with [HDFS-5776](https://issues.apache.org/jira/browse/HDFS-5776). Normally, a single thread is spawned for each read request. However, if hedged reads are enabled, the client waits some configurable amount of time, and if the read does not return, the client spawns a second read request, against a different block replica of the same data. Whichever read returns first is used, and the other read request is discarded.

Hedged reads are "...very good at eliminating outlier datanodes, which in turn makes them very good choice for latency sensitive setups. But, if you are looking for maximizing throughput, hedged reads tend to create load amplification as things get slower in general. In short, the thing to watch out for is the non-graceful performance degradation when you are running close a certain throughput threshold." (Quote from Ashu Pachauri in HBASE-17083).

Other concerns to keep in mind while running with hedged reads enabled include:

* They may lead to network congestion. See [HBASE-17083](https://issues.apache.org/jira/browse/HBASE-17083)
* Make sure you set the thread pool large enough so as blocking on the pool does not become a bottleneck (Again see [HBASE-17083](https://issues.apache.org/jira/browse/HBASE-17083))

(From Yu Li up in HBASE-17083)

Because an HBase RegionServer is a HDFS client, you can enable hedged reads in HBase, by adding the following properties to the RegionServer's hbase-site.xml and tuning the values to suit your environment.

**Configuration for Hedged Reads**

* \`dfs.client.hedged.read.threadpool.size\` - the number of threads dedicated to servicing hedged reads. If this is set to 0 (the default), hedged reads are disabled.
* \`dfs.client.hedged.read.threshold.millis\` - the number of milliseconds to wait before spawning a second read thread.

**Hedged Reads Configuration Example**

\`\`\`xml
<property>
  <name>dfs.client.hedged.read.threadpool.size</name>
  <value>20</value>  <!-- 20 threads -->
</property>
<property>
  <name>dfs.client.hedged.read.threshold.millis</name>
  <value>10</value>  <!-- 10 milliseconds -->
</property>
\`\`\`

Use the following metrics to tune the settings for hedged reads on your cluster. See [HBase Metrics](/docs/operational-management/metrics-and-monitoring) for more information.

**Metrics for Hedged Reads**

* hedgedReadOps - the number of times hedged read threads have been triggered. This could indicate that read requests are often slow, or that hedged reads are triggered too quickly.
* hedgeReadOpsWin - the number of times the hedged read thread was faster than the original thread. This could indicate that a given RegionServer is having trouble servicing requests.
* hedgedReadOpsInCurThread - the number of times hedged read was rejected from executor and needed to fallback to be executed in current thread. This could indicate that current hedged read thread pool size is not appropriate.

## Deleting from HBase

### Using HBase Tables as Queues

HBase tables are sometimes used as queues. In this case, special care must be taken to regularly perform major compactions on tables used in this manner. As is documented in [Data Model](/docs/datamodel), marking rows as deleted creates additional StoreFiles which then need to be processed on reads. Tombstones only get cleaned up with major compactions.

See also [Compaction](/docs/architecture/regions#compaction) and [Admin.majorCompact](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Admin.html#majorCompact\\(org.apache.hadoop.hbase.TableName\\)).

### Delete RPC Behavior

Be aware that \`Table.delete(Delete)\` doesn't use the writeBuffer. It will execute an RegionServer RPC with each invocation. For a large number of deletes, consider \`Table.delete(List)\`.

See [hbase.client.Delete](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html#delete\\(org.apache.hadoop.hbase.client.Delete\\))

## HDFS

Because HBase runs on [HDFS](/docs/architecture/hdfs) it is important to understand how it works and how it affects HBase.

### Current Issues With Low-Latency Reads

The original use-case for HDFS was batch processing. As such, there low-latency reads were historically not a priority. With the increased adoption of Apache HBase this is changing, and several improvements are already in development. See the [Umbrella Jira Ticket for HDFS Improvements for HBase](https://issues.apache.org/jira/browse/HDFS-1599).

### Leveraging local data

Since Hadoop 1.0.0 (also 0.22.1, 0.23.1, CDH3u3 and HDP 1.0) via [HDFS-2246](https://issues.apache.org/jira/browse/HDFS-2246), it is possible for the DFSClient to take a "short circuit" and read directly from the disk instead of going through the DataNode when the data is local. What this means for HBase is that the RegionServers can read directly off their machine's disks instead of having to open a socket to talk to the DataNode, the former being generally much faster. See JD's [Performance Talk](http://files.meetup.com/1350427/hug_ebay_jdcryans.pdf). Also see [HBase, mail # dev - read short circuit](https://lists.apache.org/thread.html/ce2ce3a3bbd20806d0c017b2e7528e78a46ccb87c063831db051949d%401347548325%40%3Cdev.hbase.apache.org%3E) thread for more discussion around short circuit reads.

To enable "short circuit" reads, it will depend on your version of Hadoop. The original shortcircuit read patch was much improved upon in Hadoop 2 in [HDFS-347](https://issues.apache.org/jira/browse/HDFS-347). See [http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/](http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/) for details on the difference between the old and new implementations. See [Hadoop shortcircuit reads configuration page](http://archive.cloudera.com/cdh4/cdh/4/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html) for how to enable the latter, better version of shortcircuit. For example, here is a minimal config. enabling short-circuit reads added to *hbase-site.xml*:

\`\`\`xml
<property>
  <name>dfs.client.read.shortcircuit</name>
  <value>true</value>
  <description>
    This configuration parameter turns on short-circuit local reads.
  </description>
</property>
<property>
  <name>dfs.domain.socket.path</name>
  <value>/home/stack/sockets/short_circuit_read_socket_PORT</value>
  <description>
    Optional.  This is a path to a UNIX domain socket that will be used for
    communication between the DataNode and local HDFS clients.
    If the string "_PORT" is present in this path, it will be replaced by the
    TCP port of the DataNode.
  </description>
</property>
\`\`\`

Be careful about permissions for the directory that hosts the shared domain socket; dfsclient will complain if open to other than the hbase user.

If you are running on an old Hadoop, one that is without [HDFS-347](https://issues.apache.org/jira/browse/HDFS-347) but that has [HDFS-2246](https://issues.apache.org/jira/browse/HDFS-2246), you must set two configurations. First, the hdfs-site.xml needs to be amended. Set the property \`dfs.block.local-path-access.user\` to be the *only* user that can use the shortcut. This has to be the user that started HBase. Then in hbase-site.xml, set \`dfs.client.read.shortcircuit\` to be \`true\`

Services — at least the HBase RegionServers — will need to be restarted in order to pick up the new configurations.

<Callout type="info" title="dfs.client.read.shortcircuit.buffer.size">
  The default for this value is too high when running on a highly trafficked HBase. In HBase, if
  this value has not been set, we set it down from the default of 1M to 128k (Since HBase 0.98.0 and
  0.96.1). See [HBASE-8143 HBase on Hadoop 2 with local short circuit reads (ssr) causes
  OOM](https://issues.apache.org/jira/browse/HBASE-8143). The Hadoop DFSClient in HBase will
  allocate a direct byte buffer of this size for *each* block it has open; given HBase keeps its
  HDFS files open all the time, this can add up quickly.
</Callout>

### Performance Comparisons of HBase vs. HDFS

A fairly common question on the dist-list is why HBase isn't as performant as HDFS files in a batch context (e.g., as a MapReduce source or sink). The short answer is that HBase is doing a lot more than HDFS (e.g., reading the KeyValues, returning the most current row or specified timestamps, etc.), and as such HBase is 4-5 times slower than HDFS in this processing context. There is room for improvement and this gap will, over time, be reduced, but HDFS will always be faster in this use-case.

## Amazon EC2

Performance questions are common on Amazon EC2 environments because it is a shared environment. You will not see the same throughput as a dedicated server. In terms of running tests on EC2, run them several times for the same reason (i.e., it's a shared environment and you don't know what else is happening on the server).

If you are running on EC2 and post performance questions on the dist-list, please state this fact up-front that because EC2 issues are practically a separate class of performance issues.

## Collocating HBase and MapReduce

It is often recommended to have different clusters for HBase and MapReduce. A better qualification of this is: don't collocate an HBase that serves live requests with a heavy MR workload. OLTP and OLAP-optimized systems have conflicting requirements and one will lose to the other, usually the former. For example, short latency-sensitive disk reads will have to wait in line behind longer reads that are trying to squeeze out as much throughput as possible. MR jobs that write to HBase will also generate flushes and compactions, which will in turn invalidate blocks in the [Block Cache](/docs/architecture/regionserver#architecture-regionserver-block-cache).

If you need to process the data from your live HBase cluster in MR, you can ship the deltas with [CopyTable](/docs/operational-management/tools#copytable) or use replication to get the new data in real time on the OLAP cluster. In the worst case, if you really need to collocate both, set MR to use less Map and Reduce slots than you'd normally configure, possibly just one.

When HBase is used for OLAP operations, it's preferable to set it up in a hardened way like configuring the ZooKeeper session timeout higher and giving more memory to the MemStores (the argument being that the Block Cache won't be used much since the workloads are usually long scans).

## Case Studies

For Performance and Troubleshooting Case Studies, see [Apache HBase Case Studies](/docs/case-studies).
`,l={title:"Apache HBase Performance Tuning",description:"Comprehensive performance tuning guide covering OS configuration, network optimization, JVM tuning, schema design, read/write patterns, and HDFS optimization for HBase clusters."},h=[{href:"/docs/compression#making-use-of-hadoop-native-libraries-in-hbase"},{href:"/docs/case-studies#case-study-1-performance-issue-on-a-single-node"},{href:"http://en.wikipedia.org/wiki/CAP_theorem"},{href:"http://codahale.com/you-cant-sacrifice-partition-tolerance/"},{href:"https://aphyr.com/tags/jepsen"},{href:"https://aphyr.com/posts/281-call-me-maybe-carly-rae-jepsen-and-the-perils-of-network-partitions"},{href:"https://rayokota.wordpress.com/2015/09/30/call-me-maybe-hbase/"},{href:"https://rayokota.wordpress.com/2015/09/30/call-me-maybe-hbase-addendum/"},{href:"http://www.slideshare.net/cloudera/hbase-hug-presentation"},{href:"http://osdir.com/ml/hotspot-gc-use/2011-11/msg00002.html"},{href:"https://issues.apache.org/jira/browse/HBASE-8163"},{href:"https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-traffic"},{href:"/docs/troubleshooting#jvm-garbage-collection-logs"},{href:"/docs/architecture/regionserver#architecture-regionserver-block-cache"},{href:"/docs/configuration/important#recommended-configurations"},{href:"/docs/performance#hedged-reads"},{href:"/docs/configuration/important#managed-compactions"},{href:"/docs/configuration/default#hbaseregionserverhandlercount-toc"},{href:"/docs/configuration/default#hfileblockcachesize-toc"},{href:"https://issues.apache.org/jira/browse/HBASE-9857"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CacheConfig.html"},{href:"/docs/configuration/default#hbaseregionserverglobalmemstoresize-toc"},{href:"/docs/configuration/default#hbaseregionserverglobalmemstoresizelowerlimit-toc"},{href:"/docs/configuration/default#hbasehstoreblockingStoreFiles-toc"},{href:"/docs/configuration/default#hbasehregionmemstoreblockmultiplier-toc"},{href:"/docs/configuration/default#hbaseregionserverchecksumverify-toc"},{href:"/docs/configuration/default#hbasehstorebytesperchecksum-toc"},{href:"/docs/configuration/default#hbasehstorechecksumalgorithm-toc"},{href:"https://issues.apache.org/jira/browse/HBASE-5074"},{href:"https://issues.apache.org/jira/browse/HBASE-11355"},{href:"/docs/zookeeper"},{href:"/docs/regionserver-sizing#on-the-number-of-column-families"},{href:"/docs/regionserver-sizing#try-to-minimize-row-and-column-sizes"},{href:"/docs/performance#however"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/TableDescriptorBuilder.html"},{href:"/docs/operational-management/region-and-capacity#determining-region-count-and-size"},{href:"http://en.wikipedia.org/wiki/Bloom_filter"},{href:"https://issues.apache.org/jira/browse/HBASE-1200"},{href:"https://issues.apache.org/jira/browse/HBASE-8450"},{href:"/docs/performance#bloom-filters"},{href:"http://www.quora.com/How-are-bloom-filters-used-in-HBase"},{href:"/docs/performance#when-to-use-bloom-filters"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.html"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.html"},{href:"/docs/architecture/regions#store"},{href:"/docs/architecture/regionserver#architecture-regionserver-block-cache"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.html"},{href:"/docs/compression"},{href:"/docs/regionserver-sizing#try-to-minimize-row-and-column-sizes"},{href:"/docs/architecture/regions#keyvalue"},{href:"/docs/architecture/bulk-loading"},{href:"/docs/regionserver-sizing#relationship-between-rowkeys-and-region-splits"},{href:"/docs/architecture/regions#manual-region-splitting"},{href:"/docs/shell#pre-splitting-tables-with-the-hbase-shell"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/TableDescriptorBuilder.html"},{href:"/docs/architecture/regionserver#disabling-the-wal"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/RegionLocator.html#getRegionLocation(byte%5B%5D)"},{href:"/docs/performance#table-creation-pre-creating-regions"},{href:"/docs/performance#hbase-configurations"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Scan.html"},{href:"/docs/case-studies#case-study-1-performance-issue-on-a-single-node"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ResultScanner.html"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Scan.html"},{href:"/docs/architecture/regionserver#off-heap-block-cache"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Scan.html"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/filter/KeyOnlyFilter.html"},{href:"/docs/performance#table-creation-pre-creating-regions"},{href:"/docs/performance#hbase-configurations"},{href:"http://en.wikipedia.org/wiki/Bloom_filter"},{href:"https://issues.apache.org/jira/browse/HBASE-1200"},{href:"https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf"},{href:"https://issues.apache.org/jira/browse/HBASE-1200"},{href:"http://www.onelab.org"},{href:"/docs/performance#bloom-filters"},{href:"https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf"},{href:"https://issues.apache.org/jira/browse/HDFS-5776"},{href:"https://issues.apache.org/jira/browse/HBASE-17083"},{href:"https://issues.apache.org/jira/browse/HBASE-17083"},{href:"/docs/operational-management/metrics-and-monitoring"},{href:"/docs/datamodel"},{href:"/docs/architecture/regions#compaction"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Admin.html#majorCompact(org.apache.hadoop.hbase.TableName)"},{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html#delete(org.apache.hadoop.hbase.client.Delete)"},{href:"/docs/architecture/hdfs"},{href:"https://issues.apache.org/jira/browse/HDFS-1599"},{href:"https://issues.apache.org/jira/browse/HDFS-2246"},{href:"http://files.meetup.com/1350427/hug_ebay_jdcryans.pdf"},{href:"https://lists.apache.org/thread.html/ce2ce3a3bbd20806d0c017b2e7528e78a46ccb87c063831db051949d%401347548325%40%3Cdev.hbase.apache.org%3E"},{href:"https://issues.apache.org/jira/browse/HDFS-347"},{href:"http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/"},{href:"http://archive.cloudera.com/cdh4/cdh/4/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html"},{href:"https://issues.apache.org/jira/browse/HDFS-347"},{href:"https://issues.apache.org/jira/browse/HDFS-2246"},{href:"https://issues.apache.org/jira/browse/HBASE-8143"},{href:"/docs/architecture/regionserver#architecture-regionserver-block-cache"},{href:"/docs/operational-management/tools#copytable"},{href:"/docs/case-studies"}],c={contents:[{heading:"memory",content:"RAM, RAM, RAM. Don't starve HBase."},{heading:"64-bit",content:"Use a 64-bit platform (and 64-bit JVM)."},{heading:"swapping",content:"Watch out for swapping. Set swappiness to 0."},{heading:"cpu",content:"Make sure you have set up your Hadoop to use native, hardware checksumming. See hadoop.native.lib."},{heading:"performance-network",content:"Perhaps the most important factor in avoiding network issues degrading Hadoop and HBase performance is the switching hardware that is used, decisions made early in the scope of the project can cause major problems when you double or triple the size of your cluster (or more)."},{heading:"performance-network",content:"Important items to consider:"},{heading:"performance-network",content:"Switching capacity of the device"},{heading:"performance-network",content:"Number of systems connected"},{heading:"performance-network",content:"Uplink capacity"},{heading:"single-switch",content:"The single most important factor in this configuration is that the switching capacity of the hardware is capable of handling the traffic which can be generated by all systems connected to the switch. Some lower priced commodity hardware can have a slower switching capacity than could be utilized by a full switch."},{heading:"multiple-switches",content:"Multiple switches are a potential pitfall in the architecture. The most common configuration of lower priced hardware is a simple 1Gbps uplink from one switch to another. This often overlooked pinch point can easily become a bottleneck for cluster communication. Especially with MapReduce jobs that are both reading and writing a lot of data the communication across this uplink could be saturated."},{heading:"multiple-switches",content:"Mitigation of this issue is fairly simple and can be accomplished in multiple ways:"},{heading:"multiple-switches",content:"Use appropriate hardware for the scale of the cluster which you're attempting to build."},{heading:"multiple-switches",content:"Use larger single switch configurations i.e. single 48 port as opposed to 2x 24 port"},{heading:"multiple-switches",content:"Configure port trunking for uplinks to utilize multiple interfaces to increase cross switch bandwidth."},{heading:"multiple-racks",content:"Multiple rack configurations carry the same potential issues as multiple switches, and can suffer performance degradation from two main areas:"},{heading:"multiple-racks",content:"Poor switch capacity performance"},{heading:"multiple-racks",content:"Insufficient uplink to another rack"},{heading:"multiple-racks",content:"If the switches in your rack have appropriate switching capacity to handle all the hosts at full speed, the next most likely issue will be caused by homing more of your cluster across racks. The easiest way to avoid issues when spanning multiple racks is to use port trunking to create a bonded uplink to other racks. The downside of this method however, is in the overhead of ports that could potentially be used. An example of this is, creating an 8Gbps port channel from rack A to rack B, using 8 of your 24 ports to communicate between racks gives you a poor ROI, using too few however can mean you're not getting the most out of your cluster."},{heading:"multiple-racks",content:"Using 10Gbe links between racks will greatly increase performance, and assuming your switches support a 10Gbe uplink or allow for an expansion card will allow you to save your ports for machines as opposed to uplinks."},{heading:"performance-network-interfaces",content:"Are all the network interfaces functioning correctly? Are you sure? See the Troubleshooting Case Study in Case Study #1 (Performance Issue On A Single Node)."},{heading:"network-consistency-and-partition-tolerance",content:"The CAP Theorem states that a distributed system can maintain two out of the following three characteristics:"},{heading:"network-consistency-and-partition-tolerance",content:"Consistency — all nodes see the same data."},{heading:"network-consistency-and-partition-tolerance",content:"Availability — every request receives a response about whether it succeeded or failed."},{heading:"network-consistency-and-partition-tolerance",content:"Partition tolerance — the system continues to operate even if some of its components become unavailable to the others."},{heading:"network-consistency-and-partition-tolerance",content:"HBase favors consistency and partition tolerance, where a decision has to be made. Coda Hale explains why partition tolerance is so important, in http://codahale.com/you-cant-sacrifice-partition-tolerance/."},{heading:"network-consistency-and-partition-tolerance",content:"Robert Yokota used an automated testing framework called Jepson to test HBase's partition tolerance in the face of network partitions, using techniques modeled after Aphyr's Call Me Maybe series. The results, available as a blog post and an addendum, show that HBase performs correctly."},{heading:"long-gc-pauses",content:"In his presentation, Avoiding Full GCs with MemStore-Local Allocation Buffers, Todd Lipcon describes two cases of stop-the-world garbage collections common in HBase, especially during loading; CMS failure modes and old generation heap fragmentation brought."},{heading:"long-gc-pauses",content:"To address the first, start the CMS earlier than default by adding -XX:CMSInitiatingOccupancyFraction and setting it down from defaults. Start at 60 or 70 percent (The lower you bring down the threshold, the more GCing is done, the more CPU used). To address the second fragmentation issue, Todd added an experimental facility, (MSLAB), that must be explicitly enabled in Apache HBase 0.90.x (It's defaulted to be on in Apache 0.92.x HBase). Set hbase.hregion.memstore.mslab.enabled to true in your Configuration. See the cited slides for background and detail. The latest JVMs do better regards fragmentation so make sure you are running a recent release. Read down in the message, Identifying concurrent mode failures caused by fragmentation. Be aware that when enabled, each MemStore instance will occupy at least an MSLAB instance of memory. If you have thousands of regions or lots of regions each with many column families, this allocation of MSLAB may be responsible for a good portion of your heap allocation and in an extreme case cause you to OOME. Disable MSLAB in this case, or lower the amount of memory it uses or float less regions per server."},{heading:"long-gc-pauses",content:"If you have a write-heavy workload, check out HBASE-8163 MemStoreChunkPool: An improvement for JAVA GC when using MSLAB. It describes configurations to lower the amount of young GC during write-heavy loadings. If you do not have HBASE-8163 installed, and you are trying to improve your young GC times, one trick to consider — courtesy of our Liang Xie — is to set the GC config -XX:PretenureSizeThreshold in hbase-env.sh to be just smaller than the size of hbase.hregion.memstore.mslab.chunksize so MSLAB allocations happen in the tenured space directly rather than first in the young gen. You'd do this because these MSLAB allocations are going to likely make it to the old gen anyways and rather than pay the price of a copies between s0 and s1 in eden space followed by the copy up from young to old gen after the MSLABs have achieved sufficient tenure, save a bit of YGC churn and allocate in the old gen directly."},{heading:"long-gc-pauses",content:"Other sources of long GCs can be the JVM itself logging. See Eliminating Large JVM GC Pauses Caused by Background IO Traffic"},{heading:"long-gc-pauses",content:"For more information about GC logs, see JVM Garbage Collection Logs."},{heading:"long-gc-pauses",content:"Consider also enabling the off-heap Block Cache. This has been shown to mitigate GC pause times. See Block Cache"},{heading:"hbase-configurations",content:"See Recommended Configurations."},{heading:"improving-the-99th-percentile",content:"Try hedged_reads."},{heading:"managing-compactions",content:"For larger systems, managing compactions and splits may be something you want to consider."},{heading:"performance-hbaseregionserverhandlercount",content:"See hbase.regionserver.handler.count."},{heading:"hfileblockcachesize",content:"See hfile.block.cache.size. A memory setting for the RegionServer process."},{heading:"prefetch-option-for-blockcache",content:"HBASE-9857 adds a new option to prefetch HFile contents when opening the BlockCache, if a Column family or RegionServer property is set. This option is available for HBase 0.98.3 and later. The purpose is to warm the BlockCache as rapidly as possible after the cache is opened, using in-memory table data, and not counting the prefetching as cache misses. This is great for fast reads, but is not a good idea if the data to be preloaded will not fit into the BlockCache. It is useful for tuning the IO impact of prefetching versus the time before all data blocks are in cache."},{heading:"prefetch-option-for-blockcache",content:"To enable prefetching on a given column family, you can use HBase Shell or use the API."},{heading:"prefetch-option-for-blockcache",content:"Enable Prefetch Using HBase Shell"},{heading:"prefetch-option-for-blockcache",content:"Enable Prefetch Using the API"},{heading:"prefetch-option-for-blockcache",content:"See the API documentation for CacheConfig."},{heading:"prefetch-option-for-blockcache",content:"To see prefetch in operation, enable TRACE level logging on org.apache.hadoop.hbase.io.hfile.HFileReaderImpl in hbase-2.0+ or on org.apache.hadoop.hbase.io.hfile.HFileReaderV2 in earlier versions, hbase-1.x, of HBase."},{heading:"hbaseregionserverglobalmemstoresize",content:"See hbase.regionserver.global.memstore.size. This memory setting is often adjusted for the RegionServer process depending on needs."},{heading:"hbaseregionserverglobalmemstoresizelowerlimit",content:"See hbase.regionserver.global.memstore.size.lower.limit. This memory setting is often adjusted for the RegionServer process depending on needs."},{heading:"hbasehstoreblockingstorefiles",content:"See hbase.hstore.blockingStoreFiles. If there is blocking in the RegionServer logs, increasing this can help."},{heading:"hbasehregionmemstoreblockmultiplier",content:"See hbase.hregion.memstore.block.multiplier. If there is enough RAM, increasing this can help."},{heading:"hbaseregionserverchecksumverify",content:"Have HBase write the checksum into the datablock and save having to do the checksum seek whenever you read."},{heading:"hbaseregionserverchecksumverify",content:"See hbase.regionserver.checksum.verify, hbase.hstore.bytes.per.checksum and hbase.hstore.checksum.algorithm. For more information see the release note on HBASE-5074 support checksums in HBase block cache."},{heading:"tuning-callqueue-options",content:"HBASE-11355 introduces several callQueue tuning mechanisms which can increase performance. See the JIRA for some benchmarking information."},{heading:"tuning-callqueue-options",content:"To increase the number of callqueues, set hbase.ipc.server.num.callqueue to a value greater than 1. To split the callqueue into separate read and write queues, set hbase.ipc.server.callqueue.read.ratio to a value between 0 and 1. This factor weights the queues toward writes (if below .5) or reads (if above .5). Another way to say this is that the factor determines what percentage of the split queues are used for reads. The following examples illustrate some of the possibilities. Note that you always have at least one write queue, no matter what setting you use."},{heading:"tuning-callqueue-options",content:"The default value of 0 does not split the queue."},{heading:"tuning-callqueue-options",content:"A value of .3 uses 30% of the queues for reading and 70% for writing. Given a value of 10 for hbase.ipc.server.num.callqueue, 3 queues would be used for reads and 7 for writes."},{heading:"tuning-callqueue-options",content:"A value of .5 uses the same number of read queues and write queues. Given a value of 10 for hbase.ipc.server.num.callqueue, 5 queues would be used for reads and 5 for writes."},{heading:"tuning-callqueue-options",content:"A value of .6 uses 60% of the queues for reading and 40% for writing. Given a value of 10 for hbase.ipc.server.num.callqueue, 6 queues would be used for reads and 4 for writes."},{heading:"tuning-callqueue-options",content:"A value of 1.0 uses one queue to process write requests, and all other queues process read requests. A value higher than 1.0 has the same effect as a value of 1.0. Given a value of 10 for hbase.ipc.server.num.callqueue, 9 queues would be used for reads and 1 for writes."},{heading:"tuning-callqueue-options",content:"You can also split the read queues so that separate queues are used for short reads (from Get operations) and long reads (from Scan operations), by setting the hbase.ipc.server.callqueue.scan.ratio option. This option is a factor between 0 and 1, which determine the ratio of read queues used for Gets and Scans. More queues are used for Gets if the value is below .5 and more are used for scans if the value is above .5. No matter what setting you use, at least one read queue is used for Get operations."},{heading:"tuning-callqueue-options",content:"A value of 0 does not split the read queue."},{heading:"tuning-callqueue-options",content:"A value of .3 uses 70% of the read queues for Gets and 30% for Scans. Given a value of 20 for hbase.ipc.server.num.callqueue and a value of .5 for hbase.ipc.server.callqueue.read.ratio, 10 queues would be used for reads, out of those 10, 7 would be used for Gets and 3 for Scans."},{heading:"tuning-callqueue-options",content:"A value of .5 uses half the read queues for Gets and half for Scans. Given a value of 20 for hbase.ipc.server.num.callqueue and a value of .5 for hbase.ipc.server.callqueue.read.ratio, 10 queues would be used for reads, out of those 10, 5 would be used for Gets and 5 for Scans."},{heading:"tuning-callqueue-options",content:"A value of .7 uses 30% of the read queues for Gets and 70% for Scans. Given a value of 20 for hbase.ipc.server.num.callqueue and a value of .5 for hbase.ipc.server.callqueue.read.ratio, 10 queues would be used for reads, out of those 10, 3 would be used for Gets and 7 for Scans."},{heading:"tuning-callqueue-options",content:"A value of 1.0 uses all but one of the read queues for Scans. Given a value of 20 for hbase.ipc.server.num.callqueue and a value of .5 for hbase.ipc.server.callqueue.read.ratio, 10 queues would be used for reads, out of those 10, 1 would be used for Gets and 9 for Scans."},{heading:"tuning-callqueue-options",content:"You can use the new option hbase.ipc.server.callqueue.handler.factor to programmatically tune the number of queues:"},{heading:"tuning-callqueue-options",content:"A value of 0 uses a single shared queue between all the handlers."},{heading:"tuning-callqueue-options",content:"A value of 1 uses a separate queue for each handler."},{heading:"tuning-callqueue-options",content:"A value between 0 and 1 tunes the number of queues against the number of handlers. For instance, a value of .5 shares one queue between each two handlers.Having more queues, such as in a situation where you have one queue per handler, reduces contention when adding a task to a queue or selecting it from a queue. The trade-off is that if you have some queues with long-running tasks, a handler may end up waiting to execute from that queue rather than processing another queue which has waiting tasks."},{heading:"tuning-callqueue-options",content:"For these values to take effect on a given RegionServer, the RegionServer must be restarted. These parameters are intended for testing purposes and should be used carefully."},{heading:"performance-zookeeper",content:"See ZooKeeper for information on configuring ZooKeeper, and see the part about having a dedicated disk."},{heading:"number-of-column-families",content:"See On the number of column families."},{heading:"key-and-attribute-lengths",content:"See Try to minimize row and column sizes. See also However... for compression caveats."},{heading:"table-regionsize",content:"The regionsize can be set on a per-table basis via setMaxFileSize on TableDescriptorBuilder in the event where certain tables require different regionsizes than the configured default regionsize."},{heading:"table-regionsize",content:"See Determining region count and size for more information."},{heading:"bloom-filters",content:'A Bloom filter, named for its creator, Burton Howard Bloom, is a data structure which is designed to predict whether a given element is a member of a set of data. A positive result from a Bloom filter is not always accurate, but a negative result is guaranteed to be accurate. Bloom filters are designed to be "accurate enough" for sets of data which are so large that conventional hashing mechanisms would be impractical. For more information about Bloom filters in general, refer to http://en.wikipedia.org/wiki/Bloom_filter.'},{heading:"bloom-filters",content:"In terms of HBase, Bloom filters provide a lightweight in-memory structure to reduce the number of disk reads for a given Get operation (Bloom filters do not work with Scans) to only the StoreFiles likely to contain the desired Row. The potential performance gain increases with the number of parallel reads."},{heading:"bloom-filters",content:"The Bloom filters themselves are stored in the metadata of each HFile and never need to be updated. When an HFile is opened because a region is deployed to a RegionServer, the Bloom filter is loaded into memory."},{heading:"bloom-filters",content:"HBase includes some tuning mechanisms for folding the Bloom filter to reduce the size and keep the false positive rate within a desired range."},{heading:"bloom-filters",content:"Bloom filters were introduced in HBASE-1200. Since HBase 0.96, row-based Bloom filters are enabled by default. (HBASE-8450)"},{heading:"bloom-filters",content:"For more information on Bloom filters in relation to HBase, see Bloom Filters for more information, or the following Quora discussion: How are bloom filters used in HBase?."},{heading:"when-to-use-bloom-filters",content:"Since HBase 0.96, row-based Bloom filters are enabled by default. You may choose to disable them or to change some tables to use row+column Bloom filters, depending on the characteristics of your data and how it is loaded into HBase."},{heading:"when-to-use-bloom-filters",content:"To determine whether Bloom filters could have a positive impact, check the value of blockCacheHitRatio in the RegionServer metrics. If Bloom filters are enabled, the value of blockCacheHitRatio should increase, because the Bloom filter is filtering out blocks that are definitely not needed."},{heading:"when-to-use-bloom-filters",content:"You can choose to enable Bloom filters for a row or for a row+column combination. If you generally scan entire rows, the row+column combination will not provide any benefit. A row-based Bloom filter can operate on a row+column Get, but not the other way around. However, if you have a large number of column-level Puts, such that a row may be present in every StoreFile, a row-based filter will always return a positive result and provide no benefit. Unless you have one column per row, row+column Bloom filters require more space, in order to store more keys. Bloom filters work best when the size of each data entry is at least a few kilobytes in size."},{heading:"when-to-use-bloom-filters",content:"Overhead will be reduced when your data is stored in a few larger StoreFiles, to avoid extra disk IO during low-level scans to find a specific row."},{heading:"when-to-use-bloom-filters",content:"Bloom filters need to be rebuilt upon deletion, so may not be appropriate in environments with a large number of deletions."},{heading:"enabling-bloom-filters",content:"Bloom filters are enabled on a Column Family. You can do this by using the setBloomFilterType method of HColumnDescriptor or using the HBase API. Valid values are NONE, ROW (default), or ROWCOL. See When To Use Bloom Filters for more information on ROW versus ROWCOL. See also the API documentation for ColumnFamilyDescriptorBuilder."},{heading:"enabling-bloom-filters",content:"The following example creates a table and enables a ROWCOL Bloom filter on the colfam1 column family."},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"You can configure the following settings in the hbase-site.xml."},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"Parameter"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"Default"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"Description"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"io.storefile.bloom.enabled"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"yes"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"Set to no to kill bloom filters server-wide if something goes wrong"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"io.storefile.bloom.error.rate"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:".01"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"The average false positive rate for bloom filters. Folding is used to maintain the false positive rate. Expressed as a decimal representation of a percentage."},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"io.storefile.bloom.max.fold"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"7"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"The guaranteed maximum fold rate. Changing this setting should not be necessary and is not recommended."},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"io.storefile.bloom.max.keys"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"128000000"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"For default (single-block) Bloom filters, this specifies the maximum number of keys."},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"io.storefile.delete.family.bloom.enabled"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"true"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"Master switch to enable Delete Family Bloom filters and store them in the StoreFile."},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"io.storefile.bloom.block.size"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"131072"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"Target Bloom block size. Bloom filter blocks of approximately this size are interleaved with data blocks."},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"hfile.block.bloom.cacheonwrite"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"false"},{heading:"configuring-server-wide-behavior-of-bloom-filters",content:"Enables cache-on-write for inline blocks of a compound Bloom filter."},{heading:"columnfamily-blocksize",content:"The blocksize can be configured for each ColumnFamily in a table, and defaults to 64k. Larger cell values require larger blocksizes. There is an inverse relationship between blocksize and the resulting StoreFile indexes (i.e., if the blocksize is doubled then the resulting indexes should be roughly halved)."},{heading:"columnfamily-blocksize",content:"See ColumnFamilyDescriptorBuilder and Store for more information."},{heading:"in-memory-columnfamilies",content:"ColumnFamilies can optionally be defined as in-memory. Data is still persisted to disk, just like any other ColumnFamily. In-memory blocks have the highest priority in the Block Cache, but it is not a guarantee that the entire table will be in memory."},{heading:"in-memory-columnfamilies",content:"See ColumnFamilyDescriptorBuilder for more information."},{heading:"performance-schema-design-compression",content:"Production systems should use compression with their ColumnFamily definitions. See Compression and Data Block Encoding In HBase for more information."},{heading:"however",content:"Compression deflates data on disk. When it's in-memory (e.g., in the MemStore) or on the wire (e.g., transferring between RegionServer and Client) it's inflated. So while using ColumnFamily compression is a best practice, but it's not going to completely eliminate the impact of over-sized Keys, over-sized ColumnFamily names, or over-sized Column names."},{heading:"however",content:"See Try to minimize row and column sizes on for schema design tips, and KeyValue for more information on HBase stores data internally."},{heading:"constants",content:"When people get started with HBase they have a tendency to write code that looks like this:"},{heading:"constants",content:"But especially when inside loops (and MapReduce jobs), converting the columnFamily and column-names to byte-arrays repeatedly is surprisingly expensive. It's better to use constants for the byte-arrays, like this:"},{heading:"batch-loading",content:"Use the bulk load tool if you can. See Bulk Loading. Otherwise, pay attention to the below."},{heading:"table-creation-pre-creating-regions",content:"Tables in HBase are initially created with one region by default. For bulk imports, this means that all clients will write to the same region until it is large enough to split and become distributed across the cluster. A useful pattern to speed up the bulk import process is to pre-create empty regions. Be somewhat conservative in this, because too-many regions can actually degrade performance."},{heading:"table-creation-pre-creating-regions",content:"There are two different approaches to pre-creating splits using the HBase API. The first approach is to rely on the default Admin strategy (which is implemented in Bytes.split)..."},{heading:"table-creation-pre-creating-regions",content:"And the other approach, using the HBase API, is to define the splits yourself..."},{heading:"table-creation-pre-creating-regions",content:"You can achieve a similar effect using the HBase Shell to create tables by specifying split options."},{heading:"table-creation-pre-creating-regions",content:"See Relationship Between RowKeys and Region Splits for issues related to understanding your keyspace and pre-creating regions. See manual region splitting decisions for discussion on manually pre-splitting regions. See Pre-splitting tables with the HBase Shell for more details of using the HBase Shell to pre-split tables."},{heading:"table-creation-deferred-log-flush",content:"The default behavior for Puts using the Write Ahead Log (WAL) is that WAL edits will be written immediately. If deferred log flush is used, WAL edits are kept in memory until the flush period. The benefit is aggregated and asynchronous WAL- writes, but the potential downside is that if the RegionServer goes down the yet-to-be-flushed edits are lost. This is safer, however, than not using WAL at all with Puts."},{heading:"table-creation-deferred-log-flush",content:"Deferred log flush can be configured on tables via TableDescriptorBuilder. The default value of hbase.regionserver.optionallogflushinterval is 1000ms."},{heading:"hbase-client-turn-off-wal-on-puts",content:"A frequent request is to disable the WAL to increase performance of Puts. This is only appropriate for bulk loads, as it puts your data at risk by removing the protection of the WAL in the event of a region server crash. Bulk loads can be re-run in the event of a crash, with little risk of data loss."},{heading:"hbase-client-turn-off-wal-on-puts",content:"type: warn"},{heading:"hbase-client-turn-off-wal-on-puts",content:"If you disable the WAL for anything other than bulk loads, your data is at risk."},{heading:"hbase-client-turn-off-wal-on-puts",content:"In general, it is best to use WAL for Puts, and where loading throughput is a concern to use bulk loading techniques instead. For normal Puts, you are not likely to see a performance improvement which would outweigh the risk. To disable the WAL, see Disabling the WAL."},{heading:"hbase-client-group-puts-by-regionserver",content:"In addition to using the writeBuffer, grouping Puts by RegionServer can reduce the number of client RPC calls per writeBuffer flush. There is a utility HTableUtil currently on MASTER that does this, but you can either copy that or implement your own version for those still on 0.90.x or earlier."},{heading:"mapreduce-skip-the-reducer",content:"When writing a lot of data to an HBase table from a MR job (e.g., with TableOutputFormat), and specifically where Puts are being emitted from the Mapper, skip the Reducer step. When a Reducer step is used, all of the output (Puts) from the Mapper will get spooled to disk, then sorted/shuffled to other Reducers that will most likely be off-node. It's far more efficient to just write directly to HBase."},{heading:"mapreduce-skip-the-reducer",content:"For summary jobs where HBase is used as a source and a sink, then writes will be coming from the Reducer step (e.g., summarize values then write out result). This is a different processing problem than from the above case."},{heading:"anti-pattern-one-hot-region",content:"If all your data is being written to one region at a time, then re-read the section on processing timeseries data."},{heading:"anti-pattern-one-hot-region",content:`Also, if you are pre-splitting regions and all your data is still winding up in a single region even though your keys aren't monotonically increasing, confirm that your keyspace actually works with the split strategy. There are a variety of reasons that regions may appear "well split" but won't work with your data. As the HBase client communicates directly with the RegionServers, this can be obtained via RegionLocator.getRegionLocation.`},{heading:"anti-pattern-one-hot-region",content:"See Table Creation: Pre-Creating Regions, as well as HBase Configurations"},{heading:"reading-from-hbase",content:"The mailing list can help if you are having performance issues."},{heading:"scan-caching",content:"If HBase is used as an input source for a MapReduce job, for example, make sure that the input Scan instance to the MapReduce job has setCaching set to something greater than the default (which is 1). Using the default value means that the map-task will make call back to the region-server for every record processed. Setting this value to 500, for example, will transfer 500 rows at a time to the client to be processed. There is a cost/benefit to have the cache value be large because it costs more in memory for both client and RegionServer, so bigger isn't always better."},{heading:"scan-caching-in-mapreduce-jobs",content:"Scan settings in MapReduce jobs deserve special attention. Timeouts can result (e.g., UnknownScannerException) in Map tasks if it takes longer to process a batch of records before the client goes back to the RegionServer for the next set of data. This problem can occur because there is non-trivial processing occurring per row. If you process rows quickly, set caching higher. If you process rows more slowly (e.g., lots of transformations per row, writes), then set caching lower."},{heading:"scan-caching-in-mapreduce-jobs",content:"Timeouts can also happen in a non-MapReduce use case (i.e., single threaded HBase client doing a Scan), but the processing that is often performed in MapReduce jobs tends to exacerbate this issue."},{heading:"scan-attribute-selection",content:"Whenever a Scan is used to process large numbers of rows (and especially when used as a MapReduce source), be aware of which attributes are selected. If scan.addFamily is called then all of the attributes in the specified ColumnFamily will be returned to the client. If only a small number of the available attributes are to be processed, then only those attributes should be specified in the input scan because attribute over-selection is a non-trivial performance penalty over large datasets."},{heading:"avoid-scan-seeks",content:"When columns are selected explicitly with scan.addColumn, HBase will schedule seek operations to seek between the selected columns. When rows have few columns and each column has only a few versions this can be inefficient. A seek operation is generally slower if does not seek at least past 5-10 columns/versions or 512-1024 bytes."},{heading:"avoid-scan-seeks",content:"In order to opportunistically look ahead a few columns/versions to see if the next column/version can be found that way before a seek operation is scheduled, a new attribute Scan.HINT_LOOKAHEAD can be set on the Scan object. The following code instructs the RegionServer to attempt two iterations of next before a seek is scheduled:"},{heading:"mapreduce---input-splits",content:'For MapReduce jobs that use HBase tables as a source, if there a pattern where the "slow" map tasks seem to have the same Input Split (i.e., the RegionServer serving the data), see the Troubleshooting Case Study in Case Study #1 (Performance Issue On A Single Node).'},{heading:"close-resultscanners",content:"This isn't so much about improving performance but rather avoiding performance problems. If you forget to close ResultScanners you can cause problems on the RegionServers. Always have ResultScanner processing enclosed in try/catch blocks."},{heading:"performance-reading-from-hbase-block-cache",content:"Scan instances can be set to use the block cache in the RegionServer via the setCacheBlocks method. For input Scans to MapReduce jobs, this should be false. For frequently accessed rows, it is advisable to use the block cache."},{heading:"performance-reading-from-hbase-block-cache",content:"Cache more data by moving your Block Cache off-heap. See Off-heap Block Cache"},{heading:"optimal-loading-of-row-keys",content:"When performing a table scan where only the row keys are needed (no families, qualifiers, values or timestamps), add a FilterList with a MUST_PASS_ALL operator to the scanner using setFilter. The filter list should include both a FirstKeyOnlyFilter and a KeyOnlyFilter. Using this filter combination will result in a worst case scenario of a RegionServer reading a single value from disk and minimal network traffic to the client for a single row."},{heading:"concurrency-monitor-data-spread",content:"When performing a high number of concurrent reads, monitor the data spread of the target tables. If the target table(s) have too few regions then the reads could likely be served from too few nodes."},{heading:"concurrency-monitor-data-spread",content:"See Table Creation: Pre-Creating Regions, as well as HBase Configurations"},{heading:"bloom-filters-1",content:"Enabling Bloom Filters can save your having to go to disk and can help improve read latencies."},{heading:"bloom-filters-1",content:"Bloom filters were developed over in HBase-1200 Add bloomfilters. For description of the development process — why static blooms rather than dynamic — and for an overview of the unique properties that pertain to blooms in HBase, as well as possible future directions, see the Development Process section of the document BloomFilters in HBase attached to HBASE-1200. The bloom filters described here are actually version two of blooms in HBase. In versions up to 0.19.x, HBase had a dynamic bloom option based on work done by the European Commission One-Lab Project 034819. The core of the HBase bloom work was later pulled up into Hadoop to implement org.apache.hadoop.io.BloomMapFile. Version 1 of HBase blooms never worked that well. Version 2 is a rewrite from scratch though again it starts with the one-lab work."},{heading:"bloom-filters-1",content:"See also Bloom Filters."},{heading:"bloom-storefile-footprint",content:"Bloom filters add an entry to the StoreFile general FileInfo data structure and then two extra entries to the StoreFile metadata section."},{heading:"bloom-storefile-footprint",content:"BloomFilter in the StoreFile FileInfo data structureFileInfo has a BLOOM_FILTER_TYPE entry which is set to NONE, ROW or ROWCOL."},{heading:"bloom-storefile-footprint",content:"BloomFilter entries in StoreFile metadataBLOOM_FILTER_META holds Bloom Size, Hash Function used, etc. It's small in size and is cached on StoreFile.Reader load.BLOOM_FILTER_DATA is the actual bloomfilter data. Obtained on-demand. Stored in the LRU cache, if it is enabled (It's enabled by default)."},{heading:"bloom-filter-configuration",content:"io.storefile.bloom.enabled global kill switchio.storefile.bloom.enabled in Configuration serves as the kill switch in case something goes wrong. Default = true."},{heading:"bloom-filter-configuration",content:"io.storefile.bloom.error.rateio.storefile.bloom.error.rate = average false positive rate. Default = 1%. Decrease rate by ½ (e.g. to .5%) == +1 bit per bloom entry."},{heading:"bloom-filter-configuration",content:"io.storefile.bloom.max.foldio.storefile.bloom.max.fold = guaranteed minimum fold rate. Most people should leave this alone. Default = 7, or can collapse to at least 1/128th of original size. See the Development Process section of the document BloomFilters in HBase for more on what this option means."},{heading:"hedged-reads",content:"Hedged reads are a feature of HDFS, introduced in Hadoop 2.4.0 with HDFS-5776. Normally, a single thread is spawned for each read request. However, if hedged reads are enabled, the client waits some configurable amount of time, and if the read does not return, the client spawns a second read request, against a different block replica of the same data. Whichever read returns first is used, and the other read request is discarded."},{heading:"hedged-reads",content:'Hedged reads are "...very good at eliminating outlier datanodes, which in turn makes them very good choice for latency sensitive setups. But, if you are looking for maximizing throughput, hedged reads tend to create load amplification as things get slower in general. In short, the thing to watch out for is the non-graceful performance degradation when you are running close a certain throughput threshold." (Quote from Ashu Pachauri in HBASE-17083).'},{heading:"hedged-reads",content:"Other concerns to keep in mind while running with hedged reads enabled include:"},{heading:"hedged-reads",content:"They may lead to network congestion. See HBASE-17083"},{heading:"hedged-reads",content:"Make sure you set the thread pool large enough so as blocking on the pool does not become a bottleneck (Again see HBASE-17083)"},{heading:"hedged-reads",content:"(From Yu Li up in HBASE-17083)"},{heading:"hedged-reads",content:"Because an HBase RegionServer is a HDFS client, you can enable hedged reads in HBase, by adding the following properties to the RegionServer's hbase-site.xml and tuning the values to suit your environment."},{heading:"hedged-reads",content:"Configuration for Hedged Reads"},{heading:"hedged-reads",content:"dfs.client.hedged.read.threadpool.size - the number of threads dedicated to servicing hedged reads. If this is set to 0 (the default), hedged reads are disabled."},{heading:"hedged-reads",content:"dfs.client.hedged.read.threshold.millis - the number of milliseconds to wait before spawning a second read thread."},{heading:"hedged-reads",content:"Hedged Reads Configuration Example"},{heading:"hedged-reads",content:"Use the following metrics to tune the settings for hedged reads on your cluster. See HBase Metrics for more information."},{heading:"hedged-reads",content:"Metrics for Hedged Reads"},{heading:"hedged-reads",content:"hedgedReadOps - the number of times hedged read threads have been triggered. This could indicate that read requests are often slow, or that hedged reads are triggered too quickly."},{heading:"hedged-reads",content:"hedgeReadOpsWin - the number of times the hedged read thread was faster than the original thread. This could indicate that a given RegionServer is having trouble servicing requests."},{heading:"hedged-reads",content:"hedgedReadOpsInCurThread - the number of times hedged read was rejected from executor and needed to fallback to be executed in current thread. This could indicate that current hedged read thread pool size is not appropriate."},{heading:"using-hbase-tables-as-queues",content:"HBase tables are sometimes used as queues. In this case, special care must be taken to regularly perform major compactions on tables used in this manner. As is documented in Data Model, marking rows as deleted creates additional StoreFiles which then need to be processed on reads. Tombstones only get cleaned up with major compactions."},{heading:"using-hbase-tables-as-queues",content:"See also Compaction and Admin.majorCompact."},{heading:"delete-rpc-behavior",content:"Be aware that Table.delete(Delete) doesn't use the writeBuffer. It will execute an RegionServer RPC with each invocation. For a large number of deletes, consider Table.delete(List)."},{heading:"delete-rpc-behavior",content:"See hbase.client.Delete"},{heading:"hdfs",content:"Because HBase runs on HDFS it is important to understand how it works and how it affects HBase."},{heading:"current-issues-with-low-latency-reads",content:"The original use-case for HDFS was batch processing. As such, there low-latency reads were historically not a priority. With the increased adoption of Apache HBase this is changing, and several improvements are already in development. See the Umbrella Jira Ticket for HDFS Improvements for HBase."},{heading:"leveraging-local-data",content:`Since Hadoop 1.0.0 (also 0.22.1, 0.23.1, CDH3u3 and HDP 1.0) via HDFS-2246, it is possible for the DFSClient to take a "short circuit" and read directly from the disk instead of going through the DataNode when the data is local. What this means for HBase is that the RegionServers can read directly off their machine's disks instead of having to open a socket to talk to the DataNode, the former being generally much faster. See JD's Performance Talk. Also see HBase, mail # dev - read short circuit thread for more discussion around short circuit reads.`},{heading:"leveraging-local-data",content:'To enable "short circuit" reads, it will depend on your version of Hadoop. The original shortcircuit read patch was much improved upon in Hadoop 2 in HDFS-347. See http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/ for details on the difference between the old and new implementations. See Hadoop shortcircuit reads configuration page for how to enable the latter, better version of shortcircuit. For example, here is a minimal config. enabling short-circuit reads added to hbase-site.xml:'},{heading:"leveraging-local-data",content:"Be careful about permissions for the directory that hosts the shared domain socket; dfsclient will complain if open to other than the hbase user."},{heading:"leveraging-local-data",content:"If you are running on an old Hadoop, one that is without HDFS-347 but that has HDFS-2246, you must set two configurations. First, the hdfs-site.xml needs to be amended. Set the property dfs.block.local-path-access.user to be the only user that can use the shortcut. This has to be the user that started HBase. Then in hbase-site.xml, set dfs.client.read.shortcircuit to be true"},{heading:"leveraging-local-data",content:"Services — at least the HBase RegionServers — will need to be restarted in order to pick up the new configurations."},{heading:"leveraging-local-data",content:"type: info"},{heading:"leveraging-local-data",content:"title: dfs.client.read.shortcircuit.buffer.size"},{heading:"leveraging-local-data",content:`The default for this value is too high when running on a highly trafficked HBase. In HBase, if
this value has not been set, we set it down from the default of 1M to 128k (Since HBase 0.98.0 and
0.96.1). See HBASE-8143 HBase on Hadoop 2 with local short circuit reads (ssr) causes
OOM. The Hadoop DFSClient in HBase will
allocate a direct byte buffer of this size for each block it has open; given HBase keeps its
HDFS files open all the time, this can add up quickly.`},{heading:"performance-comparisons-of-hbase-vs-hdfs",content:"A fairly common question on the dist-list is why HBase isn't as performant as HDFS files in a batch context (e.g., as a MapReduce source or sink). The short answer is that HBase is doing a lot more than HDFS (e.g., reading the KeyValues, returning the most current row or specified timestamps, etc.), and as such HBase is 4-5 times slower than HDFS in this processing context. There is room for improvement and this gap will, over time, be reduced, but HDFS will always be faster in this use-case."},{heading:"performance-amazon-ec2",content:"Performance questions are common on Amazon EC2 environments because it is a shared environment. You will not see the same throughput as a dedicated server. In terms of running tests on EC2, run them several times for the same reason (i.e., it's a shared environment and you don't know what else is happening on the server)."},{heading:"performance-amazon-ec2",content:"If you are running on EC2 and post performance questions on the dist-list, please state this fact up-front that because EC2 issues are practically a separate class of performance issues."},{heading:"collocating-hbase-and-mapreduce",content:"It is often recommended to have different clusters for HBase and MapReduce. A better qualification of this is: don't collocate an HBase that serves live requests with a heavy MR workload. OLTP and OLAP-optimized systems have conflicting requirements and one will lose to the other, usually the former. For example, short latency-sensitive disk reads will have to wait in line behind longer reads that are trying to squeeze out as much throughput as possible. MR jobs that write to HBase will also generate flushes and compactions, which will in turn invalidate blocks in the Block Cache."},{heading:"collocating-hbase-and-mapreduce",content:"If you need to process the data from your live HBase cluster in MR, you can ship the deltas with CopyTable or use replication to get the new data in real time on the OLAP cluster. In the worst case, if you really need to collocate both, set MR to use less Map and Reduce slots than you'd normally configure, possibly just one."},{heading:"collocating-hbase-and-mapreduce",content:"When HBase is used for OLAP operations, it's preferable to set it up in a hardened way like configuring the ZooKeeper session timeout higher and giving more memory to the MemStores (the argument being that the Block Cache won't be used much since the workloads are usually long scans)."},{heading:"performance-case-studies",content:"For Performance and Troubleshooting Case Studies, see Apache HBase Case Studies."}],headings:[{id:"operating-system",content:"Operating System"},{id:"memory",content:"Memory"},{id:"64-bit",content:"64-bit"},{id:"swapping",content:"Swapping"},{id:"cpu",content:"CPU"},{id:"performance-network",content:"Network"},{id:"single-switch",content:"Single Switch"},{id:"multiple-switches",content:"Multiple Switches"},{id:"multiple-racks",content:"Multiple Racks"},{id:"performance-network-interfaces",content:"Network Interfaces"},{id:"network-consistency-and-partition-tolerance",content:"Network Consistency and Partition Tolerance"},{id:"performance-java",content:"Java"},{id:"the-garbage-collector-and-apache-hbase",content:"The Garbage Collector and Apache HBase"},{id:"long-gc-pauses",content:"Long GC pauses"},{id:"hbase-configurations",content:"HBase Configurations"},{id:"improving-the-99th-percentile",content:"Improving the 99th Percentile"},{id:"managing-compactions",content:"Managing Compactions"},{id:"performance-hbaseregionserverhandlercount",content:"hbase.regionserver.handler.count"},{id:"hfileblockcachesize",content:"hfile.block.cache.size"},{id:"prefetch-option-for-blockcache",content:"Prefetch Option for Blockcache"},{id:"hbaseregionserverglobalmemstoresize",content:"hbase.regionserver.global.memstore.size"},{id:"hbaseregionserverglobalmemstoresizelowerlimit",content:"hbase.regionserver.global.memstore.size.lower.limit"},{id:"hbasehstoreblockingstorefiles",content:"hbase.hstore.blockingStoreFiles"},{id:"hbasehregionmemstoreblockmultiplier",content:"hbase.hregion.memstore.block.multiplier"},{id:"hbaseregionserverchecksumverify",content:"hbase.regionserver.checksum.verify"},{id:"tuning-callqueue-options",content:"Tuning callQueue Options"},{id:"performance-zookeeper",content:"ZooKeeper"},{id:"performance-schema-design",content:"Schema Design"},{id:"number-of-column-families",content:"Number of Column Families"},{id:"key-and-attribute-lengths",content:"Key and Attribute Lengths"},{id:"table-regionsize",content:"Table RegionSize"},{id:"bloom-filters",content:"Bloom Filters"},{id:"when-to-use-bloom-filters",content:"When To Use Bloom Filters"},{id:"enabling-bloom-filters",content:"Enabling Bloom Filters"},{id:"configuring-server-wide-behavior-of-bloom-filters",content:"Configuring Server-Wide Behavior of Bloom Filters"},{id:"columnfamily-blocksize",content:"ColumnFamily BlockSize"},{id:"in-memory-columnfamilies",content:"In-Memory ColumnFamilies"},{id:"performance-schema-design-compression",content:"Compression"},{id:"however",content:"However..."},{id:"hbase-general-patterns",content:"HBase General Patterns"},{id:"constants",content:"Constants"},{id:"writing-to-hbase",content:"Writing to HBase"},{id:"batch-loading",content:"Batch Loading"},{id:"table-creation-pre-creating-regions",content:"Table Creation: Pre-Creating Regions"},{id:"table-creation-deferred-log-flush",content:"Table Creation: Deferred Log Flush"},{id:"hbase-client-turn-off-wal-on-puts",content:"HBase Client: Turn off WAL on Puts"},{id:"hbase-client-group-puts-by-regionserver",content:"HBase Client: Group Puts by RegionServer"},{id:"mapreduce-skip-the-reducer",content:"MapReduce: Skip The Reducer"},{id:"anti-pattern-one-hot-region",content:"Anti-Pattern: One Hot Region"},{id:"reading-from-hbase",content:"Reading from HBase"},{id:"scan-caching",content:"Scan Caching"},{id:"scan-caching-in-mapreduce-jobs",content:"Scan Caching in MapReduce Jobs"},{id:"scan-attribute-selection",content:"Scan Attribute Selection"},{id:"avoid-scan-seeks",content:"Avoid scan seeks"},{id:"mapreduce---input-splits",content:"MapReduce - Input Splits"},{id:"close-resultscanners",content:"Close ResultScanners"},{id:"performance-reading-from-hbase-block-cache",content:"Block Cache"},{id:"optimal-loading-of-row-keys",content:"Optimal Loading of Row Keys"},{id:"concurrency-monitor-data-spread",content:"Concurrency: Monitor Data Spread"},{id:"bloom-filters-1",content:"Bloom Filters"},{id:"bloom-storefile-footprint",content:"Bloom StoreFile footprint"},{id:"bloom-filter-configuration",content:"Bloom Filter Configuration"},{id:"hedged-reads",content:"Hedged Reads"},{id:"deleting-from-hbase",content:"Deleting from HBase"},{id:"using-hbase-tables-as-queues",content:"Using HBase Tables as Queues"},{id:"delete-rpc-behavior",content:"Delete RPC Behavior"},{id:"hdfs",content:"HDFS"},{id:"current-issues-with-low-latency-reads",content:"Current Issues With Low-Latency Reads"},{id:"leveraging-local-data",content:"Leveraging local data"},{id:"performance-comparisons-of-hbase-vs-hdfs",content:"Performance Comparisons of HBase vs. HDFS"},{id:"performance-amazon-ec2",content:"Amazon EC2"},{id:"collocating-hbase-and-mapreduce",content:"Collocating HBase and MapReduce"},{id:"performance-case-studies",content:"Case Studies"}]};const d=[{depth:2,url:"#operating-system",title:e.jsx(e.Fragment,{children:"Operating System"})},{depth:3,url:"#memory",title:e.jsx(e.Fragment,{children:"Memory"})},{depth:3,url:"#64-bit",title:e.jsx(e.Fragment,{children:"64-bit"})},{depth:3,url:"#swapping",title:e.jsx(e.Fragment,{children:"Swapping"})},{depth:3,url:"#cpu",title:e.jsx(e.Fragment,{children:"CPU"})},{depth:2,url:"#performance-network",title:e.jsx(e.Fragment,{children:"Network"})},{depth:3,url:"#single-switch",title:e.jsx(e.Fragment,{children:"Single Switch"})},{depth:3,url:"#multiple-switches",title:e.jsx(e.Fragment,{children:"Multiple Switches"})},{depth:3,url:"#multiple-racks",title:e.jsx(e.Fragment,{children:"Multiple Racks"})},{depth:3,url:"#performance-network-interfaces",title:e.jsx(e.Fragment,{children:"Network Interfaces"})},{depth:3,url:"#network-consistency-and-partition-tolerance",title:e.jsx(e.Fragment,{children:"Network Consistency and Partition Tolerance"})},{depth:2,url:"#performance-java",title:e.jsx(e.Fragment,{children:"Java"})},{depth:3,url:"#the-garbage-collector-and-apache-hbase",title:e.jsx(e.Fragment,{children:"The Garbage Collector and Apache HBase"})},{depth:4,url:"#long-gc-pauses",title:e.jsx(e.Fragment,{children:"Long GC pauses"})},{depth:2,url:"#hbase-configurations",title:e.jsx(e.Fragment,{children:"HBase Configurations"})},{depth:3,url:"#improving-the-99th-percentile",title:e.jsx(e.Fragment,{children:"Improving the 99th Percentile"})},{depth:3,url:"#managing-compactions",title:e.jsx(e.Fragment,{children:"Managing Compactions"})},{depth:3,url:"#performance-hbaseregionserverhandlercount",title:e.jsx(e.Fragment,{children:e.jsx("code",{children:"hbase.regionserver.handler.count"})})},{depth:3,url:"#hfileblockcachesize",title:e.jsx(e.Fragment,{children:e.jsx("code",{children:"hfile.block.cache.size"})})},{depth:3,url:"#prefetch-option-for-blockcache",title:e.jsx(e.Fragment,{children:"Prefetch Option for Blockcache"})},{depth:3,url:"#hbaseregionserverglobalmemstoresize",title:e.jsx(e.Fragment,{children:e.jsx("code",{children:"hbase.regionserver.global.memstore.size"})})},{depth:3,url:"#hbaseregionserverglobalmemstoresizelowerlimit",title:e.jsx(e.Fragment,{children:e.jsx("code",{children:"hbase.regionserver.global.memstore.size.lower.limit"})})},{depth:3,url:"#hbasehstoreblockingstorefiles",title:e.jsx(e.Fragment,{children:e.jsx("code",{children:"hbase.hstore.blockingStoreFiles"})})},{depth:3,url:"#hbasehregionmemstoreblockmultiplier",title:e.jsx(e.Fragment,{children:e.jsx("code",{children:"hbase.hregion.memstore.block.multiplier"})})},{depth:3,url:"#hbaseregionserverchecksumverify",title:e.jsx(e.Fragment,{children:e.jsx("code",{children:"hbase.regionserver.checksum.verify"})})},{depth:3,url:"#tuning-callqueue-options",title:e.jsxs(e.Fragment,{children:["Tuning ",e.jsx("code",{children:"callQueue"})," Options"]})},{depth:2,url:"#performance-zookeeper",title:e.jsx(e.Fragment,{children:"ZooKeeper"})},{depth:2,url:"#performance-schema-design",title:e.jsx(e.Fragment,{children:"Schema Design"})},{depth:3,url:"#number-of-column-families",title:e.jsx(e.Fragment,{children:"Number of Column Families"})},{depth:3,url:"#key-and-attribute-lengths",title:e.jsx(e.Fragment,{children:"Key and Attribute Lengths"})},{depth:3,url:"#table-regionsize",title:e.jsx(e.Fragment,{children:"Table RegionSize"})},{depth:3,url:"#bloom-filters",title:e.jsx(e.Fragment,{children:"Bloom Filters"})},{depth:4,url:"#when-to-use-bloom-filters",title:e.jsx(e.Fragment,{children:"When To Use Bloom Filters"})},{depth:4,url:"#enabling-bloom-filters",title:e.jsx(e.Fragment,{children:"Enabling Bloom Filters"})},{depth:4,url:"#configuring-server-wide-behavior-of-bloom-filters",title:e.jsx(e.Fragment,{children:"Configuring Server-Wide Behavior of Bloom Filters"})},{depth:3,url:"#columnfamily-blocksize",title:e.jsx(e.Fragment,{children:"ColumnFamily BlockSize"})},{depth:3,url:"#in-memory-columnfamilies",title:e.jsx(e.Fragment,{children:"In-Memory ColumnFamilies"})},{depth:3,url:"#performance-schema-design-compression",title:e.jsx(e.Fragment,{children:"Compression"})},{depth:4,url:"#however",title:e.jsx(e.Fragment,{children:"However..."})},{depth:2,url:"#hbase-general-patterns",title:e.jsx(e.Fragment,{children:"HBase General Patterns"})},{depth:3,url:"#constants",title:e.jsx(e.Fragment,{children:"Constants"})},{depth:2,url:"#writing-to-hbase",title:e.jsx(e.Fragment,{children:"Writing to HBase"})},{depth:3,url:"#batch-loading",title:e.jsx(e.Fragment,{children:"Batch Loading"})},{depth:3,url:"#table-creation-pre-creating-regions",title:e.jsx(e.Fragment,{children:"Table Creation: Pre-Creating Regions"})},{depth:3,url:"#table-creation-deferred-log-flush",title:e.jsx(e.Fragment,{children:"Table Creation: Deferred Log Flush"})},{depth:3,url:"#hbase-client-turn-off-wal-on-puts",title:e.jsx(e.Fragment,{children:"HBase Client: Turn off WAL on Puts"})},{depth:3,url:"#hbase-client-group-puts-by-regionserver",title:e.jsx(e.Fragment,{children:"HBase Client: Group Puts by RegionServer"})},{depth:3,url:"#mapreduce-skip-the-reducer",title:e.jsx(e.Fragment,{children:"MapReduce: Skip The Reducer"})},{depth:3,url:"#anti-pattern-one-hot-region",title:e.jsx(e.Fragment,{children:"Anti-Pattern: One Hot Region"})},{depth:2,url:"#reading-from-hbase",title:e.jsx(e.Fragment,{children:"Reading from HBase"})},{depth:3,url:"#scan-caching",title:e.jsx(e.Fragment,{children:"Scan Caching"})},{depth:4,url:"#scan-caching-in-mapreduce-jobs",title:e.jsx(e.Fragment,{children:"Scan Caching in MapReduce Jobs"})},{depth:3,url:"#scan-attribute-selection",title:e.jsx(e.Fragment,{children:"Scan Attribute Selection"})},{depth:3,url:"#avoid-scan-seeks",title:e.jsx(e.Fragment,{children:"Avoid scan seeks"})},{depth:3,url:"#mapreduce---input-splits",title:e.jsx(e.Fragment,{children:"MapReduce - Input Splits"})},{depth:3,url:"#close-resultscanners",title:e.jsx(e.Fragment,{children:"Close ResultScanners"})},{depth:3,url:"#performance-reading-from-hbase-block-cache",title:e.jsx(e.Fragment,{children:"Block Cache"})},{depth:3,url:"#optimal-loading-of-row-keys",title:e.jsx(e.Fragment,{children:"Optimal Loading of Row Keys"})},{depth:3,url:"#concurrency-monitor-data-spread",title:e.jsx(e.Fragment,{children:"Concurrency: Monitor Data Spread"})},{depth:3,url:"#bloom-filters-1",title:e.jsx(e.Fragment,{children:"Bloom Filters"})},{depth:4,url:"#bloom-storefile-footprint",title:e.jsx(e.Fragment,{children:"Bloom StoreFile footprint"})},{depth:4,url:"#bloom-filter-configuration",title:e.jsx(e.Fragment,{children:"Bloom Filter Configuration"})},{depth:3,url:"#hedged-reads",title:e.jsx(e.Fragment,{children:"Hedged Reads"})},{depth:2,url:"#deleting-from-hbase",title:e.jsx(e.Fragment,{children:"Deleting from HBase"})},{depth:3,url:"#using-hbase-tables-as-queues",title:e.jsx(e.Fragment,{children:"Using HBase Tables as Queues"})},{depth:3,url:"#delete-rpc-behavior",title:e.jsx(e.Fragment,{children:"Delete RPC Behavior"})},{depth:2,url:"#hdfs",title:e.jsx(e.Fragment,{children:"HDFS"})},{depth:3,url:"#current-issues-with-low-latency-reads",title:e.jsx(e.Fragment,{children:"Current Issues With Low-Latency Reads"})},{depth:3,url:"#leveraging-local-data",title:e.jsx(e.Fragment,{children:"Leveraging local data"})},{depth:3,url:"#performance-comparisons-of-hbase-vs-hdfs",title:e.jsx(e.Fragment,{children:"Performance Comparisons of HBase vs. HDFS"})},{depth:2,url:"#performance-amazon-ec2",title:e.jsx(e.Fragment,{children:"Amazon EC2"})},{depth:2,url:"#collocating-hbase-and-mapreduce",title:e.jsx(e.Fragment,{children:"Collocating HBase and MapReduce"})},{depth:2,url:"#performance-case-studies",title:e.jsx(e.Fragment,{children:"Case Studies"})}];function n(s){const i={a:"a",br:"br",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...s.components},{Callout:t}=i;return t||a("Callout"),e.jsxs(e.Fragment,{children:[e.jsx(i.h2,{id:"operating-system",children:"Operating System"}),`
`,e.jsx(i.h3,{id:"memory",children:"Memory"}),`
`,e.jsx(i.p,{children:"RAM, RAM, RAM. Don't starve HBase."}),`
`,e.jsx(i.h3,{id:"64-bit",children:"64-bit"}),`
`,e.jsx(i.p,{children:"Use a 64-bit platform (and 64-bit JVM)."}),`
`,e.jsx(i.h3,{id:"swapping",children:"Swapping"}),`
`,e.jsxs(i.p,{children:["Watch out for swapping. Set ",e.jsx(i.code,{children:"swappiness"})," to 0."]}),`
`,e.jsx(i.h3,{id:"cpu",children:"CPU"}),`
`,e.jsxs(i.p,{children:["Make sure you have set up your Hadoop to use native, hardware checksumming. See ",e.jsx(i.a,{href:"/docs/compression#making-use-of-hadoop-native-libraries-in-hbase",children:"hadoop.native.lib"}),"."]}),`
`,e.jsx(i.h2,{id:"performance-network",children:"Network"}),`
`,e.jsx(i.p,{children:"Perhaps the most important factor in avoiding network issues degrading Hadoop and HBase performance is the switching hardware that is used, decisions made early in the scope of the project can cause major problems when you double or triple the size of your cluster (or more)."}),`
`,e.jsx(i.p,{children:"Important items to consider:"}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsx(i.li,{children:"Switching capacity of the device"}),`
`,e.jsx(i.li,{children:"Number of systems connected"}),`
`,e.jsx(i.li,{children:"Uplink capacity"}),`
`]}),`
`,e.jsx(i.h3,{id:"single-switch",children:"Single Switch"}),`
`,e.jsx(i.p,{children:"The single most important factor in this configuration is that the switching capacity of the hardware is capable of handling the traffic which can be generated by all systems connected to the switch. Some lower priced commodity hardware can have a slower switching capacity than could be utilized by a full switch."}),`
`,e.jsx(i.h3,{id:"multiple-switches",children:"Multiple Switches"}),`
`,e.jsx(i.p,{children:"Multiple switches are a potential pitfall in the architecture. The most common configuration of lower priced hardware is a simple 1Gbps uplink from one switch to another. This often overlooked pinch point can easily become a bottleneck for cluster communication. Especially with MapReduce jobs that are both reading and writing a lot of data the communication across this uplink could be saturated."}),`
`,e.jsx(i.p,{children:"Mitigation of this issue is fairly simple and can be accomplished in multiple ways:"}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsx(i.li,{children:"Use appropriate hardware for the scale of the cluster which you're attempting to build."}),`
`,e.jsx(i.li,{children:"Use larger single switch configurations i.e. single 48 port as opposed to 2x 24 port"}),`
`,e.jsx(i.li,{children:"Configure port trunking for uplinks to utilize multiple interfaces to increase cross switch bandwidth."}),`
`]}),`
`,e.jsx(i.h3,{id:"multiple-racks",children:"Multiple Racks"}),`
`,e.jsx(i.p,{children:"Multiple rack configurations carry the same potential issues as multiple switches, and can suffer performance degradation from two main areas:"}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsx(i.li,{children:"Poor switch capacity performance"}),`
`,e.jsx(i.li,{children:"Insufficient uplink to another rack"}),`
`]}),`
`,e.jsx(i.p,{children:"If the switches in your rack have appropriate switching capacity to handle all the hosts at full speed, the next most likely issue will be caused by homing more of your cluster across racks. The easiest way to avoid issues when spanning multiple racks is to use port trunking to create a bonded uplink to other racks. The downside of this method however, is in the overhead of ports that could potentially be used. An example of this is, creating an 8Gbps port channel from rack A to rack B, using 8 of your 24 ports to communicate between racks gives you a poor ROI, using too few however can mean you're not getting the most out of your cluster."}),`
`,e.jsx(i.p,{children:"Using 10Gbe links between racks will greatly increase performance, and assuming your switches support a 10Gbe uplink or allow for an expansion card will allow you to save your ports for machines as opposed to uplinks."}),`
`,e.jsx(i.h3,{id:"performance-network-interfaces",children:"Network Interfaces"}),`
`,e.jsxs(i.p,{children:["Are all the network interfaces functioning correctly? Are you sure? See the Troubleshooting Case Study in ",e.jsx(i.a,{href:"/docs/case-studies#case-study-1-performance-issue-on-a-single-node",children:"Case Study #1 (Performance Issue On A Single Node)"}),"."]}),`
`,e.jsx(i.h3,{id:"network-consistency-and-partition-tolerance",children:"Network Consistency and Partition Tolerance"}),`
`,e.jsxs(i.p,{children:["The ",e.jsx(i.a,{href:"http://en.wikipedia.org/wiki/CAP_theorem",children:"CAP Theorem"})," states that a distributed system can maintain two out of the following three characteristics:"]}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:[e.jsx(i.strong,{children:e.jsx(i.em,{children:"C"})}),"onsistency — all nodes see the same data."]}),`
`,e.jsxs(i.li,{children:[e.jsx(i.strong,{children:e.jsx(i.em,{children:"A"})}),"vailability — every request receives a response about whether it succeeded or failed."]}),`
`,e.jsxs(i.li,{children:[e.jsx(i.strong,{children:e.jsx(i.em,{children:"P"})}),"artition tolerance — the system continues to operate even if some of its components become unavailable to the others."]}),`
`]}),`
`,e.jsxs(i.p,{children:["HBase favors consistency and partition tolerance, where a decision has to be made. Coda Hale explains why partition tolerance is so important, in ",e.jsx(i.a,{href:"http://codahale.com/you-cant-sacrifice-partition-tolerance/",children:"http://codahale.com/you-cant-sacrifice-partition-tolerance/"}),"."]}),`
`,e.jsxs(i.p,{children:["Robert Yokota used an automated testing framework called ",e.jsx(i.a,{href:"https://aphyr.com/tags/jepsen",children:"Jepson"})," to test HBase's partition tolerance in the face of network partitions, using techniques modeled after Aphyr's ",e.jsx(i.a,{href:"https://aphyr.com/posts/281-call-me-maybe-carly-rae-jepsen-and-the-perils-of-network-partitions",children:"Call Me Maybe"})," series. The results, available as a ",e.jsx(i.a,{href:"https://rayokota.wordpress.com/2015/09/30/call-me-maybe-hbase/",children:"blog post"})," and an ",e.jsx(i.a,{href:"https://rayokota.wordpress.com/2015/09/30/call-me-maybe-hbase-addendum/",children:"addendum"}),", show that HBase performs correctly."]}),`
`,e.jsx(i.h2,{id:"performance-java",children:"Java"}),`
`,e.jsx(i.h3,{id:"the-garbage-collector-and-apache-hbase",children:"The Garbage Collector and Apache HBase"}),`
`,e.jsx(i.h4,{id:"long-gc-pauses",children:"Long GC pauses"}),`
`,e.jsxs(i.p,{children:["In his presentation, ",e.jsx(i.a,{href:"http://www.slideshare.net/cloudera/hbase-hug-presentation",children:"Avoiding Full GCs with MemStore-Local Allocation Buffers"}),", Todd Lipcon describes two cases of stop-the-world garbage collections common in HBase, especially during loading; CMS failure modes and old generation heap fragmentation brought."]}),`
`,e.jsxs(i.p,{children:["To address the first, start the CMS earlier than default by adding ",e.jsx(i.code,{children:"-XX:CMSInitiatingOccupancyFraction"})," and setting it down from defaults. Start at 60 or 70 percent (The lower you bring down the threshold, the more GCing is done, the more CPU used). To address the second fragmentation issue, Todd added an experimental facility, (MSLAB), that must be explicitly enabled in Apache HBase 0.90.x (It's defaulted to be ",e.jsx(i.em,{children:"on"})," in Apache 0.92.x HBase). Set ",e.jsx(i.code,{children:"hbase.hregion.memstore.mslab.enabled"})," to true in your ",e.jsx(i.code,{children:"Configuration"}),". See the cited slides for background and detail. The latest JVMs do better regards fragmentation so make sure you are running a recent release. Read down in the message, ",e.jsx(i.a,{href:"http://osdir.com/ml/hotspot-gc-use/2011-11/msg00002.html",children:"Identifying concurrent mode failures caused by fragmentation"}),". Be aware that when enabled, each MemStore instance will occupy at least an MSLAB instance of memory. If you have thousands of regions or lots of regions each with many column families, this allocation of MSLAB may be responsible for a good portion of your heap allocation and in an extreme case cause you to OOME. Disable MSLAB in this case, or lower the amount of memory it uses or float less regions per server."]}),`
`,e.jsxs(i.p,{children:["If you have a write-heavy workload, check out ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-8163",children:"HBASE-8163 MemStoreChunkPool: An improvement for JAVA GC when using MSLAB"}),". It describes configurations to lower the amount of young GC during write-heavy loadings. If you do not have HBASE-8163 installed, and you are trying to improve your young GC times, one trick to consider — courtesy of our Liang Xie — is to set the GC config ",e.jsx(i.code,{children:"-XX:PretenureSizeThreshold"})," in ",e.jsx(i.em,{children:"hbase-env.sh"})," to be just smaller than the size of ",e.jsx(i.code,{children:"hbase.hregion.memstore.mslab.chunksize"})," so MSLAB allocations happen in the tenured space directly rather than first in the young gen. You'd do this because these MSLAB allocations are going to likely make it to the old gen anyways and rather than pay the price of a copies between s0 and s1 in eden space followed by the copy up from young to old gen after the MSLABs have achieved sufficient tenure, save a bit of YGC churn and allocate in the old gen directly."]}),`
`,e.jsxs(i.p,{children:["Other sources of long GCs can be the JVM itself logging. See ",e.jsx(i.a,{href:"https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-traffic",children:"Eliminating Large JVM GC Pauses Caused by Background IO Traffic"})]}),`
`,e.jsxs(i.p,{children:["For more information about GC logs, see ",e.jsx(i.a,{href:"/docs/troubleshooting#jvm-garbage-collection-logs",children:"JVM Garbage Collection Logs"}),"."]}),`
`,e.jsxs(i.p,{children:["Consider also enabling the off-heap Block Cache. This has been shown to mitigate GC pause times. See ",e.jsx(i.a,{href:"/docs/architecture/regionserver#architecture-regionserver-block-cache",children:"Block Cache"})]}),`
`,e.jsx(i.h2,{id:"hbase-configurations",children:"HBase Configurations"}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/configuration/important#recommended-configurations",children:"Recommended Configurations"}),"."]}),`
`,e.jsx(i.h3,{id:"improving-the-99th-percentile",children:"Improving the 99th Percentile"}),`
`,e.jsxs(i.p,{children:["Try ",e.jsx(i.a,{href:"/docs/performance#hedged-reads",children:"hedged_reads"}),"."]}),`
`,e.jsx(i.h3,{id:"managing-compactions",children:"Managing Compactions"}),`
`,e.jsxs(i.p,{children:["For larger systems, managing ",e.jsx(i.a,{href:"/docs/configuration/important#managed-compactions",children:"compactions and splits"})," may be something you want to consider."]}),`
`,e.jsx(i.h3,{id:"performance-hbaseregionserverhandlercount",children:e.jsx(i.code,{children:"hbase.regionserver.handler.count"})}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/configuration/default#hbaseregionserverhandlercount-toc",children:"hbase.regionserver.handler.count"}),"."]}),`
`,e.jsx(i.h3,{id:"hfileblockcachesize",children:e.jsx(i.code,{children:"hfile.block.cache.size"})}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/configuration/default#hfileblockcachesize-toc",children:"hfile.block.cache.size"}),". A memory setting for the RegionServer process."]}),`
`,e.jsx(i.h3,{id:"prefetch-option-for-blockcache",children:"Prefetch Option for Blockcache"}),`
`,e.jsxs(i.p,{children:[e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-9857",children:"HBASE-9857"})," adds a new option to prefetch HFile contents when opening the BlockCache, if a Column family or RegionServer property is set. This option is available for HBase 0.98.3 and later. The purpose is to warm the BlockCache as rapidly as possible after the cache is opened, using in-memory table data, and not counting the prefetching as cache misses. This is great for fast reads, but is not a good idea if the data to be preloaded will not fit into the BlockCache. It is useful for tuning the IO impact of prefetching versus the time before all data blocks are in cache."]}),`
`,e.jsx(i.p,{children:"To enable prefetching on a given column family, you can use HBase Shell or use the API."}),`
`,e.jsx(i.p,{children:e.jsx(i.strong,{children:"Enable Prefetch Using HBase Shell"})}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M20.156.083c3.033.525 3.893 2.598 3.829 4.77L24 4.822 22.635 22.71 4.89 23.926h.016C3.433 23.864.15 23.729 0 19.139l1.645-3 2.819 6.586.503 1.172 2.805-9.144-.03.007.016-.03 9.255 2.956-1.396-5.431-.99-3.9 8.82-.569-.615-.51L16.5 2.114 20.159.073l-.003.01zM0 19.089zM5.13 5.073c3.561-3.533 8.157-5.621 9.922-3.84 1.762 1.777-.105 6.105-3.673 9.636-3.563 3.532-8.103 5.734-9.864 3.957-1.766-1.777.045-6.217 3.612-9.75l.003-.003z" fill="currentColor" /></svg>',children:e.jsx(i.code,{children:e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"hbase"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:">"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" create "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'MyTable'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", { "}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"NAME"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" => "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'myCF'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", "}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"PREFETCH_BLOCKS_ON_OPEN"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" => "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'true'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" }"})]})})})}),`
`,e.jsx(i.p,{children:e.jsx(i.strong,{children:"Enable Prefetch Using the API"})}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"// ..."})}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"HTableDescriptor tableDesc "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" HTableDescriptor"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"("}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:'"myTable"'}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:");"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"HColumnDescriptor cfDesc "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" HColumnDescriptor"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"("}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:'"myCF"'}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:");"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"cfDesc."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"setPrefetchBlocksOnOpen"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"("}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"true"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:");"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"tableDesc."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"addFamily"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(cfDesc);"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"// ..."})})]})})}),`
`,e.jsxs(i.p,{children:["See the API documentation for ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CacheConfig.html",children:"CacheConfig"}),"."]}),`
`,e.jsxs(i.p,{children:["To see prefetch in operation, enable TRACE level logging on ",e.jsx(i.code,{children:"org.apache.hadoop.hbase.io.hfile.HFileReaderImpl"})," in hbase-2.0+ or on ",e.jsx(i.code,{children:"org.apache.hadoop.hbase.io.hfile.HFileReaderV2"})," in earlier versions, hbase-1.x, of HBase."]}),`
`,e.jsx(i.h3,{id:"hbaseregionserverglobalmemstoresize",children:e.jsx(i.code,{children:"hbase.regionserver.global.memstore.size"})}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/configuration/default#hbaseregionserverglobalmemstoresize-toc",children:"hbase.regionserver.global.memstore.size"}),". This memory setting is often adjusted for the RegionServer process depending on needs."]}),`
`,e.jsx(i.h3,{id:"hbaseregionserverglobalmemstoresizelowerlimit",children:e.jsx(i.code,{children:"hbase.regionserver.global.memstore.size.lower.limit"})}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/configuration/default#hbaseregionserverglobalmemstoresizelowerlimit-toc",children:"hbase.regionserver.global.memstore.size.lower.limit"}),". This memory setting is often adjusted for the RegionServer process depending on needs."]}),`
`,e.jsx(i.h3,{id:"hbasehstoreblockingstorefiles",children:e.jsx(i.code,{children:"hbase.hstore.blockingStoreFiles"})}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/configuration/default#hbasehstoreblockingStoreFiles-toc",children:"hbase.hstore.blockingStoreFiles"}),". If there is blocking in the RegionServer logs, increasing this can help."]}),`
`,e.jsx(i.h3,{id:"hbasehregionmemstoreblockmultiplier",children:e.jsx(i.code,{children:"hbase.hregion.memstore.block.multiplier"})}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/configuration/default#hbasehregionmemstoreblockmultiplier-toc",children:"hbase.hregion.memstore.block.multiplier"}),". If there is enough RAM, increasing this can help."]}),`
`,e.jsx(i.h3,{id:"hbaseregionserverchecksumverify",children:e.jsx(i.code,{children:"hbase.regionserver.checksum.verify"})}),`
`,e.jsx(i.p,{children:"Have HBase write the checksum into the datablock and save having to do the checksum seek whenever you read."}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/configuration/default#hbaseregionserverchecksumverify-toc",children:"hbase.regionserver.checksum.verify"}),", ",e.jsx(i.a,{href:"/docs/configuration/default#hbasehstorebytesperchecksum-toc",children:"hbase.hstore.bytes.per.checksum"})," and ",e.jsx(i.a,{href:"/docs/configuration/default#hbasehstorechecksumalgorithm-toc",children:"hbase.hstore.checksum.algorithm"}),". For more information see the release note on ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-5074",children:"HBASE-5074 support checksums in HBase block cache"}),"."]}),`
`,e.jsxs(i.h3,{id:"tuning-callqueue-options",children:["Tuning ",e.jsx(i.code,{children:"callQueue"})," Options"]}),`
`,e.jsxs(i.p,{children:[e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-11355",children:"HBASE-11355"})," introduces several callQueue tuning mechanisms which can increase performance. See the JIRA for some benchmarking information."]}),`
`,e.jsxs(i.p,{children:["To increase the number of callqueues, set ",e.jsx(i.code,{children:"hbase.ipc.server.num.callqueue"})," to a value greater than ",e.jsx(i.code,{children:"1"}),". To split the callqueue into separate read and write queues, set ",e.jsx(i.code,{children:"hbase.ipc.server.callqueue.read.ratio"})," to a value between ",e.jsx(i.code,{children:"0"})," and ",e.jsx(i.code,{children:"1"}),". This factor weights the queues toward writes (if below .5) or reads (if above .5). Another way to say this is that the factor determines what percentage of the split queues are used for reads. The following examples illustrate some of the possibilities. Note that you always have at least one write queue, no matter what setting you use."]}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:["The default value of ",e.jsx(i.code,{children:"0"})," does not split the queue."]}),`
`,e.jsxs(i.li,{children:["A value of ",e.jsx(i.code,{children:".3"})," uses 30% of the queues for reading and 70% for writing. Given a value of ",e.jsx(i.code,{children:"10"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.num.callqueue"}),", 3 queues would be used for reads and 7 for writes."]}),`
`,e.jsxs(i.li,{children:["A value of ",e.jsx(i.code,{children:".5"})," uses the same number of read queues and write queues. Given a value of ",e.jsx(i.code,{children:"10"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.num.callqueue"}),", 5 queues would be used for reads and 5 for writes."]}),`
`,e.jsxs(i.li,{children:["A value of ",e.jsx(i.code,{children:".6"})," uses 60% of the queues for reading and 40% for writing. Given a value of ",e.jsx(i.code,{children:"10"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.num.callqueue"}),", 6 queues would be used for reads and 4 for writes."]}),`
`,e.jsxs(i.li,{children:["A value of ",e.jsx(i.code,{children:"1.0"})," uses one queue to process write requests, and all other queues process read requests. A value higher than ",e.jsx(i.code,{children:"1.0"})," has the same effect as a value of ",e.jsx(i.code,{children:"1.0"}),". Given a value of ",e.jsx(i.code,{children:"10"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.num.callqueue"}),", 9 queues would be used for reads and 1 for writes."]}),`
`]}),`
`,e.jsxs(i.p,{children:["You can also split the read queues so that separate queues are used for short reads (from Get operations) and long reads (from Scan operations), by setting the ",e.jsx(i.code,{children:"hbase.ipc.server.callqueue.scan.ratio"})," option. This option is a factor between 0 and 1, which determine the ratio of read queues used for Gets and Scans. More queues are used for Gets if the value is below ",e.jsx(i.code,{children:".5"})," and more are used for scans if the value is above ",e.jsx(i.code,{children:".5"}),". No matter what setting you use, at least one read queue is used for Get operations."]}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:["A value of ",e.jsx(i.code,{children:"0"})," does not split the read queue."]}),`
`,e.jsxs(i.li,{children:["A value of ",e.jsx(i.code,{children:".3"})," uses 70% of the read queues for Gets and 30% for Scans. Given a value of ",e.jsx(i.code,{children:"20"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.num.callqueue"})," and a value of ",e.jsx(i.code,{children:".5"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.callqueue.read.ratio"}),", 10 queues would be used for reads, out of those 10, 7 would be used for Gets and 3 for Scans."]}),`
`,e.jsxs(i.li,{children:["A value of ",e.jsx(i.code,{children:".5"})," uses half the read queues for Gets and half for Scans. Given a value of ",e.jsx(i.code,{children:"20"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.num.callqueue"})," and a value of ",e.jsx(i.code,{children:".5"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.callqueue.read.ratio"}),", 10 queues would be used for reads, out of those 10, 5 would be used for Gets and 5 for Scans."]}),`
`,e.jsxs(i.li,{children:["A value of ",e.jsx(i.code,{children:".7"})," uses 30% of the read queues for Gets and 70% for Scans. Given a value of ",e.jsx(i.code,{children:"20"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.num.callqueue"})," and a value of ",e.jsx(i.code,{children:".5"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.callqueue.read.ratio"}),", 10 queues would be used for reads, out of those 10, 3 would be used for Gets and 7 for Scans."]}),`
`,e.jsxs(i.li,{children:["A value of ",e.jsx(i.code,{children:"1.0"})," uses all but one of the read queues for Scans. Given a value of ",e.jsx(i.code,{children:"20"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.num.callqueue"})," and a value of ",e.jsx(i.code,{children:".5"})," for ",e.jsx(i.code,{children:"hbase.ipc.server.callqueue.read.ratio"}),", 10 queues would be used for reads, out of those 10, 1 would be used for Gets and 9 for Scans."]}),`
`]}),`
`,e.jsxs(i.p,{children:["You can use the new option ",e.jsx(i.code,{children:"hbase.ipc.server.callqueue.handler.factor"})," to programmatically tune the number of queues:"]}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:["A value of ",e.jsx(i.code,{children:"0"})," uses a single shared queue between all the handlers."]}),`
`,e.jsxs(i.li,{children:["A value of ",e.jsx(i.code,{children:"1"})," uses a separate queue for each handler."]}),`
`,e.jsxs(i.li,{children:["A value between ",e.jsx(i.code,{children:"0"})," and ",e.jsx(i.code,{children:"1"})," tunes the number of queues against the number of handlers. For instance, a value of ",e.jsx(i.code,{children:".5"})," shares one queue between each two handlers.",e.jsx(i.br,{}),`
`,"Having more queues, such as in a situation where you have one queue per handler, reduces contention when adding a task to a queue or selecting it from a queue. The trade-off is that if you have some queues with long-running tasks, a handler may end up waiting to execute from that queue rather than processing another queue which has waiting tasks."]}),`
`]}),`
`,e.jsx(i.p,{children:"For these values to take effect on a given RegionServer, the RegionServer must be restarted. These parameters are intended for testing purposes and should be used carefully."}),`
`,e.jsx(i.h2,{id:"performance-zookeeper",children:"ZooKeeper"}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/zookeeper",children:"ZooKeeper"})," for information on configuring ZooKeeper, and see the part about having a dedicated disk."]}),`
`,e.jsx(i.h2,{id:"performance-schema-design",children:"Schema Design"}),`
`,e.jsx(i.h3,{id:"number-of-column-families",children:"Number of Column Families"}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/regionserver-sizing#on-the-number-of-column-families",children:"On the number of column families"}),"."]}),`
`,e.jsx(i.h3,{id:"key-and-attribute-lengths",children:"Key and Attribute Lengths"}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/regionserver-sizing#try-to-minimize-row-and-column-sizes",children:"Try to minimize row and column sizes"}),". See also ",e.jsx(i.a,{href:"/docs/performance#however",children:"However..."})," for compression caveats."]}),`
`,e.jsx(i.h3,{id:"table-regionsize",children:"Table RegionSize"}),`
`,e.jsxs(i.p,{children:["The regionsize can be set on a per-table basis via ",e.jsx(i.code,{children:"setMaxFileSize"})," on ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/TableDescriptorBuilder.html",children:"TableDescriptorBuilder"})," in the event where certain tables require different regionsizes than the configured default regionsize."]}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/operational-management/region-and-capacity#determining-region-count-and-size",children:"Determining region count and size"})," for more information."]}),`
`,e.jsx(i.h3,{id:"bloom-filters",children:"Bloom Filters"}),`
`,e.jsxs(i.p,{children:['A Bloom filter, named for its creator, Burton Howard Bloom, is a data structure which is designed to predict whether a given element is a member of a set of data. A positive result from a Bloom filter is not always accurate, but a negative result is guaranteed to be accurate. Bloom filters are designed to be "accurate enough" for sets of data which are so large that conventional hashing mechanisms would be impractical. For more information about Bloom filters in general, refer to ',e.jsx(i.a,{href:"http://en.wikipedia.org/wiki/Bloom_filter",children:"http://en.wikipedia.org/wiki/Bloom_filter"}),"."]}),`
`,e.jsx(i.p,{children:"In terms of HBase, Bloom filters provide a lightweight in-memory structure to reduce the number of disk reads for a given Get operation (Bloom filters do not work with Scans) to only the StoreFiles likely to contain the desired Row. The potential performance gain increases with the number of parallel reads."}),`
`,e.jsx(i.p,{children:"The Bloom filters themselves are stored in the metadata of each HFile and never need to be updated. When an HFile is opened because a region is deployed to a RegionServer, the Bloom filter is loaded into memory."}),`
`,e.jsx(i.p,{children:"HBase includes some tuning mechanisms for folding the Bloom filter to reduce the size and keep the false positive rate within a desired range."}),`
`,e.jsxs(i.p,{children:["Bloom filters were introduced in ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-1200",children:"HBASE-1200"}),". Since HBase 0.96, row-based Bloom filters are enabled by default. (",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-8450",children:"HBASE-8450"}),")"]}),`
`,e.jsxs(i.p,{children:["For more information on Bloom filters in relation to HBase, see ",e.jsx(i.a,{href:"/docs/performance#bloom-filters",children:"Bloom Filters"})," for more information, or the following Quora discussion: ",e.jsx(i.a,{href:"http://www.quora.com/How-are-bloom-filters-used-in-HBase",children:"How are bloom filters used in HBase?"}),"."]}),`
`,e.jsx(i.h4,{id:"when-to-use-bloom-filters",children:"When To Use Bloom Filters"}),`
`,e.jsx(i.p,{children:"Since HBase 0.96, row-based Bloom filters are enabled by default. You may choose to disable them or to change some tables to use row+column Bloom filters, depending on the characteristics of your data and how it is loaded into HBase."}),`
`,e.jsxs(i.p,{children:["To determine whether Bloom filters could have a positive impact, check the value of ",e.jsx(i.code,{children:"blockCacheHitRatio"})," in the RegionServer metrics. If Bloom filters are enabled, the value of ",e.jsx(i.code,{children:"blockCacheHitRatio"})," should increase, because the Bloom filter is filtering out blocks that are definitely not needed."]}),`
`,e.jsx(i.p,{children:"You can choose to enable Bloom filters for a row or for a row+column combination. If you generally scan entire rows, the row+column combination will not provide any benefit. A row-based Bloom filter can operate on a row+column Get, but not the other way around. However, if you have a large number of column-level Puts, such that a row may be present in every StoreFile, a row-based filter will always return a positive result and provide no benefit. Unless you have one column per row, row+column Bloom filters require more space, in order to store more keys. Bloom filters work best when the size of each data entry is at least a few kilobytes in size."}),`
`,e.jsx(i.p,{children:"Overhead will be reduced when your data is stored in a few larger StoreFiles, to avoid extra disk IO during low-level scans to find a specific row."}),`
`,e.jsx(i.p,{children:"Bloom filters need to be rebuilt upon deletion, so may not be appropriate in environments with a large number of deletions."}),`
`,e.jsx(i.h4,{id:"enabling-bloom-filters",children:"Enabling Bloom Filters"}),`
`,e.jsxs(i.p,{children:["Bloom filters are enabled on a Column Family. You can do this by using the setBloomFilterType method of HColumnDescriptor or using the HBase API. Valid values are ",e.jsx(i.code,{children:"NONE"}),", ",e.jsx(i.code,{children:"ROW"})," (default), or ",e.jsx(i.code,{children:"ROWCOL"}),". See ",e.jsx(i.a,{href:"/docs/performance#when-to-use-bloom-filters",children:"When To Use Bloom Filters"})," for more information on ",e.jsx(i.code,{children:"ROW"})," versus ",e.jsx(i.code,{children:"ROWCOL"}),". See also the API documentation for ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.html",children:"ColumnFamilyDescriptorBuilder"}),"."]}),`
`,e.jsxs(i.p,{children:["The following example creates a table and enables a ROWCOL Bloom filter on the ",e.jsx(i.code,{children:"colfam1"})," column family."]}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M20.156.083c3.033.525 3.893 2.598 3.829 4.77L24 4.822 22.635 22.71 4.89 23.926h.016C3.433 23.864.15 23.729 0 19.139l1.645-3 2.819 6.586.503 1.172 2.805-9.144-.03.007.016-.03 9.255 2.956-1.396-5.431-.99-3.9 8.82-.569-.615-.51L16.5 2.114 20.159.073l-.003.01zM0 19.089zM5.13 5.073c3.561-3.533 8.157-5.621 9.922-3.84 1.762 1.777-.105 6.105-3.673 9.636-3.563 3.532-8.103 5.734-9.864 3.957-1.766-1.777.045-6.217 3.612-9.75l.003-.003z" fill="currentColor" /></svg>',children:e.jsx(i.code,{children:e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"hbase"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:">"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" create "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'mytable'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:",{"}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"NAME"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" => "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'colfam1'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", "}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"BLOOMFILTER"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" => "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'ROWCOL'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"}"})]})})})}),`
`,e.jsx(i.h4,{id:"configuring-server-wide-behavior-of-bloom-filters",children:"Configuring Server-Wide Behavior of Bloom Filters"}),`
`,e.jsxs(i.p,{children:["You can configure the following settings in the ",e.jsx(i.em,{children:"hbase-site.xml"}),"."]}),`
`,e.jsxs(i.table,{children:[e.jsx(i.thead,{children:e.jsxs(i.tr,{children:[e.jsx(i.th,{children:"Parameter"}),e.jsx(i.th,{children:"Default"}),e.jsx(i.th,{children:"Description"})]})}),e.jsxs(i.tbody,{children:[e.jsxs(i.tr,{children:[e.jsx(i.td,{children:"io.storefile.bloom.enabled"}),e.jsx(i.td,{children:"yes"}),e.jsx(i.td,{children:"Set to no to kill bloom filters server-wide if something goes wrong"})]}),e.jsxs(i.tr,{children:[e.jsx(i.td,{children:"io.storefile.bloom.error.rate"}),e.jsx(i.td,{children:".01"}),e.jsx(i.td,{children:"The average false positive rate for bloom filters. Folding is used to maintain the false positive rate. Expressed as a decimal representation of a percentage."})]}),e.jsxs(i.tr,{children:[e.jsx(i.td,{children:"io.storefile.bloom.max.fold"}),e.jsx(i.td,{children:"7"}),e.jsx(i.td,{children:"The guaranteed maximum fold rate. Changing this setting should not be necessary and is not recommended."})]}),e.jsxs(i.tr,{children:[e.jsx(i.td,{children:"io.storefile.bloom.max.keys"}),e.jsx(i.td,{children:"128000000"}),e.jsx(i.td,{children:"For default (single-block) Bloom filters, this specifies the maximum number of keys."})]}),e.jsxs(i.tr,{children:[e.jsx(i.td,{children:"io.storefile.delete.family.bloom.enabled"}),e.jsx(i.td,{children:"true"}),e.jsx(i.td,{children:"Master switch to enable Delete Family Bloom filters and store them in the StoreFile."})]}),e.jsxs(i.tr,{children:[e.jsx(i.td,{children:"io.storefile.bloom.block.size"}),e.jsx(i.td,{children:"131072"}),e.jsx(i.td,{children:"Target Bloom block size. Bloom filter blocks of approximately this size are interleaved with data blocks."})]}),e.jsxs(i.tr,{children:[e.jsx(i.td,{children:"hfile.block.bloom.cacheonwrite"}),e.jsx(i.td,{children:"false"}),e.jsx(i.td,{children:"Enables cache-on-write for inline blocks of a compound Bloom filter."})]})]})]}),`
`,e.jsx(i.h3,{id:"columnfamily-blocksize",children:"ColumnFamily BlockSize"}),`
`,e.jsx(i.p,{children:"The blocksize can be configured for each ColumnFamily in a table, and defaults to 64k. Larger cell values require larger blocksizes. There is an inverse relationship between blocksize and the resulting StoreFile indexes (i.e., if the blocksize is doubled then the resulting indexes should be roughly halved)."}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.html",children:"ColumnFamilyDescriptorBuilder"})," and ",e.jsx(i.a,{href:"/docs/architecture/regions#store",children:"Store"})," for more information."]}),`
`,e.jsx(i.h3,{id:"in-memory-columnfamilies",children:"In-Memory ColumnFamilies"}),`
`,e.jsxs(i.p,{children:["ColumnFamilies can optionally be defined as in-memory. Data is still persisted to disk, just like any other ColumnFamily. In-memory blocks have the highest priority in the ",e.jsx(i.a,{href:"/docs/architecture/regionserver#architecture-regionserver-block-cache",children:"Block Cache"}),", but it is not a guarantee that the entire table will be in memory."]}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.html",children:"ColumnFamilyDescriptorBuilder"})," for more information."]}),`
`,e.jsx(i.h3,{id:"performance-schema-design-compression",children:"Compression"}),`
`,e.jsxs(i.p,{children:["Production systems should use compression with their ColumnFamily definitions. See ",e.jsx(i.a,{href:"/docs/compression",children:"Compression and Data Block Encoding In HBase"})," for more information."]}),`
`,e.jsx(i.h4,{id:"however",children:"However..."}),`
`,e.jsxs(i.p,{children:["Compression deflates data ",e.jsx(i.em,{children:"on disk"}),". When it's in-memory (e.g., in the MemStore) or on the wire (e.g., transferring between RegionServer and Client) it's inflated. So while using ColumnFamily compression is a best practice, but it's not going to completely eliminate the impact of over-sized Keys, over-sized ColumnFamily names, or over-sized Column names."]}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/regionserver-sizing#try-to-minimize-row-and-column-sizes",children:"Try to minimize row and column sizes"})," on for schema design tips, and ",e.jsx(i.a,{href:"/docs/architecture/regions#keyvalue",children:"KeyValue"})," for more information on HBase stores data internally."]}),`
`,e.jsx(i.h2,{id:"hbase-general-patterns",children:"HBase General Patterns"}),`
`,e.jsx(i.h3,{id:"constants",children:"Constants"}),`
`,e.jsx(i.p,{children:"When people get started with HBase they have a tendency to write code that looks like this:"}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"Get get "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" Get"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(rowkey);"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"Result r "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" table."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"get"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(get);"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"byte"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"[] b "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" r."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getValue"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(Bytes."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"toBytes"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"("}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:'"cf"'}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"), Bytes."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"toBytes"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"("}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:'"attr"'}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"));  "}),e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"// returns current version of value"})]})]})})}),`
`,e.jsx(i.p,{children:"But especially when inside loops (and MapReduce jobs), converting the columnFamily and column-names to byte-arrays repeatedly is surprisingly expensive. It's better to use constants for the byte-arrays, like this:"}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"public"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" static"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" final"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" byte"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"[] CF "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:' "cf"'}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getBytes"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"public"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" static"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" final"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" byte"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"[] ATTR "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:' "attr"'}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getBytes"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"..."})}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"Get get "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" Get"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(rowkey);"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"Result r "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" table."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"get"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(get);"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"byte"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"[] b "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" r."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getValue"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(CF, ATTR);  "}),e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"// returns current version of value"})]})]})})}),`
`,e.jsx(i.h2,{id:"writing-to-hbase",children:"Writing to HBase"}),`
`,e.jsx(i.h3,{id:"batch-loading",children:"Batch Loading"}),`
`,e.jsxs(i.p,{children:["Use the bulk load tool if you can. See ",e.jsx(i.a,{href:"/docs/architecture/bulk-loading",children:"Bulk Loading"}),". Otherwise, pay attention to the below."]}),`
`,e.jsx(i.h3,{id:"table-creation-pre-creating-regions",children:"Table Creation: Pre-Creating Regions"}),`
`,e.jsx(i.p,{children:"Tables in HBase are initially created with one region by default. For bulk imports, this means that all clients will write to the same region until it is large enough to split and become distributed across the cluster. A useful pattern to speed up the bulk import process is to pre-create empty regions. Be somewhat conservative in this, because too-many regions can actually degrade performance."}),`
`,e.jsxs(i.p,{children:["There are two different approaches to pre-creating splits using the HBase API. The first approach is to rely on the default ",e.jsx(i.code,{children:"Admin"})," strategy (which is implemented in ",e.jsx(i.code,{children:"Bytes.split"}),")..."]}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"byte"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"[] startKey "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" ...;      "}),e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"// your lowest key"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"byte"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"[] endKey "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" ...;        "}),e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"// your highest key"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"int"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" numberOfRegions "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" ...;  "}),e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"// # of regions to create"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"admin."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"createTable"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(table, startKey, endKey, numberOfRegions);"})]})]})})}),`
`,e.jsx(i.p,{children:"And the other approach, using the HBase API, is to define the splits yourself..."}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"byte"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"[][] splits "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" ...;   "}),e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"// create your own splits"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"admin."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"createTable"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(table, splits);"})]})]})})}),`
`,e.jsx(i.p,{children:"You can achieve a similar effect using the HBase Shell to create tables by specifying split options."}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M20.156.083c3.033.525 3.893 2.598 3.829 4.77L24 4.822 22.635 22.71 4.89 23.926h.016C3.433 23.864.15 23.729 0 19.139l1.645-3 2.819 6.586.503 1.172 2.805-9.144-.03.007.016-.03 9.255 2.956-1.396-5.431-.99-3.9 8.82-.569-.615-.51L16.5 2.114 20.159.073l-.003.01zM0 19.089zM5.13 5.073c3.561-3.533 8.157-5.621 9.922-3.84 1.762 1.777-.105 6.105-3.673 9.636-3.563 3.532-8.103 5.734-9.864 3.957-1.766-1.777.045-6.217 3.612-9.75l.003-.003z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"# create table with specific split points"})}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"hbase"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:">"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"create "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'t1'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'f1'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"SPLITS"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" => ["}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'\\x10\\x00'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'\\x20\\x00'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'\\x30\\x00'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'\\x40\\x00'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"]"})]}),`
`,e.jsx(i.span,{className:"line"}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"# create table with four regions based on random bytes keys"})}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"hbase"}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:">"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"create "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'t2'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'f1'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", { "}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"NUMREGIONS"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" => "}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"4"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" , "}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"SPLITALGO"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" => "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'UniformSplit'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" }"})]}),`
`,e.jsx(i.span,{className:"line"}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"# create table with five regions based on hex keys"})}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"create "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'t3'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:","}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'f1'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", { "}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"NUMREGIONS"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" => "}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"5"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:", "}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"SPLITALGO"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" => "}),e.jsx(i.span,{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},children:"'HexStringSplit'"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" }"})]})]})})}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/regionserver-sizing#relationship-between-rowkeys-and-region-splits",children:"Relationship Between RowKeys and Region Splits"})," for issues related to understanding your keyspace and pre-creating regions. See ",e.jsx(i.a,{href:"/docs/architecture/regions#manual-region-splitting",children:"manual region splitting decisions"})," for discussion on manually pre-splitting regions. See ",e.jsx(i.a,{href:"/docs/shell#pre-splitting-tables-with-the-hbase-shell",children:"Pre-splitting tables with the HBase Shell"})," for more details of using the HBase Shell to pre-split tables."]}),`
`,e.jsx(i.h3,{id:"table-creation-deferred-log-flush",children:"Table Creation: Deferred Log Flush"}),`
`,e.jsxs(i.p,{children:["The default behavior for Puts using the Write Ahead Log (WAL) is that ",e.jsx(i.code,{children:"WAL"})," edits will be written immediately. If deferred log flush is used, WAL edits are kept in memory until the flush period. The benefit is aggregated and asynchronous ",e.jsx(i.code,{children:"WAL"}),"- writes, but the potential downside is that if the RegionServer goes down the yet-to-be-flushed edits are lost. This is safer, however, than not using WAL at all with Puts."]}),`
`,e.jsxs(i.p,{children:["Deferred log flush can be configured on tables via ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/TableDescriptorBuilder.html",children:"TableDescriptorBuilder"}),". The default value of ",e.jsx(i.code,{children:"hbase.regionserver.optionallogflushinterval"})," is 1000ms."]}),`
`,e.jsx(i.h3,{id:"hbase-client-turn-off-wal-on-puts",children:"HBase Client: Turn off WAL on Puts"}),`
`,e.jsx(i.p,{children:"A frequent request is to disable the WAL to increase performance of Puts. This is only appropriate for bulk loads, as it puts your data at risk by removing the protection of the WAL in the event of a region server crash. Bulk loads can be re-run in the event of a crash, with little risk of data loss."}),`
`,e.jsx(t,{type:"warn",children:e.jsx(i.p,{children:"If you disable the WAL for anything other than bulk loads, your data is at risk."})}),`
`,e.jsxs(i.p,{children:["In general, it is best to use WAL for Puts, and where loading throughput is a concern to use bulk loading techniques instead. For normal Puts, you are not likely to see a performance improvement which would outweigh the risk. To disable the WAL, see ",e.jsx(i.a,{href:"/docs/architecture/regionserver#disabling-the-wal",children:"Disabling the WAL"}),"."]}),`
`,e.jsx(i.h3,{id:"hbase-client-group-puts-by-regionserver",children:"HBase Client: Group Puts by RegionServer"}),`
`,e.jsxs(i.p,{children:["In addition to using the writeBuffer, grouping ",e.jsx(i.code,{children:"Put"}),"s by RegionServer can reduce the number of client RPC calls per writeBuffer flush. There is a utility ",e.jsx(i.code,{children:"HTableUtil"})," currently on MASTER that does this, but you can either copy that or implement your own version for those still on 0.90.x or earlier."]}),`
`,e.jsx(i.h3,{id:"mapreduce-skip-the-reducer",children:"MapReduce: Skip The Reducer"}),`
`,e.jsxs(i.p,{children:["When writing a lot of data to an HBase table from a MR job (e.g., with ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html",children:"TableOutputFormat"}),"), and specifically where Puts are being emitted from the Mapper, skip the Reducer step. When a Reducer step is used, all of the output (Puts) from the Mapper will get spooled to disk, then sorted/shuffled to other Reducers that will most likely be off-node. It's far more efficient to just write directly to HBase."]}),`
`,e.jsx(i.p,{children:"For summary jobs where HBase is used as a source and a sink, then writes will be coming from the Reducer step (e.g., summarize values then write out result). This is a different processing problem than from the above case."}),`
`,e.jsx(i.h3,{id:"anti-pattern-one-hot-region",children:"Anti-Pattern: One Hot Region"}),`
`,e.jsx(i.p,{children:"If all your data is being written to one region at a time, then re-read the section on processing timeseries data."}),`
`,e.jsxs(i.p,{children:["Also, if you are pre-splitting regions and all your data is ",e.jsx(i.em,{children:"still"}),` winding up in a single region even though your keys aren't monotonically increasing, confirm that your keyspace actually works with the split strategy. There are a variety of reasons that regions may appear "well split" but won't work with your data. As the HBase client communicates directly with the RegionServers, this can be obtained via `,e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/RegionLocator.html#getRegionLocation(byte%5B%5D)",children:"RegionLocator.getRegionLocation"}),"."]}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/performance#table-creation-pre-creating-regions",children:"Table Creation: Pre-Creating Regions"}),", as well as ",e.jsx(i.a,{href:"/docs/performance#hbase-configurations",children:"HBase Configurations"})]}),`
`,e.jsx(i.h2,{id:"reading-from-hbase",children:"Reading from HBase"}),`
`,e.jsx(i.p,{children:"The mailing list can help if you are having performance issues."}),`
`,e.jsx(i.h3,{id:"scan-caching",children:"Scan Caching"}),`
`,e.jsxs(i.p,{children:["If HBase is used as an input source for a MapReduce job, for example, make sure that the input ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Scan.html",children:"Scan"})," instance to the MapReduce job has ",e.jsx(i.code,{children:"setCaching"})," set to something greater than the default (which is 1). Using the default value means that the map-task will make call back to the region-server for every record processed. Setting this value to 500, for example, will transfer 500 rows at a time to the client to be processed. There is a cost/benefit to have the cache value be large because it costs more in memory for both client and RegionServer, so bigger isn't always better."]}),`
`,e.jsx(i.h4,{id:"scan-caching-in-mapreduce-jobs",children:"Scan Caching in MapReduce Jobs"}),`
`,e.jsx(i.p,{children:"Scan settings in MapReduce jobs deserve special attention. Timeouts can result (e.g., UnknownScannerException) in Map tasks if it takes longer to process a batch of records before the client goes back to the RegionServer for the next set of data. This problem can occur because there is non-trivial processing occurring per row. If you process rows quickly, set caching higher. If you process rows more slowly (e.g., lots of transformations per row, writes), then set caching lower."}),`
`,e.jsx(i.p,{children:"Timeouts can also happen in a non-MapReduce use case (i.e., single threaded HBase client doing a Scan), but the processing that is often performed in MapReduce jobs tends to exacerbate this issue."}),`
`,e.jsx(i.h3,{id:"scan-attribute-selection",children:"Scan Attribute Selection"}),`
`,e.jsxs(i.p,{children:["Whenever a Scan is used to process large numbers of rows (and especially when used as a MapReduce source), be aware of which attributes are selected. If ",e.jsx(i.code,{children:"scan.addFamily"})," is called then ",e.jsx(i.em,{children:"all"})," of the attributes in the specified ColumnFamily will be returned to the client. If only a small number of the available attributes are to be processed, then only those attributes should be specified in the input scan because attribute over-selection is a non-trivial performance penalty over large datasets."]}),`
`,e.jsx(i.h3,{id:"avoid-scan-seeks",children:"Avoid scan seeks"}),`
`,e.jsxs(i.p,{children:["When columns are selected explicitly with ",e.jsx(i.code,{children:"scan.addColumn"}),", HBase will schedule seek operations to seek between the selected columns. When rows have few columns and each column has only a few versions this can be inefficient. A seek operation is generally slower if does not seek at least past 5-10 columns/versions or 512-1024 bytes."]}),`
`,e.jsxs(i.p,{children:["In order to opportunistically look ahead a few columns/versions to see if the next column/version can be found that way before a seek operation is scheduled, a new attribute ",e.jsx(i.code,{children:"Scan.HINT_LOOKAHEAD"})," can be set on the Scan object. The following code instructs the RegionServer to attempt two iterations of next before a seek is scheduled:"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"Scan scan "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" Scan"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"scan."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"addColumn"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(...);"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"scan."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"setAttribute"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(Scan.HINT_LOOKAHEAD, Bytes."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"toBytes"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"("}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:"2"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"));"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"table."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getScanner"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(scan);"})]})]})})}),`
`,e.jsx(i.h3,{id:"mapreduce---input-splits",children:"MapReduce - Input Splits"}),`
`,e.jsxs(i.p,{children:['For MapReduce jobs that use HBase tables as a source, if there a pattern where the "slow" map tasks seem to have the same Input Split (i.e., the RegionServer serving the data), see the Troubleshooting Case Study in ',e.jsx(i.a,{href:"/docs/case-studies#case-study-1-performance-issue-on-a-single-node",children:"Case Study #1 (Performance Issue On A Single Node)"}),"."]}),`
`,e.jsx(i.h3,{id:"close-resultscanners",children:"Close ResultScanners"}),`
`,e.jsxs(i.p,{children:["This isn't so much about improving performance but rather ",e.jsx(i.em,{children:"avoiding"})," performance problems. If you forget to close ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/ResultScanner.html",children:"ResultScanners"})," you can cause problems on the RegionServers. Always have ResultScanner processing enclosed in try/catch blocks."]}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"Scan scan "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:" new"}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:" Scan"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"// set attrs..."})}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"ResultScanner rs "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" table."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"getScanner"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(scan);"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"try"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" {"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"  for"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" (Result r "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" rs."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"next"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"(); r "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"!="}),e.jsx(i.span,{style:{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},children:" null"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"; r "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"="}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" rs."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"next"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"()) {"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"    // process result..."})}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  }"})}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"} "}),e.jsx(i.span,{style:{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},children:"finally"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:" {"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  rs."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"close"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();  "}),e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"// always close the ResultScanner!"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"}"})}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"table."}),e.jsx(i.span,{style:{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},children:"close"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"();"})]})]})})}),`
`,e.jsx(i.h3,{id:"performance-reading-from-hbase-block-cache",children:"Block Cache"}),`
`,e.jsxs(i.p,{children:[e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Scan.html",children:"Scan"})," instances can be set to use the block cache in the RegionServer via the ",e.jsx(i.code,{children:"setCacheBlocks"})," method. For input Scans to MapReduce jobs, this should be ",e.jsx(i.code,{children:"false"}),". For frequently accessed rows, it is advisable to use the block cache."]}),`
`,e.jsxs(i.p,{children:["Cache more data by moving your Block Cache off-heap. See ",e.jsx(i.a,{href:"/docs/architecture/regionserver#off-heap-block-cache",children:"Off-heap Block Cache"})]}),`
`,e.jsx(i.h3,{id:"optimal-loading-of-row-keys",children:"Optimal Loading of Row Keys"}),`
`,e.jsxs(i.p,{children:["When performing a table ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Scan.html",children:"scan"})," where only the row keys are needed (no families, qualifiers, values or timestamps), add a FilterList with a ",e.jsx(i.code,{children:"MUST_PASS_ALL"})," operator to the scanner using ",e.jsx(i.code,{children:"setFilter"}),". The filter list should include both a ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html",children:"FirstKeyOnlyFilter"})," and a ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/filter/KeyOnlyFilter.html",children:"KeyOnlyFilter"}),". Using this filter combination will result in a worst case scenario of a RegionServer reading a single value from disk and minimal network traffic to the client for a single row."]}),`
`,e.jsx(i.h3,{id:"concurrency-monitor-data-spread",children:"Concurrency: Monitor Data Spread"}),`
`,e.jsx(i.p,{children:"When performing a high number of concurrent reads, monitor the data spread of the target tables. If the target table(s) have too few regions then the reads could likely be served from too few nodes."}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"/docs/performance#table-creation-pre-creating-regions",children:"Table Creation: Pre-Creating Regions"}),", as well as ",e.jsx(i.a,{href:"/docs/performance#hbase-configurations",children:"HBase Configurations"})]}),`
`,e.jsx(i.h3,{id:"bloom-filters-1",children:"Bloom Filters"}),`
`,e.jsx(i.p,{children:"Enabling Bloom Filters can save your having to go to disk and can help improve read latencies."}),`
`,e.jsxs(i.p,{children:[e.jsx(i.a,{href:"http://en.wikipedia.org/wiki/Bloom_filter",children:"Bloom filters"})," were developed over in ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-1200",children:"HBase-1200 Add bloomfilters"}),". For description of the development process — why static blooms rather than dynamic — and for an overview of the unique properties that pertain to blooms in HBase, as well as possible future directions, see the ",e.jsx(i.em,{children:"Development Process"})," section of the document ",e.jsx(i.a,{href:"https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf",children:"BloomFilters in HBase"})," attached to ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-1200",children:"HBASE-1200"}),". The bloom filters described here are actually version two of blooms in HBase. In versions up to 0.19.x, HBase had a dynamic bloom option based on work done by the ",e.jsx(i.a,{href:"http://www.onelab.org",children:"European Commission One-Lab Project 034819"}),". The core of the HBase bloom work was later pulled up into Hadoop to implement org.apache.hadoop.io.BloomMapFile. Version 1 of HBase blooms never worked that well. Version 2 is a rewrite from scratch though again it starts with the one-lab work."]}),`
`,e.jsxs(i.p,{children:["See also ",e.jsx(i.a,{href:"/docs/performance#bloom-filters",children:"Bloom Filters"}),"."]}),`
`,e.jsx(i.h4,{id:"bloom-storefile-footprint",children:"Bloom StoreFile footprint"}),`
`,e.jsxs(i.p,{children:["Bloom filters add an entry to the ",e.jsx(i.code,{children:"StoreFile"})," general ",e.jsx(i.code,{children:"FileInfo"})," data structure and then two extra entries to the ",e.jsx(i.code,{children:"StoreFile"})," metadata section."]}),`
`,e.jsxs(i.p,{children:[e.jsxs(i.strong,{children:["BloomFilter in the ",e.jsx(i.code,{children:"StoreFile"})," ",e.jsx(i.code,{children:"FileInfo"})," data structure"]}),e.jsx(i.br,{}),`
`,e.jsx(i.code,{children:"FileInfo"})," has a ",e.jsx(i.code,{children:"BLOOM_FILTER_TYPE"})," entry which is set to ",e.jsx(i.code,{children:"NONE"}),", ",e.jsx(i.code,{children:"ROW"})," or ",e.jsx(i.code,{children:"ROWCOL."})]}),`
`,e.jsxs(i.p,{children:[e.jsxs(i.strong,{children:["BloomFilter entries in ",e.jsx(i.code,{children:"StoreFile"})," metadata"]}),e.jsx(i.br,{}),`
`,e.jsx(i.code,{children:"BLOOM_FILTER_META"})," holds Bloom Size, Hash Function used, etc. It's small in size and is cached on ",e.jsx(i.code,{children:"StoreFile.Reader"})," load.",e.jsx(i.br,{}),`
`,e.jsx(i.code,{children:"BLOOM_FILTER_DATA"})," is the actual bloomfilter data. Obtained on-demand. Stored in the LRU cache, if it is enabled (It's enabled by default)."]}),`
`,e.jsx(i.h4,{id:"bloom-filter-configuration",children:"Bloom Filter Configuration"}),`
`,e.jsxs(i.p,{children:[e.jsxs(i.strong,{children:[e.jsx(i.code,{children:"io.storefile.bloom.enabled"})," global kill switch"]}),e.jsx(i.br,{}),`
`,e.jsx(i.code,{children:"io.storefile.bloom.enabled"})," in ",e.jsx(i.code,{children:"Configuration"})," serves as the kill switch in case something goes wrong. Default = ",e.jsx(i.code,{children:"true"}),"."]}),`
`,e.jsxs(i.p,{children:[e.jsx(i.code,{children:"io.storefile.bloom.error.rate"}),e.jsx(i.br,{}),`
`,e.jsx(i.code,{children:"io.storefile.bloom.error.rate"})," = average false positive rate. Default = 1%. Decrease rate by ½ (e.g. to .5%) == +1 bit per bloom entry."]}),`
`,e.jsxs(i.p,{children:[e.jsx(i.code,{children:"io.storefile.bloom.max.fold"}),e.jsx(i.br,{}),`
`,e.jsx(i.code,{children:"io.storefile.bloom.max.fold"})," = guaranteed minimum fold rate. Most people should leave this alone. Default = 7, or can collapse to at least 1/128th of original size. See the ",e.jsx(i.em,{children:"Development Process"})," section of the document ",e.jsx(i.a,{href:"https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf",children:"BloomFilters in HBase"})," for more on what this option means."]}),`
`,e.jsx(i.h3,{id:"hedged-reads",children:"Hedged Reads"}),`
`,e.jsxs(i.p,{children:["Hedged reads are a feature of HDFS, introduced in Hadoop 2.4.0 with ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HDFS-5776",children:"HDFS-5776"}),". Normally, a single thread is spawned for each read request. However, if hedged reads are enabled, the client waits some configurable amount of time, and if the read does not return, the client spawns a second read request, against a different block replica of the same data. Whichever read returns first is used, and the other read request is discarded."]}),`
`,e.jsx(i.p,{children:'Hedged reads are "...very good at eliminating outlier datanodes, which in turn makes them very good choice for latency sensitive setups. But, if you are looking for maximizing throughput, hedged reads tend to create load amplification as things get slower in general. In short, the thing to watch out for is the non-graceful performance degradation when you are running close a certain throughput threshold." (Quote from Ashu Pachauri in HBASE-17083).'}),`
`,e.jsx(i.p,{children:"Other concerns to keep in mind while running with hedged reads enabled include:"}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:["They may lead to network congestion. See ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-17083",children:"HBASE-17083"})]}),`
`,e.jsxs(i.li,{children:["Make sure you set the thread pool large enough so as blocking on the pool does not become a bottleneck (Again see ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-17083",children:"HBASE-17083"}),")"]}),`
`]}),`
`,e.jsx(i.p,{children:"(From Yu Li up in HBASE-17083)"}),`
`,e.jsx(i.p,{children:"Because an HBase RegionServer is a HDFS client, you can enable hedged reads in HBase, by adding the following properties to the RegionServer's hbase-site.xml and tuning the values to suit your environment."}),`
`,e.jsx(i.p,{children:e.jsx(i.strong,{children:"Configuration for Hedged Reads"})}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsxs(i.li,{children:[e.jsx(i.code,{children:"dfs.client.hedged.read.threadpool.size"})," - the number of threads dedicated to servicing hedged reads. If this is set to 0 (the default), hedged reads are disabled."]}),`
`,e.jsxs(i.li,{children:[e.jsx(i.code,{children:"dfs.client.hedged.read.threshold.millis"})," - the number of milliseconds to wait before spawning a second read thread."]}),`
`]}),`
`,e.jsx(i.p,{children:e.jsx(i.strong,{children:"Hedged Reads Configuration Example"})}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">dfs.client.hedged.read.threadpool.size</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">20</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">  "}),e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"<!-- 20 threads -->"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">dfs.client.hedged.read.threshold.millis</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">10</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">  "}),e.jsx(i.span,{style:{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},children:"<!-- 10 milliseconds -->"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsxs(i.p,{children:["Use the following metrics to tune the settings for hedged reads on your cluster. See ",e.jsx(i.a,{href:"/docs/operational-management/metrics-and-monitoring",children:"HBase Metrics"})," for more information."]}),`
`,e.jsx(i.p,{children:e.jsx(i.strong,{children:"Metrics for Hedged Reads"})}),`
`,e.jsxs(i.ul,{children:[`
`,e.jsx(i.li,{children:"hedgedReadOps - the number of times hedged read threads have been triggered. This could indicate that read requests are often slow, or that hedged reads are triggered too quickly."}),`
`,e.jsx(i.li,{children:"hedgeReadOpsWin - the number of times the hedged read thread was faster than the original thread. This could indicate that a given RegionServer is having trouble servicing requests."}),`
`,e.jsx(i.li,{children:"hedgedReadOpsInCurThread - the number of times hedged read was rejected from executor and needed to fallback to be executed in current thread. This could indicate that current hedged read thread pool size is not appropriate."}),`
`]}),`
`,e.jsx(i.h2,{id:"deleting-from-hbase",children:"Deleting from HBase"}),`
`,e.jsx(i.h3,{id:"using-hbase-tables-as-queues",children:"Using HBase Tables as Queues"}),`
`,e.jsxs(i.p,{children:["HBase tables are sometimes used as queues. In this case, special care must be taken to regularly perform major compactions on tables used in this manner. As is documented in ",e.jsx(i.a,{href:"/docs/datamodel",children:"Data Model"}),", marking rows as deleted creates additional StoreFiles which then need to be processed on reads. Tombstones only get cleaned up with major compactions."]}),`
`,e.jsxs(i.p,{children:["See also ",e.jsx(i.a,{href:"/docs/architecture/regions#compaction",children:"Compaction"})," and ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Admin.html#majorCompact(org.apache.hadoop.hbase.TableName)",children:"Admin.majorCompact"}),"."]}),`
`,e.jsx(i.h3,{id:"delete-rpc-behavior",children:"Delete RPC Behavior"}),`
`,e.jsxs(i.p,{children:["Be aware that ",e.jsx(i.code,{children:"Table.delete(Delete)"})," doesn't use the writeBuffer. It will execute an RegionServer RPC with each invocation. For a large number of deletes, consider ",e.jsx(i.code,{children:"Table.delete(List)"}),"."]}),`
`,e.jsxs(i.p,{children:["See ",e.jsx(i.a,{href:"https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html#delete(org.apache.hadoop.hbase.client.Delete)",children:"hbase.client.Delete"})]}),`
`,e.jsx(i.h2,{id:"hdfs",children:"HDFS"}),`
`,e.jsxs(i.p,{children:["Because HBase runs on ",e.jsx(i.a,{href:"/docs/architecture/hdfs",children:"HDFS"})," it is important to understand how it works and how it affects HBase."]}),`
`,e.jsx(i.h3,{id:"current-issues-with-low-latency-reads",children:"Current Issues With Low-Latency Reads"}),`
`,e.jsxs(i.p,{children:["The original use-case for HDFS was batch processing. As such, there low-latency reads were historically not a priority. With the increased adoption of Apache HBase this is changing, and several improvements are already in development. See the ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HDFS-1599",children:"Umbrella Jira Ticket for HDFS Improvements for HBase"}),"."]}),`
`,e.jsx(i.h3,{id:"leveraging-local-data",children:"Leveraging local data"}),`
`,e.jsxs(i.p,{children:["Since Hadoop 1.0.0 (also 0.22.1, 0.23.1, CDH3u3 and HDP 1.0) via ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HDFS-2246",children:"HDFS-2246"}),`, it is possible for the DFSClient to take a "short circuit" and read directly from the disk instead of going through the DataNode when the data is local. What this means for HBase is that the RegionServers can read directly off their machine's disks instead of having to open a socket to talk to the DataNode, the former being generally much faster. See JD's `,e.jsx(i.a,{href:"http://files.meetup.com/1350427/hug_ebay_jdcryans.pdf",children:"Performance Talk"}),". Also see ",e.jsx(i.a,{href:"https://lists.apache.org/thread.html/ce2ce3a3bbd20806d0c017b2e7528e78a46ccb87c063831db051949d%401347548325%40%3Cdev.hbase.apache.org%3E",children:"HBase, mail # dev - read short circuit"})," thread for more discussion around short circuit reads."]}),`
`,e.jsxs(i.p,{children:['To enable "short circuit" reads, it will depend on your version of Hadoop. The original shortcircuit read patch was much improved upon in Hadoop 2 in ',e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HDFS-347",children:"HDFS-347"}),". See ",e.jsx(i.a,{href:"http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/",children:"http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/"})," for details on the difference between the old and new implementations. See ",e.jsx(i.a,{href:"http://archive.cloudera.com/cdh4/cdh/4/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html",children:"Hadoop shortcircuit reads configuration page"})," for how to enable the latter, better version of shortcircuit. For example, here is a minimal config. enabling short-circuit reads added to ",e.jsx(i.em,{children:"hbase-site.xml"}),":"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(i.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(i.code,{children:[e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">dfs.client.read.shortcircuit</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">true</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"description"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"    This configuration parameter turns on short-circuit local reads."})}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  </"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"description"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">dfs.domain.socket.path</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">/home/stack/sockets/short_circuit_read_socket_PORT</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"description"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"    Optional.  This is a path to a UNIX domain socket that will be used for"})}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"    communication between the DataNode and local HDFS clients."})}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:'    If the string "_PORT" is present in this path, it will be replaced by the'})}),`
`,e.jsx(i.span,{className:"line",children:e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"    TCP port of the DataNode."})}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  </"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"description"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(i.span,{className:"line",children:[e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(i.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(i.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsx(i.p,{children:"Be careful about permissions for the directory that hosts the shared domain socket; dfsclient will complain if open to other than the hbase user."}),`
`,e.jsxs(i.p,{children:["If you are running on an old Hadoop, one that is without ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HDFS-347",children:"HDFS-347"})," but that has ",e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HDFS-2246",children:"HDFS-2246"}),", you must set two configurations. First, the hdfs-site.xml needs to be amended. Set the property ",e.jsx(i.code,{children:"dfs.block.local-path-access.user"})," to be the ",e.jsx(i.em,{children:"only"})," user that can use the shortcut. This has to be the user that started HBase. Then in hbase-site.xml, set ",e.jsx(i.code,{children:"dfs.client.read.shortcircuit"})," to be ",e.jsx(i.code,{children:"true"})]}),`
`,e.jsx(i.p,{children:"Services — at least the HBase RegionServers — will need to be restarted in order to pick up the new configurations."}),`
`,e.jsx(t,{type:"info",title:"dfs.client.read.shortcircuit.buffer.size",children:e.jsxs(i.p,{children:[`The default for this value is too high when running on a highly trafficked HBase. In HBase, if
this value has not been set, we set it down from the default of 1M to 128k (Since HBase 0.98.0 and
0.96.1). See `,e.jsx(i.a,{href:"https://issues.apache.org/jira/browse/HBASE-8143",children:`HBASE-8143 HBase on Hadoop 2 with local short circuit reads (ssr) causes
OOM`}),`. The Hadoop DFSClient in HBase will
allocate a direct byte buffer of this size for `,e.jsx(i.em,{children:"each"}),` block it has open; given HBase keeps its
HDFS files open all the time, this can add up quickly.`]})}),`
`,e.jsx(i.h3,{id:"performance-comparisons-of-hbase-vs-hdfs",children:"Performance Comparisons of HBase vs. HDFS"}),`
`,e.jsx(i.p,{children:"A fairly common question on the dist-list is why HBase isn't as performant as HDFS files in a batch context (e.g., as a MapReduce source or sink). The short answer is that HBase is doing a lot more than HDFS (e.g., reading the KeyValues, returning the most current row or specified timestamps, etc.), and as such HBase is 4-5 times slower than HDFS in this processing context. There is room for improvement and this gap will, over time, be reduced, but HDFS will always be faster in this use-case."}),`
`,e.jsx(i.h2,{id:"performance-amazon-ec2",children:"Amazon EC2"}),`
`,e.jsx(i.p,{children:"Performance questions are common on Amazon EC2 environments because it is a shared environment. You will not see the same throughput as a dedicated server. In terms of running tests on EC2, run them several times for the same reason (i.e., it's a shared environment and you don't know what else is happening on the server)."}),`
`,e.jsx(i.p,{children:"If you are running on EC2 and post performance questions on the dist-list, please state this fact up-front that because EC2 issues are practically a separate class of performance issues."}),`
`,e.jsx(i.h2,{id:"collocating-hbase-and-mapreduce",children:"Collocating HBase and MapReduce"}),`
`,e.jsxs(i.p,{children:["It is often recommended to have different clusters for HBase and MapReduce. A better qualification of this is: don't collocate an HBase that serves live requests with a heavy MR workload. OLTP and OLAP-optimized systems have conflicting requirements and one will lose to the other, usually the former. For example, short latency-sensitive disk reads will have to wait in line behind longer reads that are trying to squeeze out as much throughput as possible. MR jobs that write to HBase will also generate flushes and compactions, which will in turn invalidate blocks in the ",e.jsx(i.a,{href:"/docs/architecture/regionserver#architecture-regionserver-block-cache",children:"Block Cache"}),"."]}),`
`,e.jsxs(i.p,{children:["If you need to process the data from your live HBase cluster in MR, you can ship the deltas with ",e.jsx(i.a,{href:"/docs/operational-management/tools#copytable",children:"CopyTable"})," or use replication to get the new data in real time on the OLAP cluster. In the worst case, if you really need to collocate both, set MR to use less Map and Reduce slots than you'd normally configure, possibly just one."]}),`
`,e.jsx(i.p,{children:"When HBase is used for OLAP operations, it's preferable to set it up in a hardened way like configuring the ZooKeeper session timeout higher and giving more memory to the MemStores (the argument being that the Block Cache won't be used much since the workloads are usually long scans)."}),`
`,e.jsx(i.h2,{id:"performance-case-studies",children:"Case Studies"}),`
`,e.jsxs(i.p,{children:["For Performance and Troubleshooting Case Studies, see ",e.jsx(i.a,{href:"/docs/case-studies",children:"Apache HBase Case Studies"}),"."]})]})}function u(s={}){const{wrapper:i}=s.components||{};return i?e.jsx(i,{...s,children:e.jsx(n,{...s})}):n(s)}function a(s,i){throw new Error("Expected component `"+s+"` to be defined: you likely forgot to import, pass, or provide it.")}export{r as _markdown,u as default,h as extractedReferences,l as frontmatter,c as structuredData,d as toc};
