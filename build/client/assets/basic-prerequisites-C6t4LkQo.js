import{j as e}from"./chunk-OIYGIGL5-BFuAKb0n.js";let r=`## Java

HBase runs on the Java Virtual Machine, thus all HBase deployments require a JVM runtime.

The following table summarizes the recommendations of the HBase community with respect to running on various Java versions. The ✅ symbol indicates a base level of testing and willingness to help diagnose and address issues you might run into; these are the expected deployment combinations. An entry of ⚠️ means that there may be challenges with this combination, and you should look for more information before deciding to pursue this as your deployment strategy. The ❌ means this combination does not work; either an older Java version is considered deprecated by the HBase community, or this combination is known to not work. For combinations of newer JDK with older HBase releases, it's likely there are known compatibility issues that cannot be addressed under our compatibility guarantees, making the combination impossible. In some cases, specific guidance on limitations (e.g. whether compiling / unit tests work, specific operational issues, etc) are also noted. Assume any combination not listed here is considered ❌.

<Callout type="warn">
  HBase recommends downstream users rely only on JDK releases that are marked as Long-Term Supported
  (LTS), either from the OpenJDK project or vendors. At the time of this writing, the following JDK
  releases are NOT LTS releases and are NOT tested or advocated for use by the Apache HBase
  community: JDK9, JDK10, JDK12, JDK13, and JDK14. Community discussion around this decision is
  recorded on [HBASE-20264](https://issues.apache.org/jira/browse/HBASE-20264).
</Callout>

<Callout type="tip">
  At this time, all testing performed by the Apache HBase project runs on the HotSpot variant of the
  JVM. When selecting your JDK distribution, please take this into consideration.
</Callout>

**Java support by release line**

| HBase Version | JDK 6 | JDK 7 | JDK 8 | JDK 11 | JDK 17 |
| :-----------: | :---: | :---: | :---: | :----: | :----: |
|   HBase 2.6   |   ❌   |   ❌   |   ✅   |    ✅   |    ✅   |
|   HBase 2.5   |   ❌   |   ❌   |   ✅   |    ✅   |  ⚠️\\*  |
|   HBase 2.4   |   ❌   |   ❌   |   ✅   |    ✅   |    ❌   |
|   HBase 2.3   |   ❌   |   ❌   |   ✅   |  ⚠️\\*  |    ❌   |
| HBase 2.0-2.2 |   ❌   |   ❌   |   ✅   |    ❌   |    ❌   |
|   HBase 1.2+  |   ❌   |   ✅   |   ✅   |    ❌   |    ❌   |
| HBase 1.0-1.1 |   ❌   |   ✅   |   ⚠️  |    ❌   |    ❌   |
|   HBase 0.98  |   ✅   |   ✅   |   ⚠️  |    ❌   |    ❌   |
|   HBase 0.94  |   ✅   |   ✅   |   ❌   |    ❌   |    ❌   |

<Callout type="warn">
  Preliminary support for JDK11 is introduced with HBase 2.3.0, and for JDK17 is introduced with HBase 2.5.x. We will compile and run test suites with JDK11/17 in pre commit checks and nightly checks. We will mark the support as ✅ as long as we have run some ITs with the JDK version and also there are users in the community use the JDK version in real production clusters.

  For JDK11/JDK17 support in HBase, please refer to [HBASE-22972](https://issues.apache.org/jira/browse/HBASE-22972) and [HBASE-26038](https://issues.apache.org/jira/browse/HBASE-26038)

  For JDK11/JDK17 support in Hadoop, which may also affect HBase, please refer to [HADOOP-15338](https://issues.apache.org/jira/browse/HADOOP-15338) and [HADOOP-17177](https://issues.apache.org/jira/browse/HADOOP-17177)
</Callout>

<Callout type="info">
  You must set \`JAVA_HOME\` on each node of your cluster. *hbase-env.sh* provides a handy mechanism
  to do this.
</Callout>

### Operating System Utilities

#### ssh

HBase uses the Secure Shell (ssh) command and utilities extensively to communicate between cluster nodes. Each server in the cluster must be running \`ssh\` so that the Hadoop and HBase daemons can be managed. You must be able to connect to all nodes via SSH, including the local node, from the Master as well as any backup Master, using a shared key rather than a password. You can see the basic methodology for such a set-up in Linux or Unix systems at "[Procedure: Configure Passwordless SSH Access](/docs/getting-started#procedure-configure-passwordless-ssh-access)" chapter. If your cluster nodes use OS X, see the section, [SSH: Setting up Remote Desktop and Enabling Self-Login](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=120730246#RunningHadoopOnOSX10.564-bit\\(Single-NodeCluster\\)-SSH:SettingupRemoteDesktopandEnablingSelf-Login) on the Hadoop wiki.

#### DNS

HBase uses the local hostname to self-report its IP address.

#### NTP

The clocks on cluster nodes should be synchronized. A small amount of variation is acceptable, but larger amounts of skew can cause erratic and unexpected behavior. Time synchronization is one of the first things to check if you see unexplained problems in your cluster. It is recommended that you run a Network Time Protocol (NTP) service, or another time-synchronization mechanism on your cluster and that all nodes look to the same service for time synchronization. See the [Basic NTP Configuration](http://www.tldp.org/LDP/sag/html/basic-ntp-config.html) at *The Linux Documentation Project (TLDP)* to set up NTP.

#### Limits on Number of Files and Processes (ulimit)

Apache HBase is a database. It requires the ability to open a large number of files at once. Many Linux distributions limit the number of files a single user is allowed to open to \`1024\` (or \`256\` on older versions of OS X). You can check this limit on your servers by running the command \`ulimit -n\` when logged in as the user which runs HBase. See [the Troubleshooting section](/docs/troubleshooting#javaioioexceptiontoo-many-open-files) for some of the problems you may experience if the limit is too low. You may also notice errors such as the following:

\`\`\`text
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901
\`\`\`

It is recommended to raise the ulimit to at least 10,000, but more likely 10,240, because the value is usually expressed in multiples of 1024. Each ColumnFamily has at least one StoreFile, and possibly more than six StoreFiles if the region is under load. The number of open files required depends upon the number of ColumnFamilies and the number of regions. The following is a rough formula for calculating the potential number of open files on a RegionServer.

**Calculate the Potential Number of Open Files:**

\`\`\`text
(StoreFiles per ColumnFamily) x (regions per RegionServer)
\`\`\`

For example, assuming that a schema had 3 ColumnFamilies per region with an average of 3 StoreFiles per ColumnFamily, and there are 100 regions per RegionServer, the JVM will open \`3 * 3 * 100 = 900\` file descriptors, not counting open JAR files, configuration files, and others. Opening a file does not take many resources, and the risk of allowing a user to open too many files is minimal.

Another related setting is the number of processes a user is allowed to run at once. In Linux and Unix, the number of processes is set using the \`ulimit -u\` command. This should not be confused with the \`nproc\` command, which controls the number of CPUs available to a given user. Under load, a \`ulimit -u\` that is too low can cause OutOfMemoryError exceptions.

Configuring the maximum number of file descriptors and processes for the user who is running the HBase process is an operating system configuration, rather than an HBase configuration. It is also important to be sure that the settings are changed for the user that actually runs HBase. To see which user started HBase, and that user's ulimit configuration, look at the first line of the HBase log for that instance.

#### Example: \`ulimit\` Settings on Ubuntu \\[!toc]

To configure ulimit settings on Ubuntu, edit */etc/security/limits.conf*, which is a space-delimited file with four columns. Refer to the man page for *limits.conf* for details about the format of this file. In the following example, the first line sets both soft and hard limits for the number of open files (nofile) to 32768 for the operating system user with the username hadoop. The second line sets the number of processes to 32000 for the same user.

\`\`\`text
hadoop  -       nofile  32768
hadoop  -       nproc   32000
\`\`\`

The settings are only applied if the Pluggable Authentication Module (PAM) environment is directed to use them. To configure PAM to use these limits, be sure that the */etc/pam.d/common-session* file contains the following line:

\`\`\`text
session required  pam_limits.so
\`\`\`

#### Linux Shell

All of the shell scripts that come with HBase rely on the [GNU Bash](http://www.gnu.org/software/bash) shell.

#### Windows

Running production systems on Windows machines is not recommended.

## Hadoop

The following table summarizes the versions of [Hadoop](https://hadoop.apache.org) supported with each version of HBase. Older versions not appearing in this table are considered unsupported and likely missing necessary features, while newer versions are untested but may be suitable.

Based on the version of HBase, you should select the most appropriate version of Hadoop. You can use Apache Hadoop, or a vendor's distribution of Hadoop. No distinction is made here. See [the Hadoop wiki](https://cwiki.apache.org/confluence/display/HADOOP2/Distributions+and+Commercial+Support) for information about vendors of Hadoop.

<Callout type="tip">
  Comparing to Hadoop 1.x, Hadoop 2.x is faster and includes features, such as short-circuit reads (see [Leveraging local data](/docs/performance#leveraging-local-data)), which will help improve your HBase random read profile. Hadoop 2.x also includes important bug fixes that will improve your overall HBase experience. HBase does not support running with earlier versions of Hadoop. See the table below for requirements specific to different HBase versions.

  Today, Hadoop 3.x is recommended as the last Hadoop 2.x release 2.10.2 was released years ago, and there is no release for Hadoop 2.x for a very long time, although the Hadoop community does not officially EOL Hadoop 2.x yet.
</Callout>

Use the following legend to interpret these tables:

* ✅ = Tested to be fully-functional
* ❌ = Known to not be fully-functional, or there are [CVEs](https://hadoop.apache.org/cve_list.html) so we drop the support in newer minor releases
* ⚠️ = Not tested, may/may-not function

|                    | HBase-2.5.x | HBase-2.6.x |
| ------------------ | ----------- | ----------- |
| Hadoop-2.10.\\[0-1] | ❌           | ❌           |
| Hadoop-2.10.2+     | ✅           | ✅           |
| Hadoop-3.1.0       | ❌           | ❌           |
| Hadoop-3.1.1+      | ❌           | ❌           |
| Hadoop-3.2.\\[0-2]  | ❌           | ❌           |
| Hadoop-3.2.3+      | ✅           | ❌           |
| Hadoop-3.3.\\[0-1]  | ❌           | ❌           |
| Hadoop-3.3.\\[2-4]  | ✅           | ❌           |
| Hadoop-3.3.5+      | ✅           | ✅           |
| Hadoop-3.4.0+      | ✅ (2.5.11+) | ✅ (2.6.2+)  |

### Hadoop version support matrix for active release lines

|               | HBase-2.3.x | HBase-2.4.x |
| ------------- | ----------- | ----------- |
| Hadoop-2.10.x | ✅           | ✅           |
| Hadoop-3.1.0  | ❌           | ❌           |
| Hadoop-3.1.1+ | ✅           | ✅           |
| Hadoop-3.2.x  | ✅           | ✅           |
| Hadoop-3.3.x  | ✅           | ✅           |

### Hadoop version support matrix for EOM 2.3+ release lines

|                   | HBase-2.0.x | HBase-2.1.x | HBase-2.2.x |
| ----------------- | ----------- | ----------- | ----------- |
| Hadoop-2.6.1+     | ✅           | ❌           | ❌           |
| Hadoop-2.7.\\[0-6] | ❌           | ❌           | ❌           |
| Hadoop-2.7.7+     | ✅           | ✅           | ❌           |
| Hadoop-2.8.\\[0-2] | ❌           | ❌           | ❌           |
| Hadoop-2.8.\\[3-4] | ✅           | ✅           | ❌           |
| Hadoop-2.8.5+     | ✅           | ✅           | ✅           |
| Hadoop-2.9.\\[0-1] | ⚠️          | ❌           | ❌           |
| Hadoop-2.9.2+     | ⚠️          | ⚠️          | ✅           |
| Hadoop-3.0.\\[0-2] | ❌           | ❌           | ❌           |
| Hadoop-3.0.3+     | ❌           | ✅           | ❌           |
| Hadoop-3.1.0      | ❌           | ❌           | ❌           |
| Hadoop-3.1.1+     | ❌           | ✅           | ✅           |

### Hadoop version support matrix for EOM 2.x release lines

|                   | HBase-1.5.x | HBase-1.6.x | HBase-1.7.x |
| ----------------- | ----------- | ----------- | ----------- |
| Hadoop-2.7.7+     | ✅           | ❌           | ❌           |
| Hadoop-2.8.\\[0-4] | ❌           | ❌           | ❌           |
| Hadoop-2.8.5+     | ✅           | ✅           | ✅           |
| Hadoop-2.9.\\[0-1] | ❌           | ❌           | ❌           |
| Hadoop-2.9.2+     | ✅           | ✅           | ✅           |
| Hadoop-2.10.x     | ⚠️          | ✅           | ✅           |

### Hadoop version support matrix for EOM 1.5+ release lines

|               | HBase-1.0.x (Hadoop 1.x is NOT supported) | HBase-1.1.x | HBase-1.2.x | HBase-1.3.x | HBase-1.4.x |
| ------------- | ----------------------------------------- | ----------- | ----------- | ----------- | ----------- |
| Hadoop-2.4.x  | ✅                                         | ✅           | ✅           | ✅           | ❌           |
| Hadoop-2.5.x  | ✅                                         | ✅           | ✅           | ✅           | ❌           |
| Hadoop-2.6.0  | ❌                                         | ❌           | ❌           | ❌           | ❌           |
| Hadoop-2.6.1+ | ⚠️                                        | ⚠️          | ✅           | ✅           | ❌           |
| Hadoop-2.7.0  | ❌                                         | ❌           | ❌           | ❌           | ❌           |
| Hadoop-2.7.1+ | ⚠️                                        | ⚠️          | ✅           | ✅           | ✅           |

### Hadoop version support matrix for EOM 1.x release lines

|                    | HBase-0.92.x | HBase-0.94.x | HBase-0.96.x | HBase-0.98.x (Support for Hadoop 1.1+ is deprecated.) |
| ------------------ | ------------ | ------------ | ------------ | ----------------------------------------------------- |
| Hadoop-0.20.205    | ✅            | ❌            | ❌            | ❌                                                     |
| Hadoop-0.22.x      | ✅            | ❌            | ❌            | ❌                                                     |
| Hadoop-1.0.x       | ❌            | ❌            | ❌            | ❌                                                     |
| Hadoop-1.1.x       | ⚠️           | ✅            | ✅            | ⚠️                                                    |
| Hadoop-0.23.x      | ❌            | ✅            | ⚠️           | ❌                                                     |
| Hadoop-2.0.x-alpha | ❌            | ⚠️           | ❌            | ❌                                                     |
| Hadoop-2.1.0-beta  | ❌            | ⚠️           | ✅            | ❌                                                     |
| Hadoop-2.2.0       | ❌            | ⚠️           | ✅            | ✅                                                     |
| Hadoop-2.3.x       | ❌            | ⚠️           | ✅            | ✅                                                     |
| Hadoop-2.4.x       | ❌            | ⚠️           | ✅            | ✅                                                     |
| Hadoop-2.5.x       | ❌            | ⚠️           | ✅            | ✅                                                     |

### Hadoop version support matrix for EOM pre-1.0 release lines

<Callout type="idea">
  Starting around the time of Hadoop version 2.7.0, the Hadoop PMC got into the habit of calling out
  new minor releases on their major version 2 release line as not stable / production ready. As
  such, HBase expressly advises downstream users to avoid running on top of these releases. Note
  that additionally the 2.8.1 release was given the same caveat by the Hadoop PMC. For reference,
  see the release announcements for [Apache Hadoop
  2.7.0](https://s.apache.org/hadoop-2.7.0-announcement), [Apache Hadoop
  2.8.0](https://s.apache.org/hadoop-2.8.0-announcement), [Apache Hadoop
  2.8.1](https://s.apache.org/hadoop-2.8.1-announcement), and [Apache Hadoop
  2.9.0](https://s.apache.org/hadoop-2.9.0-announcement).
</Callout>

<Callout type="idea">
  The Hadoop PMC called out the 3.1.0 release as not stable / production ready. As such, HBase
  expressly advises downstream users to avoid running on top of this release. For reference, see the
  [release announcement for Hadoop 3.1.0](https://s.apache.org/hadoop-3.1.0-announcement).
</Callout>

<Callout type="info">
  Because HBase depends on Hadoop, it bundles Hadoop jars under its *lib* directory. The bundled
  jars are ONLY for use in stand-alone mode. In distributed mode, it is *critical* that the version
  of Hadoop that is out on your cluster match what is under HBase. Replace the hadoop jars found in
  the HBase lib directory with the equivalent hadoop jars from the version you are running on your
  cluster to avoid version mismatch issues. Make sure you replace the jars under HBase across your
  whole cluster. Hadoop version mismatch issues have various manifestations. Check for mismatch if
  HBase appears hung.
</Callout>

### Hadoop 3 Support for the HBase Binary Releases and Maven Artifacts

For HBase 2.5.1 and earlier, the official HBase binary releases and Maven artifacts were built with Hadoop 2.x.

Starting with HBase 2.5.2, HBase provides binary releases and Maven artifacts built with both Hadoop 2.x and Hadoop 3.x. The Hadoop 2 artifacts do not have any version suffix, the Hadoop 3 artifacts add the \`-hadoop-3\` suffix to the version. i.e. \`hbase-2.5.2-bin.tar.gz.asc\` is the Binary release built with Hadoop2, and \`hbase-2.5.2-hadoop3-bin.tar.gz\` is the release built with Hadoop 3.

### Hadoop 3 version policy

Each HBase release has a default Hadoop 3 version. This is used when the Hadoop 3 version is not specified during build, and for building the official binary releases and artifacts. Generally when a new minor version is released (i.e. 2.5.0) the default version is set to the latest supported Hadoop 3 version at the start of the release process.

Up to HBase 2.5.10 and 2.6.1 even if HBase added support for newer Hadoop 3 releases in a patch release, the default Hadoop 3 version (and the one used in the official binary releases) was not updated. This simplified upgrading, but meant that HBase releases often included old unfixed CVEs both from Hadoop and Hadoop's dependencies, even when newer Hadoop releases with fixes were available.

Starting with HBase 2.5.11 and 2.6.2, the default Hadoop 3 version is always set to the latest supported Hadoop 3 version, and is also used for the \`-hadoop3\` binary releases and artifacts. This will drastically reduce the number of known CVEs shipped in the HBase binary releases, and make sure that all fixes and improvements in Hadoop are included.

### \`dfs.datanode.max.transfer.threads\`

An HDFS DataNode has an upper bound on the number of files that it will serve at any one time. Before doing any loading, make sure you have configured Hadoop's *conf/hdfs-site.xml*, setting the \`dfs.datanode.max.transfer.threads\` value to at least the following:

\`\`\`xml
<property>
  <name>dfs.datanode.max.transfer.threads</name>
  <value>4096</value>
</property>
\`\`\`

Be sure to restart your HDFS after making the above configuration.

Not having this configuration in place makes for strange-looking failures. One manifestation is a complaint about missing blocks. For example:

\`\`\`text
10/12/08 20:10:31 INFO hdfs.DFSClient: Could not obtain block
          blk_XXXXXXXXXXXXXXXXXXXXXX_YYYYYYYY from any node: java.io.IOException: No live nodes
          contain current block. Will get new block locations from namenode and retry...
\`\`\`

See also [Case Studies](/docs/case-studies#case-study-4-maxtransferthreads-config) and note that this property was previously known as \`dfs.datanode.max.xcievers\` (e.g. [Hadoop HDFS: Deceived by Xciever](http://ccgtech.blogspot.com/2010/02/hadoop-hdfs-deceived-by-xciever.html)).

## ZooKeeper Requirements

An Apache ZooKeeper quorum is required. The exact version depends on your version of HBase, though the minimum ZooKeeper version is 3.4.x due to the \`useMulti\` feature made default in 1.0.0 (see [HBASE-16598](https://issues.apache.org/jira/browse/HBASE-16598)).
`,d={title:"Basic Prerequisites",description:"This section lists required services and some required system configuration."},h=[{href:"https://issues.apache.org/jira/browse/HBASE-20264"},{href:"https://issues.apache.org/jira/browse/HBASE-22972"},{href:"https://issues.apache.org/jira/browse/HBASE-26038"},{href:"https://issues.apache.org/jira/browse/HADOOP-15338"},{href:"https://issues.apache.org/jira/browse/HADOOP-17177"},{href:"/docs/getting-started#procedure-configure-passwordless-ssh-access"},{href:"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=120730246#RunningHadoopOnOSX10.564-bit(Single-NodeCluster)-SSH:SettingupRemoteDesktopandEnablingSelf-Login"},{href:"http://www.tldp.org/LDP/sag/html/basic-ntp-config.html"},{href:"/docs/troubleshooting#javaioioexceptiontoo-many-open-files"},{href:"http://www.gnu.org/software/bash"},{href:"https://hadoop.apache.org"},{href:"https://cwiki.apache.org/confluence/display/HADOOP2/Distributions+and+Commercial+Support"},{href:"/docs/performance#leveraging-local-data"},{href:"https://hadoop.apache.org/cve_list.html"},{href:"https://s.apache.org/hadoop-2.7.0-announcement"},{href:"https://s.apache.org/hadoop-2.8.0-announcement"},{href:"https://s.apache.org/hadoop-2.8.1-announcement"},{href:"https://s.apache.org/hadoop-2.9.0-announcement"},{href:"https://s.apache.org/hadoop-3.1.0-announcement"},{href:"/docs/case-studies#case-study-4-maxtransferthreads-config"},{href:"http://ccgtech.blogspot.com/2010/02/hadoop-hdfs-deceived-by-xciever.html"},{href:"https://issues.apache.org/jira/browse/HBASE-16598"}],l={contents:[{heading:"configuration-basic-prerequisites-java",content:"HBase runs on the Java Virtual Machine, thus all HBase deployments require a JVM runtime."},{heading:"configuration-basic-prerequisites-java",content:"The following table summarizes the recommendations of the HBase community with respect to running on various Java versions. The ✅ symbol indicates a base level of testing and willingness to help diagnose and address issues you might run into; these are the expected deployment combinations. An entry of ⚠️ means that there may be challenges with this combination, and you should look for more information before deciding to pursue this as your deployment strategy. The ❌ means this combination does not work; either an older Java version is considered deprecated by the HBase community, or this combination is known to not work. For combinations of newer JDK with older HBase releases, it's likely there are known compatibility issues that cannot be addressed under our compatibility guarantees, making the combination impossible. In some cases, specific guidance on limitations (e.g. whether compiling / unit tests work, specific operational issues, etc) are also noted. Assume any combination not listed here is considered ❌."},{heading:"configuration-basic-prerequisites-java",content:"type: warn"},{heading:"configuration-basic-prerequisites-java",content:`HBase recommends downstream users rely only on JDK releases that are marked as Long-Term Supported
(LTS), either from the OpenJDK project or vendors. At the time of this writing, the following JDK
releases are NOT LTS releases and are NOT tested or advocated for use by the Apache HBase
community: JDK9, JDK10, JDK12, JDK13, and JDK14. Community discussion around this decision is
recorded on HBASE-20264.`},{heading:"configuration-basic-prerequisites-java",content:"type: tip"},{heading:"configuration-basic-prerequisites-java",content:`At this time, all testing performed by the Apache HBase project runs on the HotSpot variant of the
JVM. When selecting your JDK distribution, please take this into consideration.`},{heading:"configuration-basic-prerequisites-java",content:"Java support by release line"},{heading:"configuration-basic-prerequisites-java",content:"HBase Version"},{heading:"configuration-basic-prerequisites-java",content:"JDK 6"},{heading:"configuration-basic-prerequisites-java",content:"JDK 7"},{heading:"configuration-basic-prerequisites-java",content:"JDK 8"},{heading:"configuration-basic-prerequisites-java",content:"JDK 11"},{heading:"configuration-basic-prerequisites-java",content:"JDK 17"},{heading:"configuration-basic-prerequisites-java",content:"HBase 2.6"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"HBase 2.5"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"⚠️*"},{heading:"configuration-basic-prerequisites-java",content:"HBase 2.4"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"HBase 2.3"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"⚠️*"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"HBase 2.0-2.2"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"HBase 1.2+"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"HBase 1.0-1.1"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"⚠️"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"HBase 0.98"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"⚠️"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"HBase 0.94"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"✅"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"❌"},{heading:"configuration-basic-prerequisites-java",content:"type: warn"},{heading:"configuration-basic-prerequisites-java",content:"Preliminary support for JDK11 is introduced with HBase 2.3.0, and for JDK17 is introduced with HBase 2.5.x. We will compile and run test suites with JDK11/17 in pre commit checks and nightly checks. We will mark the support as ✅ as long as we have run some ITs with the JDK version and also there are users in the community use the JDK version in real production clusters."},{heading:"configuration-basic-prerequisites-java",content:"For JDK11/JDK17 support in HBase, please refer to HBASE-22972 and HBASE-26038"},{heading:"configuration-basic-prerequisites-java",content:"For JDK11/JDK17 support in Hadoop, which may also affect HBase, please refer to HADOOP-15338 and HADOOP-17177"},{heading:"configuration-basic-prerequisites-java",content:"type: info"},{heading:"configuration-basic-prerequisites-java",content:`You must set JAVA_HOME on each node of your cluster. hbase-env.sh provides a handy mechanism
to do this.`},{heading:"ssh",content:'HBase uses the Secure Shell (ssh) command and utilities extensively to communicate between cluster nodes. Each server in the cluster must be running ssh so that the Hadoop and HBase daemons can be managed. You must be able to connect to all nodes via SSH, including the local node, from the Master as well as any backup Master, using a shared key rather than a password. You can see the basic methodology for such a set-up in Linux or Unix systems at "Procedure: Configure Passwordless SSH Access" chapter. If your cluster nodes use OS X, see the section, SSH: Setting up Remote Desktop and Enabling Self-Login on the Hadoop wiki.'},{heading:"dns",content:"HBase uses the local hostname to self-report its IP address."},{heading:"ntp",content:"The clocks on cluster nodes should be synchronized. A small amount of variation is acceptable, but larger amounts of skew can cause erratic and unexpected behavior. Time synchronization is one of the first things to check if you see unexplained problems in your cluster. It is recommended that you run a Network Time Protocol (NTP) service, or another time-synchronization mechanism on your cluster and that all nodes look to the same service for time synchronization. See the Basic NTP Configuration at The Linux Documentation Project (TLDP) to set up NTP."},{heading:"limits-on-number-of-files-and-processes-ulimit",content:"Apache HBase is a database. It requires the ability to open a large number of files at once. Many Linux distributions limit the number of files a single user is allowed to open to 1024 (or 256 on older versions of OS X). You can check this limit on your servers by running the command ulimit -n when logged in as the user which runs HBase. See the Troubleshooting section for some of the problems you may experience if the limit is too low. You may also notice errors such as the following:"},{heading:"limits-on-number-of-files-and-processes-ulimit",content:"It is recommended to raise the ulimit to at least 10,000, but more likely 10,240, because the value is usually expressed in multiples of 1024. Each ColumnFamily has at least one StoreFile, and possibly more than six StoreFiles if the region is under load. The number of open files required depends upon the number of ColumnFamilies and the number of regions. The following is a rough formula for calculating the potential number of open files on a RegionServer."},{heading:"limits-on-number-of-files-and-processes-ulimit",content:"Calculate the Potential Number of Open Files:"},{heading:"limits-on-number-of-files-and-processes-ulimit",content:"For example, assuming that a schema had 3 ColumnFamilies per region with an average of 3 StoreFiles per ColumnFamily, and there are 100 regions per RegionServer, the JVM will open 3 * 3 * 100 = 900 file descriptors, not counting open JAR files, configuration files, and others. Opening a file does not take many resources, and the risk of allowing a user to open too many files is minimal."},{heading:"limits-on-number-of-files-and-processes-ulimit",content:"Another related setting is the number of processes a user is allowed to run at once. In Linux and Unix, the number of processes is set using the ulimit -u command. This should not be confused with the nproc command, which controls the number of CPUs available to a given user. Under load, a ulimit -u that is too low can cause OutOfMemoryError exceptions."},{heading:"limits-on-number-of-files-and-processes-ulimit",content:"Configuring the maximum number of file descriptors and processes for the user who is running the HBase process is an operating system configuration, rather than an HBase configuration. It is also important to be sure that the settings are changed for the user that actually runs HBase. To see which user started HBase, and that user's ulimit configuration, look at the first line of the HBase log for that instance."},{heading:"example-ulimit-settings-on-ubuntu-toc",content:"To configure ulimit settings on Ubuntu, edit /etc/security/limits.conf, which is a space-delimited file with four columns. Refer to the man page for limits.conf for details about the format of this file. In the following example, the first line sets both soft and hard limits for the number of open files (nofile) to 32768 for the operating system user with the username hadoop. The second line sets the number of processes to 32000 for the same user."},{heading:"example-ulimit-settings-on-ubuntu-toc",content:"The settings are only applied if the Pluggable Authentication Module (PAM) environment is directed to use them. To configure PAM to use these limits, be sure that the /etc/pam.d/common-session file contains the following line:"},{heading:"linux-shell",content:"All of the shell scripts that come with HBase rely on the GNU Bash shell."},{heading:"windows",content:"Running production systems on Windows machines is not recommended."},{heading:"hadoop",content:"The following table summarizes the versions of Hadoop supported with each version of HBase. Older versions not appearing in this table are considered unsupported and likely missing necessary features, while newer versions are untested but may be suitable."},{heading:"hadoop",content:"Based on the version of HBase, you should select the most appropriate version of Hadoop. You can use Apache Hadoop, or a vendor's distribution of Hadoop. No distinction is made here. See the Hadoop wiki for information about vendors of Hadoop."},{heading:"hadoop",content:"type: tip"},{heading:"hadoop",content:"Comparing to Hadoop 1.x, Hadoop 2.x is faster and includes features, such as short-circuit reads (see Leveraging local data), which will help improve your HBase random read profile. Hadoop 2.x also includes important bug fixes that will improve your overall HBase experience. HBase does not support running with earlier versions of Hadoop. See the table below for requirements specific to different HBase versions."},{heading:"hadoop",content:"Today, Hadoop 3.x is recommended as the last Hadoop 2.x release 2.10.2 was released years ago, and there is no release for Hadoop 2.x for a very long time, although the Hadoop community does not officially EOL Hadoop 2.x yet."},{heading:"hadoop",content:"Use the following legend to interpret these tables:"},{heading:"hadoop",content:"✅ = Tested to be fully-functional"},{heading:"hadoop",content:"❌ = Known to not be fully-functional, or there are CVEs so we drop the support in newer minor releases"},{heading:"hadoop",content:"⚠️ = Not tested, may/may-not function"},{heading:"hadoop",content:"HBase-2.5.x"},{heading:"hadoop",content:"HBase-2.6.x"},{heading:"hadoop",content:"Hadoop-2.10.[0-1]"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"Hadoop-2.10.2+"},{heading:"hadoop",content:"✅"},{heading:"hadoop",content:"✅"},{heading:"hadoop",content:"Hadoop-3.1.0"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"Hadoop-3.1.1+"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"Hadoop-3.2.[0-2]"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"Hadoop-3.2.3+"},{heading:"hadoop",content:"✅"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"Hadoop-3.3.[0-1]"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"Hadoop-3.3.[2-4]"},{heading:"hadoop",content:"✅"},{heading:"hadoop",content:"❌"},{heading:"hadoop",content:"Hadoop-3.3.5+"},{heading:"hadoop",content:"✅"},{heading:"hadoop",content:"✅"},{heading:"hadoop",content:"Hadoop-3.4.0+"},{heading:"hadoop",content:"✅ (2.5.11+)"},{heading:"hadoop",content:"✅ (2.6.2+)"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"HBase-2.3.x"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"HBase-2.4.x"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"Hadoop-2.10.x"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"Hadoop-3.1.0"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"Hadoop-3.1.1+"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"Hadoop-3.2.x"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"Hadoop-3.3.x"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-active-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"HBase-2.0.x"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"HBase-2.1.x"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"HBase-2.2.x"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-2.6.1+"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-2.7.[0-6]"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-2.7.7+"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-2.8.[0-2]"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-2.8.[3-4]"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-2.8.5+"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-2.9.[0-1]"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-2.9.2+"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-3.0.[0-2]"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-3.0.3+"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-3.1.0"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop-3.1.1+"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"HBase-1.5.x"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"HBase-1.6.x"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"HBase-1.7.x"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"Hadoop-2.7.7+"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"Hadoop-2.8.[0-4]"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"Hadoop-2.8.5+"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"Hadoop-2.9.[0-1]"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"Hadoop-2.9.2+"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"Hadoop-2.10.x"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"HBase-1.0.x (Hadoop 1.x is NOT supported)"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"HBase-1.1.x"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"HBase-1.2.x"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"HBase-1.3.x"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"HBase-1.4.x"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"Hadoop-2.4.x"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"Hadoop-2.5.x"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"Hadoop-2.6.0"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"Hadoop-2.6.1+"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"Hadoop-2.7.0"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"Hadoop-2.7.1+"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"HBase-0.92.x"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"HBase-0.94.x"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"HBase-0.96.x"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"HBase-0.98.x (Support for Hadoop 1.1+ is deprecated.)"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop-0.20.205"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop-0.22.x"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop-1.0.x"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop-1.1.x"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop-0.23.x"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop-2.0.x-alpha"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop-2.1.0-beta"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop-2.2.0"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop-2.3.x"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop-2.4.x"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop-2.5.x"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"❌"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"⚠️"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"✅"},{heading:"hadoop-version-support-matrix-for-eom-pre-10-release-lines",content:"type: idea"},{heading:"hadoop-version-support-matrix-for-eom-pre-10-release-lines",content:`Starting around the time of Hadoop version 2.7.0, the Hadoop PMC got into the habit of calling out
new minor releases on their major version 2 release line as not stable / production ready. As
such, HBase expressly advises downstream users to avoid running on top of these releases. Note
that additionally the 2.8.1 release was given the same caveat by the Hadoop PMC. For reference,
see the release announcements for Apache Hadoop
2.7.0, Apache Hadoop
2.8.0, Apache Hadoop
2.8.1, and Apache Hadoop
2.9.0.`},{heading:"hadoop-version-support-matrix-for-eom-pre-10-release-lines",content:"type: idea"},{heading:"hadoop-version-support-matrix-for-eom-pre-10-release-lines",content:`The Hadoop PMC called out the 3.1.0 release as not stable / production ready. As such, HBase
expressly advises downstream users to avoid running on top of this release. For reference, see the
release announcement for Hadoop 3.1.0.`},{heading:"hadoop-version-support-matrix-for-eom-pre-10-release-lines",content:"type: info"},{heading:"hadoop-version-support-matrix-for-eom-pre-10-release-lines",content:`Because HBase depends on Hadoop, it bundles Hadoop jars under its lib directory. The bundled
jars are ONLY for use in stand-alone mode. In distributed mode, it is critical that the version
of Hadoop that is out on your cluster match what is under HBase. Replace the hadoop jars found in
the HBase lib directory with the equivalent hadoop jars from the version you are running on your
cluster to avoid version mismatch issues. Make sure you replace the jars under HBase across your
whole cluster. Hadoop version mismatch issues have various manifestations. Check for mismatch if
HBase appears hung.`},{heading:"hadoop-3-support-for-the-hbase-binary-releases-and-maven-artifacts",content:"For HBase 2.5.1 and earlier, the official HBase binary releases and Maven artifacts were built with Hadoop 2.x."},{heading:"hadoop-3-support-for-the-hbase-binary-releases-and-maven-artifacts",content:"Starting with HBase 2.5.2, HBase provides binary releases and Maven artifacts built with both Hadoop 2.x and Hadoop 3.x. The Hadoop 2 artifacts do not have any version suffix, the Hadoop 3 artifacts add the -hadoop-3 suffix to the version. i.e. hbase-2.5.2-bin.tar.gz.asc is the Binary release built with Hadoop2, and hbase-2.5.2-hadoop3-bin.tar.gz is the release built with Hadoop 3."},{heading:"hadoop-3-version-policy",content:"Each HBase release has a default Hadoop 3 version. This is used when the Hadoop 3 version is not specified during build, and for building the official binary releases and artifacts. Generally when a new minor version is released (i.e. 2.5.0) the default version is set to the latest supported Hadoop 3 version at the start of the release process."},{heading:"hadoop-3-version-policy",content:"Up to HBase 2.5.10 and 2.6.1 even if HBase added support for newer Hadoop 3 releases in a patch release, the default Hadoop 3 version (and the one used in the official binary releases) was not updated. This simplified upgrading, but meant that HBase releases often included old unfixed CVEs both from Hadoop and Hadoop's dependencies, even when newer Hadoop releases with fixes were available."},{heading:"hadoop-3-version-policy",content:"Starting with HBase 2.5.11 and 2.6.2, the default Hadoop 3 version is always set to the latest supported Hadoop 3 version, and is also used for the -hadoop3 binary releases and artifacts. This will drastically reduce the number of known CVEs shipped in the HBase binary releases, and make sure that all fixes and improvements in Hadoop are included."},{heading:"dfsdatanodemaxtransferthreads",content:"An HDFS DataNode has an upper bound on the number of files that it will serve at any one time. Before doing any loading, make sure you have configured Hadoop's conf/hdfs-site.xml, setting the dfs.datanode.max.transfer.threads value to at least the following:"},{heading:"dfsdatanodemaxtransferthreads",content:"Be sure to restart your HDFS after making the above configuration."},{heading:"dfsdatanodemaxtransferthreads",content:"Not having this configuration in place makes for strange-looking failures. One manifestation is a complaint about missing blocks. For example:"},{heading:"dfsdatanodemaxtransferthreads",content:"See also Case Studies and note that this property was previously known as dfs.datanode.max.xcievers (e.g. Hadoop HDFS: Deceived by Xciever)."},{heading:"zookeeper-requirements",content:"An Apache ZooKeeper quorum is required. The exact version depends on your version of HBase, though the minimum ZooKeeper version is 3.4.x due to the useMulti feature made default in 1.0.0 (see HBASE-16598)."}],headings:[{id:"configuration-basic-prerequisites-java",content:"Java"},{id:"operating-system-utilities",content:"Operating System Utilities"},{id:"ssh",content:"ssh"},{id:"dns",content:"DNS"},{id:"ntp",content:"NTP"},{id:"limits-on-number-of-files-and-processes-ulimit",content:"Limits on Number of Files and Processes (ulimit)"},{id:"example-ulimit-settings-on-ubuntu-toc",content:"Example: ulimit Settings on Ubuntu [!toc]"},{id:"linux-shell",content:"Linux Shell"},{id:"windows",content:"Windows"},{id:"hadoop",content:"Hadoop"},{id:"hadoop-version-support-matrix-for-active-release-lines",content:"Hadoop version support matrix for active release lines"},{id:"hadoop-version-support-matrix-for-eom-23-release-lines",content:"Hadoop version support matrix for EOM 2.3+ release lines"},{id:"hadoop-version-support-matrix-for-eom-2x-release-lines",content:"Hadoop version support matrix for EOM 2.x release lines"},{id:"hadoop-version-support-matrix-for-eom-15-release-lines",content:"Hadoop version support matrix for EOM 1.5+ release lines"},{id:"hadoop-version-support-matrix-for-eom-1x-release-lines",content:"Hadoop version support matrix for EOM 1.x release lines"},{id:"hadoop-version-support-matrix-for-eom-pre-10-release-lines",content:"Hadoop version support matrix for EOM pre-1.0 release lines"},{id:"hadoop-3-support-for-the-hbase-binary-releases-and-maven-artifacts",content:"Hadoop 3 Support for the HBase Binary Releases and Maven Artifacts"},{id:"hadoop-3-version-policy",content:"Hadoop 3 version policy"},{id:"dfsdatanodemaxtransferthreads",content:"dfs.datanode.max.transfer.threads"},{id:"zookeeper-requirements",content:"ZooKeeper Requirements"}]};const c=[{depth:2,url:"#configuration-basic-prerequisites-java",title:e.jsx(e.Fragment,{children:"Java"})},{depth:3,url:"#operating-system-utilities",title:e.jsx(e.Fragment,{children:"Operating System Utilities"})},{depth:4,url:"#ssh",title:e.jsx(e.Fragment,{children:"ssh"})},{depth:4,url:"#dns",title:e.jsx(e.Fragment,{children:"DNS"})},{depth:4,url:"#ntp",title:e.jsx(e.Fragment,{children:"NTP"})},{depth:4,url:"#limits-on-number-of-files-and-processes-ulimit",title:e.jsx(e.Fragment,{children:"Limits on Number of Files and Processes (ulimit)"})},{depth:4,url:"#linux-shell",title:e.jsx(e.Fragment,{children:"Linux Shell"})},{depth:4,url:"#windows",title:e.jsx(e.Fragment,{children:"Windows"})},{depth:2,url:"#hadoop",title:e.jsx(e.Fragment,{children:"Hadoop"})},{depth:3,url:"#hadoop-version-support-matrix-for-active-release-lines",title:e.jsx(e.Fragment,{children:"Hadoop version support matrix for active release lines"})},{depth:3,url:"#hadoop-version-support-matrix-for-eom-23-release-lines",title:e.jsx(e.Fragment,{children:"Hadoop version support matrix for EOM 2.3+ release lines"})},{depth:3,url:"#hadoop-version-support-matrix-for-eom-2x-release-lines",title:e.jsx(e.Fragment,{children:"Hadoop version support matrix for EOM 2.x release lines"})},{depth:3,url:"#hadoop-version-support-matrix-for-eom-15-release-lines",title:e.jsx(e.Fragment,{children:"Hadoop version support matrix for EOM 1.5+ release lines"})},{depth:3,url:"#hadoop-version-support-matrix-for-eom-1x-release-lines",title:e.jsx(e.Fragment,{children:"Hadoop version support matrix for EOM 1.x release lines"})},{depth:3,url:"#hadoop-version-support-matrix-for-eom-pre-10-release-lines",title:e.jsx(e.Fragment,{children:"Hadoop version support matrix for EOM pre-1.0 release lines"})},{depth:3,url:"#hadoop-3-support-for-the-hbase-binary-releases-and-maven-artifacts",title:e.jsx(e.Fragment,{children:"Hadoop 3 Support for the HBase Binary Releases and Maven Artifacts"})},{depth:3,url:"#hadoop-3-version-policy",title:e.jsx(e.Fragment,{children:"Hadoop 3 version policy"})},{depth:3,url:"#dfsdatanodemaxtransferthreads",title:e.jsx(e.Fragment,{children:e.jsx("code",{children:"dfs.datanode.max.transfer.threads"})})},{depth:2,url:"#zookeeper-requirements",title:e.jsx(e.Fragment,{children:"ZooKeeper Requirements"})}];function t(s){const o={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...s.components},{Callout:n}=o;return n||i("Callout"),e.jsxs(e.Fragment,{children:[e.jsx(o.h2,{id:"configuration-basic-prerequisites-java",children:"Java"}),`
`,e.jsx(o.p,{children:"HBase runs on the Java Virtual Machine, thus all HBase deployments require a JVM runtime."}),`
`,e.jsx(o.p,{children:"The following table summarizes the recommendations of the HBase community with respect to running on various Java versions. The ✅ symbol indicates a base level of testing and willingness to help diagnose and address issues you might run into; these are the expected deployment combinations. An entry of ⚠️ means that there may be challenges with this combination, and you should look for more information before deciding to pursue this as your deployment strategy. The ❌ means this combination does not work; either an older Java version is considered deprecated by the HBase community, or this combination is known to not work. For combinations of newer JDK with older HBase releases, it's likely there are known compatibility issues that cannot be addressed under our compatibility guarantees, making the combination impossible. In some cases, specific guidance on limitations (e.g. whether compiling / unit tests work, specific operational issues, etc) are also noted. Assume any combination not listed here is considered ❌."}),`
`,e.jsx(n,{type:"warn",children:e.jsxs(o.p,{children:[`HBase recommends downstream users rely only on JDK releases that are marked as Long-Term Supported
(LTS), either from the OpenJDK project or vendors. At the time of this writing, the following JDK
releases are NOT LTS releases and are NOT tested or advocated for use by the Apache HBase
community: JDK9, JDK10, JDK12, JDK13, and JDK14. Community discussion around this decision is
recorded on `,e.jsx(o.a,{href:"https://issues.apache.org/jira/browse/HBASE-20264",children:"HBASE-20264"}),"."]})}),`
`,e.jsx(n,{type:"tip",children:e.jsx(o.p,{children:`At this time, all testing performed by the Apache HBase project runs on the HotSpot variant of the
JVM. When selecting your JDK distribution, please take this into consideration.`})}),`
`,e.jsx(o.p,{children:e.jsx(o.strong,{children:"Java support by release line"})}),`
`,e.jsxs(o.table,{children:[e.jsx(o.thead,{children:e.jsxs(o.tr,{children:[e.jsx(o.th,{style:{textAlign:"center"},children:"HBase Version"}),e.jsx(o.th,{style:{textAlign:"center"},children:"JDK 6"}),e.jsx(o.th,{style:{textAlign:"center"},children:"JDK 7"}),e.jsx(o.th,{style:{textAlign:"center"},children:"JDK 8"}),e.jsx(o.th,{style:{textAlign:"center"},children:"JDK 11"}),e.jsx(o.th,{style:{textAlign:"center"},children:"JDK 17"})]})}),e.jsxs(o.tbody,{children:[e.jsxs(o.tr,{children:[e.jsx(o.td,{style:{textAlign:"center"},children:"HBase 2.6"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{style:{textAlign:"center"},children:"HBase 2.5"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"⚠️*"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{style:{textAlign:"center"},children:"HBase 2.4"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{style:{textAlign:"center"},children:"HBase 2.3"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"⚠️*"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{style:{textAlign:"center"},children:"HBase 2.0-2.2"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{style:{textAlign:"center"},children:"HBase 1.2+"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{style:{textAlign:"center"},children:"HBase 1.0-1.1"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"⚠️"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{style:{textAlign:"center"},children:"HBase 0.98"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"⚠️"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{style:{textAlign:"center"},children:"HBase 0.94"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"✅"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"}),e.jsx(o.td,{style:{textAlign:"center"},children:"❌"})]})]})]}),`
`,e.jsxs(n,{type:"warn",children:[e.jsx(o.p,{children:"Preliminary support for JDK11 is introduced with HBase 2.3.0, and for JDK17 is introduced with HBase 2.5.x. We will compile and run test suites with JDK11/17 in pre commit checks and nightly checks. We will mark the support as ✅ as long as we have run some ITs with the JDK version and also there are users in the community use the JDK version in real production clusters."}),e.jsxs(o.p,{children:["For JDK11/JDK17 support in HBase, please refer to ",e.jsx(o.a,{href:"https://issues.apache.org/jira/browse/HBASE-22972",children:"HBASE-22972"})," and ",e.jsx(o.a,{href:"https://issues.apache.org/jira/browse/HBASE-26038",children:"HBASE-26038"})]}),e.jsxs(o.p,{children:["For JDK11/JDK17 support in Hadoop, which may also affect HBase, please refer to ",e.jsx(o.a,{href:"https://issues.apache.org/jira/browse/HADOOP-15338",children:"HADOOP-15338"})," and ",e.jsx(o.a,{href:"https://issues.apache.org/jira/browse/HADOOP-17177",children:"HADOOP-17177"})]})]}),`
`,e.jsx(n,{type:"info",children:e.jsxs(o.p,{children:["You must set ",e.jsx(o.code,{children:"JAVA_HOME"})," on each node of your cluster. ",e.jsx(o.em,{children:"hbase-env.sh"}),` provides a handy mechanism
to do this.`]})}),`
`,e.jsx(o.h3,{id:"operating-system-utilities",children:"Operating System Utilities"}),`
`,e.jsx(o.h4,{id:"ssh",children:"ssh"}),`
`,e.jsxs(o.p,{children:["HBase uses the Secure Shell (ssh) command and utilities extensively to communicate between cluster nodes. Each server in the cluster must be running ",e.jsx(o.code,{children:"ssh"}),' so that the Hadoop and HBase daemons can be managed. You must be able to connect to all nodes via SSH, including the local node, from the Master as well as any backup Master, using a shared key rather than a password. You can see the basic methodology for such a set-up in Linux or Unix systems at "',e.jsx(o.a,{href:"/docs/getting-started#procedure-configure-passwordless-ssh-access",children:"Procedure: Configure Passwordless SSH Access"}),'" chapter. If your cluster nodes use OS X, see the section, ',e.jsx(o.a,{href:"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=120730246#RunningHadoopOnOSX10.564-bit(Single-NodeCluster)-SSH:SettingupRemoteDesktopandEnablingSelf-Login",children:"SSH: Setting up Remote Desktop and Enabling Self-Login"})," on the Hadoop wiki."]}),`
`,e.jsx(o.h4,{id:"dns",children:"DNS"}),`
`,e.jsx(o.p,{children:"HBase uses the local hostname to self-report its IP address."}),`
`,e.jsx(o.h4,{id:"ntp",children:"NTP"}),`
`,e.jsxs(o.p,{children:["The clocks on cluster nodes should be synchronized. A small amount of variation is acceptable, but larger amounts of skew can cause erratic and unexpected behavior. Time synchronization is one of the first things to check if you see unexplained problems in your cluster. It is recommended that you run a Network Time Protocol (NTP) service, or another time-synchronization mechanism on your cluster and that all nodes look to the same service for time synchronization. See the ",e.jsx(o.a,{href:"http://www.tldp.org/LDP/sag/html/basic-ntp-config.html",children:"Basic NTP Configuration"})," at ",e.jsx(o.em,{children:"The Linux Documentation Project (TLDP)"})," to set up NTP."]}),`
`,e.jsx(o.h4,{id:"limits-on-number-of-files-and-processes-ulimit",children:"Limits on Number of Files and Processes (ulimit)"}),`
`,e.jsxs(o.p,{children:["Apache HBase is a database. It requires the ability to open a large number of files at once. Many Linux distributions limit the number of files a single user is allowed to open to ",e.jsx(o.code,{children:"1024"})," (or ",e.jsx(o.code,{children:"256"})," on older versions of OS X). You can check this limit on your servers by running the command ",e.jsx(o.code,{children:"ulimit -n"})," when logged in as the user which runs HBase. See ",e.jsx(o.a,{href:"/docs/troubleshooting#javaioioexceptiontoo-many-open-files",children:"the Troubleshooting section"})," for some of the problems you may experience if the limit is too low. You may also notice errors such as the following:"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(o.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(o.code,{children:[e.jsx(o.span,{className:"line",children:e.jsx(o.span,{children:"2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException"})}),`
`,e.jsx(o.span,{className:"line",children:e.jsx(o.span,{children:"2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901"})})]})})}),`
`,e.jsx(o.p,{children:"It is recommended to raise the ulimit to at least 10,000, but more likely 10,240, because the value is usually expressed in multiples of 1024. Each ColumnFamily has at least one StoreFile, and possibly more than six StoreFiles if the region is under load. The number of open files required depends upon the number of ColumnFamilies and the number of regions. The following is a rough formula for calculating the potential number of open files on a RegionServer."}),`
`,e.jsx(o.p,{children:e.jsx(o.strong,{children:"Calculate the Potential Number of Open Files:"})}),`
`,e.jsx(e.Fragment,{children:e.jsx(o.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsx(o.code,{children:e.jsx(o.span,{className:"line",children:e.jsx(o.span,{children:"(StoreFiles per ColumnFamily) x (regions per RegionServer)"})})})})}),`
`,e.jsxs(o.p,{children:["For example, assuming that a schema had 3 ColumnFamilies per region with an average of 3 StoreFiles per ColumnFamily, and there are 100 regions per RegionServer, the JVM will open ",e.jsx(o.code,{children:"3 * 3 * 100 = 900"})," file descriptors, not counting open JAR files, configuration files, and others. Opening a file does not take many resources, and the risk of allowing a user to open too many files is minimal."]}),`
`,e.jsxs(o.p,{children:["Another related setting is the number of processes a user is allowed to run at once. In Linux and Unix, the number of processes is set using the ",e.jsx(o.code,{children:"ulimit -u"})," command. This should not be confused with the ",e.jsx(o.code,{children:"nproc"})," command, which controls the number of CPUs available to a given user. Under load, a ",e.jsx(o.code,{children:"ulimit -u"})," that is too low can cause OutOfMemoryError exceptions."]}),`
`,e.jsx(o.p,{children:"Configuring the maximum number of file descriptors and processes for the user who is running the HBase process is an operating system configuration, rather than an HBase configuration. It is also important to be sure that the settings are changed for the user that actually runs HBase. To see which user started HBase, and that user's ulimit configuration, look at the first line of the HBase log for that instance."}),`
`,e.jsxs(o.h4,{id:"example-ulimit-settings-on-ubuntu-toc",children:["Example: ",e.jsx(o.code,{children:"ulimit"})," Settings on Ubuntu"]}),`
`,e.jsxs(o.p,{children:["To configure ulimit settings on Ubuntu, edit ",e.jsx(o.em,{children:"/etc/security/limits.conf"}),", which is a space-delimited file with four columns. Refer to the man page for ",e.jsx(o.em,{children:"limits.conf"})," for details about the format of this file. In the following example, the first line sets both soft and hard limits for the number of open files (nofile) to 32768 for the operating system user with the username hadoop. The second line sets the number of processes to 32000 for the same user."]}),`
`,e.jsx(e.Fragment,{children:e.jsx(o.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(o.code,{children:[e.jsx(o.span,{className:"line",children:e.jsx(o.span,{children:"hadoop  -       nofile  32768"})}),`
`,e.jsx(o.span,{className:"line",children:e.jsx(o.span,{children:"hadoop  -       nproc   32000"})})]})})}),`
`,e.jsxs(o.p,{children:["The settings are only applied if the Pluggable Authentication Module (PAM) environment is directed to use them. To configure PAM to use these limits, be sure that the ",e.jsx(o.em,{children:"/etc/pam.d/common-session"})," file contains the following line:"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(o.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsx(o.code,{children:e.jsx(o.span,{className:"line",children:e.jsx(o.span,{children:"session required  pam_limits.so"})})})})}),`
`,e.jsx(o.h4,{id:"linux-shell",children:"Linux Shell"}),`
`,e.jsxs(o.p,{children:["All of the shell scripts that come with HBase rely on the ",e.jsx(o.a,{href:"http://www.gnu.org/software/bash",children:"GNU Bash"})," shell."]}),`
`,e.jsx(o.h4,{id:"windows",children:"Windows"}),`
`,e.jsx(o.p,{children:"Running production systems on Windows machines is not recommended."}),`
`,e.jsx(o.h2,{id:"hadoop",children:"Hadoop"}),`
`,e.jsxs(o.p,{children:["The following table summarizes the versions of ",e.jsx(o.a,{href:"https://hadoop.apache.org",children:"Hadoop"})," supported with each version of HBase. Older versions not appearing in this table are considered unsupported and likely missing necessary features, while newer versions are untested but may be suitable."]}),`
`,e.jsxs(o.p,{children:["Based on the version of HBase, you should select the most appropriate version of Hadoop. You can use Apache Hadoop, or a vendor's distribution of Hadoop. No distinction is made here. See ",e.jsx(o.a,{href:"https://cwiki.apache.org/confluence/display/HADOOP2/Distributions+and+Commercial+Support",children:"the Hadoop wiki"})," for information about vendors of Hadoop."]}),`
`,e.jsxs(n,{type:"tip",children:[e.jsxs(o.p,{children:["Comparing to Hadoop 1.x, Hadoop 2.x is faster and includes features, such as short-circuit reads (see ",e.jsx(o.a,{href:"/docs/performance#leveraging-local-data",children:"Leveraging local data"}),"), which will help improve your HBase random read profile. Hadoop 2.x also includes important bug fixes that will improve your overall HBase experience. HBase does not support running with earlier versions of Hadoop. See the table below for requirements specific to different HBase versions."]}),e.jsx(o.p,{children:"Today, Hadoop 3.x is recommended as the last Hadoop 2.x release 2.10.2 was released years ago, and there is no release for Hadoop 2.x for a very long time, although the Hadoop community does not officially EOL Hadoop 2.x yet."})]}),`
`,e.jsx(o.p,{children:"Use the following legend to interpret these tables:"}),`
`,e.jsxs(o.ul,{children:[`
`,e.jsx(o.li,{children:"✅ = Tested to be fully-functional"}),`
`,e.jsxs(o.li,{children:["❌ = Known to not be fully-functional, or there are ",e.jsx(o.a,{href:"https://hadoop.apache.org/cve_list.html",children:"CVEs"})," so we drop the support in newer minor releases"]}),`
`,e.jsx(o.li,{children:"⚠️ = Not tested, may/may-not function"}),`
`]}),`
`,e.jsxs(o.table,{children:[e.jsx(o.thead,{children:e.jsxs(o.tr,{children:[e.jsx(o.th,{}),e.jsx(o.th,{children:"HBase-2.5.x"}),e.jsx(o.th,{children:"HBase-2.6.x"})]})}),e.jsxs(o.tbody,{children:[e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.10.[0-1]"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.10.2+"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.1.0"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.1.1+"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.2.[0-2]"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.2.3+"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.3.[0-1]"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.3.[2-4]"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.3.5+"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.4.0+"}),e.jsx(o.td,{children:"✅ (2.5.11+)"}),e.jsx(o.td,{children:"✅ (2.6.2+)"})]})]})]}),`
`,e.jsx(o.h3,{id:"hadoop-version-support-matrix-for-active-release-lines",children:"Hadoop version support matrix for active release lines"}),`
`,e.jsxs(o.table,{children:[e.jsx(o.thead,{children:e.jsxs(o.tr,{children:[e.jsx(o.th,{}),e.jsx(o.th,{children:"HBase-2.3.x"}),e.jsx(o.th,{children:"HBase-2.4.x"})]})}),e.jsxs(o.tbody,{children:[e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.10.x"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.1.0"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.1.1+"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.2.x"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.3.x"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]})]})]}),`
`,e.jsx(o.h3,{id:"hadoop-version-support-matrix-for-eom-23-release-lines",children:"Hadoop version support matrix for EOM 2.3+ release lines"}),`
`,e.jsxs(o.table,{children:[e.jsx(o.thead,{children:e.jsxs(o.tr,{children:[e.jsx(o.th,{}),e.jsx(o.th,{children:"HBase-2.0.x"}),e.jsx(o.th,{children:"HBase-2.1.x"}),e.jsx(o.th,{children:"HBase-2.2.x"})]})}),e.jsxs(o.tbody,{children:[e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.6.1+"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.7.[0-6]"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.7.7+"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.8.[0-2]"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.8.[3-4]"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.8.5+"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.9.[0-1]"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.9.2+"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.0.[0-2]"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.0.3+"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.1.0"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-3.1.1+"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]})]})]}),`
`,e.jsx(o.h3,{id:"hadoop-version-support-matrix-for-eom-2x-release-lines",children:"Hadoop version support matrix for EOM 2.x release lines"}),`
`,e.jsxs(o.table,{children:[e.jsx(o.thead,{children:e.jsxs(o.tr,{children:[e.jsx(o.th,{}),e.jsx(o.th,{children:"HBase-1.5.x"}),e.jsx(o.th,{children:"HBase-1.6.x"}),e.jsx(o.th,{children:"HBase-1.7.x"})]})}),e.jsxs(o.tbody,{children:[e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.7.7+"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.8.[0-4]"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.8.5+"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.9.[0-1]"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.9.2+"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.10.x"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]})]})]}),`
`,e.jsx(o.h3,{id:"hadoop-version-support-matrix-for-eom-15-release-lines",children:"Hadoop version support matrix for EOM 1.5+ release lines"}),`
`,e.jsxs(o.table,{children:[e.jsx(o.thead,{children:e.jsxs(o.tr,{children:[e.jsx(o.th,{}),e.jsx(o.th,{children:"HBase-1.0.x (Hadoop 1.x is NOT supported)"}),e.jsx(o.th,{children:"HBase-1.1.x"}),e.jsx(o.th,{children:"HBase-1.2.x"}),e.jsx(o.th,{children:"HBase-1.3.x"}),e.jsx(o.th,{children:"HBase-1.4.x"})]})}),e.jsxs(o.tbody,{children:[e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.4.x"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.5.x"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.6.0"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.6.1+"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.7.0"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.7.1+"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]})]})]}),`
`,e.jsx(o.h3,{id:"hadoop-version-support-matrix-for-eom-1x-release-lines",children:"Hadoop version support matrix for EOM 1.x release lines"}),`
`,e.jsxs(o.table,{children:[e.jsx(o.thead,{children:e.jsxs(o.tr,{children:[e.jsx(o.th,{}),e.jsx(o.th,{children:"HBase-0.92.x"}),e.jsx(o.th,{children:"HBase-0.94.x"}),e.jsx(o.th,{children:"HBase-0.96.x"}),e.jsx(o.th,{children:"HBase-0.98.x (Support for Hadoop 1.1+ is deprecated.)"})]})}),e.jsxs(o.tbody,{children:[e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-0.20.205"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-0.22.x"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-1.0.x"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-1.1.x"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"⚠️"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-0.23.x"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.0.x-alpha"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.1.0-beta"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"❌"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.2.0"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.3.x"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.4.x"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]}),e.jsxs(o.tr,{children:[e.jsx(o.td,{children:"Hadoop-2.5.x"}),e.jsx(o.td,{children:"❌"}),e.jsx(o.td,{children:"⚠️"}),e.jsx(o.td,{children:"✅"}),e.jsx(o.td,{children:"✅"})]})]})]}),`
`,e.jsx(o.h3,{id:"hadoop-version-support-matrix-for-eom-pre-10-release-lines",children:"Hadoop version support matrix for EOM pre-1.0 release lines"}),`
`,e.jsx(n,{type:"idea",children:e.jsxs(o.p,{children:[`Starting around the time of Hadoop version 2.7.0, the Hadoop PMC got into the habit of calling out
new minor releases on their major version 2 release line as not stable / production ready. As
such, HBase expressly advises downstream users to avoid running on top of these releases. Note
that additionally the 2.8.1 release was given the same caveat by the Hadoop PMC. For reference,
see the release announcements for `,e.jsx(o.a,{href:"https://s.apache.org/hadoop-2.7.0-announcement",children:`Apache Hadoop
2.7.0`}),", ",e.jsx(o.a,{href:"https://s.apache.org/hadoop-2.8.0-announcement",children:`Apache Hadoop
2.8.0`}),", ",e.jsx(o.a,{href:"https://s.apache.org/hadoop-2.8.1-announcement",children:`Apache Hadoop
2.8.1`}),", and ",e.jsx(o.a,{href:"https://s.apache.org/hadoop-2.9.0-announcement",children:`Apache Hadoop
2.9.0`}),"."]})}),`
`,e.jsx(n,{type:"idea",children:e.jsxs(o.p,{children:[`The Hadoop PMC called out the 3.1.0 release as not stable / production ready. As such, HBase
expressly advises downstream users to avoid running on top of this release. For reference, see the
`,e.jsx(o.a,{href:"https://s.apache.org/hadoop-3.1.0-announcement",children:"release announcement for Hadoop 3.1.0"}),"."]})}),`
`,e.jsx(n,{type:"info",children:e.jsxs(o.p,{children:["Because HBase depends on Hadoop, it bundles Hadoop jars under its ",e.jsx(o.em,{children:"lib"}),` directory. The bundled
jars are ONLY for use in stand-alone mode. In distributed mode, it is `,e.jsx(o.em,{children:"critical"}),` that the version
of Hadoop that is out on your cluster match what is under HBase. Replace the hadoop jars found in
the HBase lib directory with the equivalent hadoop jars from the version you are running on your
cluster to avoid version mismatch issues. Make sure you replace the jars under HBase across your
whole cluster. Hadoop version mismatch issues have various manifestations. Check for mismatch if
HBase appears hung.`]})}),`
`,e.jsx(o.h3,{id:"hadoop-3-support-for-the-hbase-binary-releases-and-maven-artifacts",children:"Hadoop 3 Support for the HBase Binary Releases and Maven Artifacts"}),`
`,e.jsx(o.p,{children:"For HBase 2.5.1 and earlier, the official HBase binary releases and Maven artifacts were built with Hadoop 2.x."}),`
`,e.jsxs(o.p,{children:["Starting with HBase 2.5.2, HBase provides binary releases and Maven artifacts built with both Hadoop 2.x and Hadoop 3.x. The Hadoop 2 artifacts do not have any version suffix, the Hadoop 3 artifacts add the ",e.jsx(o.code,{children:"-hadoop-3"})," suffix to the version. i.e. ",e.jsx(o.code,{children:"hbase-2.5.2-bin.tar.gz.asc"})," is the Binary release built with Hadoop2, and ",e.jsx(o.code,{children:"hbase-2.5.2-hadoop3-bin.tar.gz"})," is the release built with Hadoop 3."]}),`
`,e.jsx(o.h3,{id:"hadoop-3-version-policy",children:"Hadoop 3 version policy"}),`
`,e.jsx(o.p,{children:"Each HBase release has a default Hadoop 3 version. This is used when the Hadoop 3 version is not specified during build, and for building the official binary releases and artifacts. Generally when a new minor version is released (i.e. 2.5.0) the default version is set to the latest supported Hadoop 3 version at the start of the release process."}),`
`,e.jsx(o.p,{children:"Up to HBase 2.5.10 and 2.6.1 even if HBase added support for newer Hadoop 3 releases in a patch release, the default Hadoop 3 version (and the one used in the official binary releases) was not updated. This simplified upgrading, but meant that HBase releases often included old unfixed CVEs both from Hadoop and Hadoop's dependencies, even when newer Hadoop releases with fixes were available."}),`
`,e.jsxs(o.p,{children:["Starting with HBase 2.5.11 and 2.6.2, the default Hadoop 3 version is always set to the latest supported Hadoop 3 version, and is also used for the ",e.jsx(o.code,{children:"-hadoop3"})," binary releases and artifacts. This will drastically reduce the number of known CVEs shipped in the HBase binary releases, and make sure that all fixes and improvements in Hadoop are included."]}),`
`,e.jsx(o.h3,{id:"dfsdatanodemaxtransferthreads",children:e.jsx(o.code,{children:"dfs.datanode.max.transfer.threads"})}),`
`,e.jsxs(o.p,{children:["An HDFS DataNode has an upper bound on the number of files that it will serve at any one time. Before doing any loading, make sure you have configured Hadoop's ",e.jsx(o.em,{children:"conf/hdfs-site.xml"}),", setting the ",e.jsx(o.code,{children:"dfs.datanode.max.transfer.threads"})," value to at least the following:"]}),`
`,e.jsx(e.Fragment,{children:e.jsx(o.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(o.code,{children:[e.jsxs(o.span,{className:"line",children:[e.jsx(o.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"<"}),e.jsx(o.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(o.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(o.span,{className:"line",children:[e.jsx(o.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(o.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(o.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">dfs.datanode.max.transfer.threads</"}),e.jsx(o.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"name"}),e.jsx(o.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(o.span,{className:"line",children:[e.jsx(o.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"  <"}),e.jsx(o.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(o.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">4096</"}),e.jsx(o.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"value"}),e.jsx(o.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]}),`
`,e.jsxs(o.span,{className:"line",children:[e.jsx(o.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:"</"}),e.jsx(o.span,{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"},children:"property"}),e.jsx(o.span,{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},children:">"})]})]})})}),`
`,e.jsx(o.p,{children:"Be sure to restart your HDFS after making the above configuration."}),`
`,e.jsx(o.p,{children:"Not having this configuration in place makes for strange-looking failures. One manifestation is a complaint about missing blocks. For example:"}),`
`,e.jsx(e.Fragment,{children:e.jsx(o.pre,{className:"shiki shiki-themes github-light github-dark",style:{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},tabIndex:"0",icon:'<svg viewBox="0 0 24 24"><path d="M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z" fill="currentColor" /></svg>',children:e.jsxs(o.code,{children:[e.jsx(o.span,{className:"line",children:e.jsx(o.span,{children:"10/12/08 20:10:31 INFO hdfs.DFSClient: Could not obtain block"})}),`
`,e.jsx(o.span,{className:"line",children:e.jsx(o.span,{children:"          blk_XXXXXXXXXXXXXXXXXXXXXX_YYYYYYYY from any node: java.io.IOException: No live nodes"})}),`
`,e.jsx(o.span,{className:"line",children:e.jsx(o.span,{children:"          contain current block. Will get new block locations from namenode and retry..."})})]})})}),`
`,e.jsxs(o.p,{children:["See also ",e.jsx(o.a,{href:"/docs/case-studies#case-study-4-maxtransferthreads-config",children:"Case Studies"})," and note that this property was previously known as ",e.jsx(o.code,{children:"dfs.datanode.max.xcievers"})," (e.g. ",e.jsx(o.a,{href:"http://ccgtech.blogspot.com/2010/02/hadoop-hdfs-deceived-by-xciever.html",children:"Hadoop HDFS: Deceived by Xciever"}),")."]}),`
`,e.jsx(o.h2,{id:"zookeeper-requirements",children:"ZooKeeper Requirements"}),`
`,e.jsxs(o.p,{children:["An Apache ZooKeeper quorum is required. The exact version depends on your version of HBase, though the minimum ZooKeeper version is 3.4.x due to the ",e.jsx(o.code,{children:"useMulti"})," feature made default in 1.0.0 (see ",e.jsx(o.a,{href:"https://issues.apache.org/jira/browse/HBASE-16598",children:"HBASE-16598"}),")."]})]})}function p(s={}){const{wrapper:o}=s.components||{};return o?e.jsx(o,{...s,children:e.jsx(t,{...s})}):t(s)}function i(s,o){throw new Error("Expected component `"+s+"` to be defined: you likely forgot to import, pass, or provide it.")}export{r as _markdown,p as default,h as extractedReferences,d as frontmatter,l as structuredData,c as toc};
